[{"categories":["操作系统"],"content":"book-riscv-rev4 : xv6: a simple, Unix-like teaching operating system 操作系统原型——xv6分析与实验 (罗秋明) xv6 book chapter 1 xv6实现了Unix所引入的接口，还复刻了Unix的内部架构设计。 当进程需要调用内核服务时，会调用系统调用（OS的一个接口调用），进入内核，内核执行完服务后再返回。当用户调用系统调用时，硬件会提升特权级别并启动内核中预先安排的功能（这里应该指的是trap）。 xv6进程由用户内存空间（指令、数据和堆栈）以及进程状态组成。内核为每个进程分配一个进程标识符（PID），fork创建新进程，会将原进程的内存（包括指令、数据和堆栈）完全复制到新进程中，返回两个参数：在原进程中返回新进程的PID、在新进程中返回零。 xv6 syscall 这里system call表中列出了sleep的描述，传入int类型整数，会暂停n个时钟周期。exit终止当前进程，释放资源，传入0表示成功，1表示失败；wait返回子进程exit后的PID，并将子进程的退出状态复制到传入的status参数，如果有子进程则等待一个退出，如果没有子进程立即返回-1;exec会从文件系统中加载一个文件替换调用进程的内存，该文件需为ELF格式，通常是编译程序后的结果，指令会从ELF声明的入口点开始执行，第二参数是字符串参数，通常字符串数组第一个元素是程序名称（被忽略掉），注意exec要求最后一个元素必须是0（NULL）表示参数列表的结束；sbrk将进程内存扩展n个字节，返回新内存的地址；read从文件描述符fd中读取n个字节复制到buf中，并返回已读取的字节数，每个文件描述符都关联一个偏移量，read会从当前文件描述符对应的文件偏移量读取数据，无法读取时返回0表示结束；write会将n个字节从缓冲区写入fd，也会从文件偏移量开始；dup复制一个现有文件描述符，返回一个指向原描述符对象的新描述符，两个描述符共享一个偏移量；pipe创建一个管道，p[0]放读fd，p[1]放写fd。 xv6 book chapter 2 OS必须满足三大核心要求：多路复用、隔离与交互。xv6运行在多核RISC-V处理器上，RISC-V是一款64位CPU，xv6专为qemu的\"-machine virt\"选项模拟硬件环境设计，包括内存、ROM、串口、磁盘驱动器等。 某些嵌入式设备或实时OS系统实现的方式是：将系统调用实现为库，供应用程序调用，应用程序可以以这种方式直接与硬件资源交互。当有多个应用程序时，必须定期让出CPU权，这种协作分时机制需要应用程序之间是相互信任的，但更多的是需要隔离的。实现隔离就应避免直接访问硬件资源，而将其抽象为服务。 为实现强隔离机制，OS需要确保：应用程序无法修改OS的数据结构和指令，应用程序无法访问其他进程的内存。硬件上的强隔离：RISC-V具有三种指令模式：机器模式、监督模式（特权指令、内核空间）和用户模式（用户指令、用户空间）。若应用程序需要调用内核函数（系统调用）必须切换至内核模式。为此，RISC-V提供了ecall指令，将CPU切换至内核模式，并通过内核指定的入口点（入口点位置很重要，内核必须能够控制）进入内核。 为了减少内核代码出错导致内核崩溃的可能性，微内核架构尽量减少了内核空间的代码，仅包含启动应用程序、发送消息、访问硬件设备等少数底层功能，其他大部分功能驻留到用户空间中。 将所有系统调用的实现都在监督模式下运行，这种架构是宏内核，整个OS有一个拥有完整硬件特权的单一程序构成，各个组件之间的协作更为便捷。Linux采用的是宏内核，采用微内核架构的OS在嵌入式领域得到广泛使用。xv6采用的是宏内核，虽然整体功能量要比现实工业级的微内核还小。 xv6中隔离的基本单元是进程，每个进程为程序提供看似私有的地址空间以及看似独立的CPU来执行指令。xv6进程虚拟地址最大是0x3fffffffff（MAXVA），在顶端放了一个4096字节的跳转页（trampoline，用户态转为内核态）和陷阱页（trapframe，保存进程用户寄存器），进程最重要的内核状态包括页表、内核栈和运行状态，这些状态信息由proc结构体维护。每个进程又有一个线程，用于保存执行进程所需的状态信息。线程的大部分状态信息（局部变量、函数调用返回地址等）都存储在线程栈中。每个进程有两个栈：用户栈和内核栈，用户态使用用户栈，内核态使用内核栈。可以通过ecall发起系统调用，ecall提升硬件级别，并将PC指向内核定义的入口点，执行入口点的代码，入口点的代码会切换进程的内核栈，并执行实现系统调用的内核指令，执行完成后切换回用户栈，并通过sret指令返回用户空间。 进程通过两种设计思路实现上述两种虚拟化（内存虚拟化和CPU虚拟化），内存虚拟化：虚拟地址空间，CPU虚拟化：线程。在xv6中，进程仅包含一个地址空间和一个线程，实际为充分利用多核CPU会配置多个线程。 xv6内核如何启动并运行第一个进程？当RISC-V开机时，会初始化硬件并运行存储在ROM中的引导加载程序，该程序将xv6内核加载到内存中，然后在机器模式下，从入口点（entry.S）开始执行xv6指令，设置堆栈以便运行start.c代码。函数start会执行一些仅在机器模式下运行的配置操作，然后切换至监督模式（寄存器mstatus），将返回地址设为主程序地址（写入主程序地址到寄存器mepc），禁用监督模式下的虚拟地址转换（向页表寄存器satp写入0），并将所有中断和异常都委托给监督模式处理。主程序main.c初始化多个设备后调用用户初始化函数proc.c创建首个进程，该进程首先执行一段汇编代码initcode.S，加载exec系统调用，将exec系统调用号SYS_EXEC加载到寄存器a7中，然后调用ecall进入内核。内核通过系统调用中a7寄存器的值来调用所需的系统调用函数，exec系统调用会执行exec(\"/init\")替换当前进程的内存和寄存器，init进程会在需要时创建一个新的控制台设备文件（开启文件描述符0、1、2），接着在控制台上启动一个shell，系统就启动了。 用户调用如何传递到内核中的exec系统调用实现？初始化代码.S将exec的参数存入寄存器a0和a1，并将系统调用号存入a7，ecall进入内核，依次执行uservec、usertrap和system call操作，system call从陷阱页中保存的a7读取系统调用号，从而执行对应的system call。返回时，将返回值记录在p-\u003etrapframe-\u003ea0中，就导致用户空间的exec()调用返回该值。 内核中系统调用参数是用户代码传递的参数，这些参数会放在寄存器中，内核trap代码会将用户寄存器保存到当前进程的陷阱页中，以便内核代码进行提取。内核函数argint、argaddr、argfd会从陷阱页中提取系统调用参第n个数，同时调用argraw来获取相应的用户寄存器保存值。 xv6 book chapter 3 xv6主要用了两种技巧：1. 将相同内存（trampoline）映射到多个地址空间；2. 用未映射页来保护内核堆栈和用户堆栈。 xv6基于Sv39 RISC-V运行，使用了低39位的虚拟地址。而这39位又划分为低12位及剩余的27位，27位可构成2^27个页表项（PTE），对应转换为44位的物理地址，低12位则为偏移量，直接复制到物理地址的低12位。 地址转换 RISC-V CPU具体将虚拟地址转为物理地址的步骤： 页表以三级树状结构存储在物理内存中（注意，页表存在物理内存中，没存在虚拟内存，也没存在MMU中），每一级页表大小是4KB，占一页，内含512（2^9）个页表项。 将上述27位虚拟地址划分为3个9位，每9位选择每级页表中的页表项。如果所需的三个PTE中任何一个不存在，分页硬件会引发分页错误异常，并由内核处理该异常。与单级页表相比，三级页表能节省较大的内存空间，比如当应用程序仅使用从地址零开始的少数页面时，顶层目录中的1至511号条目将失效，内核无需为这些条目分配中间目录，也就无需分配对应的底层页面，从而为中间目录节省了511个页面，为底层目录节省了511*512个页面。 这样虽然节省了页面，但增加了CPU加载PTE的开销，每次要读取3个PTE，为避免开销，会将PTE缓存在TLB中。 每个PTE的低10位是标志位，用于告知分页硬件如何使用相关虚拟地址，如是否有效、是否可读、是否可写等。 要让CPU使用页表，就要将顶层页目录的物理地址写到satp寄存器中，从而读取到顶层页目录的内容进行后续查表。每个CPU有自己的satp寄存器，从而实现拥有自己的虚拟地址空间，进而同时运行不同的进程。 多级页表 内核页表只有一个，大部分直接映射到物理内存，从KERNBASE（0x80000000）开始到PHYSTOP（0x88000000）这部分是QEMU模拟的地址空间范围，主要存放kernel的代码段、数据段、堆区；在KERNBASE之下将IO设备接口以内存映射寄存器的方式暴露给软件，直接与硬件通信；trampoline和栈页不是直接映射的，由于栈页放在了虚拟地址的高地址，因此保留了一个未映射的保护页，当内核栈溢出时会触发异常。 地址空间映射 代码层面上，以kvm开头的操作内核页表，以uvm开头的操作用户页表，其他同时用于两者。RISC-V中包含一个名为sfence.vma的指令，用于刷新当前CPU的TLB，xv6会在重新加载satp寄存器之后，trampoline切换用户页","date":"2025-08-30","objectID":"/posts/xv6-note/:0:0","tags":["xv6","读书笔记"],"title":"xv6读书笔记","uri":"/posts/xv6-note/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 mmap 先读一下从内核世界透视mmap内存映射的本质（原理篇）这篇文章。 #include \u003csys/mman.h\u003e void* mmap(void* addr, size_t length, int prot, int flags, int fd, off_t offset); 系统调用mmap映射的是虚拟内存中的文件映射与匿名映射区，在这段虚拟内存区域中，包含了一段一段的虚拟映射区，每调用一次mmap就会在文件映射与匿名映射区划分一段作为申请的虚拟内存。 addr：指定映射的虚拟内存起始地址，一般设为NULL，交由内核决定起始地址。 length：申请的内存有多大，决定匿名映射的物理内存有多大或文件映射的文件区域有多大。addr和length必须按照PAGE_SIZE对齐。 如果是文件映射，就要通过参数fd指定要映射文件的描述符，通过offset指定文件映射区在文件的偏移。Linux中内存页和磁盘块大小一般情况都是4KB，这里的offset也必须按照4KB对齐。 mmap映射的虚拟内存在内核中用struct vm_area_struct结构表示，进程中的VMA有两种组织形式，双向链表和红黑树。mmap系统调用本质是先要在虚拟内存空间中划分出一段VMA出来，这段VMA区域的大小由vm_start，vm_end表示，而它们由mmap参数addr、length决定；随后内核会对这段VMA进行相关的映射，如果是文件映射，内核会将映射的文件以及要映射文件区域在文件中的offset，与VMA中的vm_file、vm_pgoff关联映射起来，他们由mmap参数fd、offset决定；mmap映射的这段VMA中的相关权限和标志位，是由mmap参数的prot、flags决定的，最终会映射到VMA中的vm_page_prot、vm_flags中，指定进程对这块VMA的访问权限和相关标志位。PS：进程所依赖的动态链接库.so文件也是通过mmap文件映射的方式将代码段、数据段映射到文件映射与匿名映射区中。 mmap中参数prot指定VMA的访问权限，取值有四种： #define PROT_READ 0x1 /* page can be read */ #define PROT_WRITE 0x2 /* page can be written */ #define PROT_EXEC 0x4 /* page can be executed */ #define PROT_NONE 0x0 /* page can not be accessed */ mmap中参数flags指定VMA映射方式，OS对物理页的管理类型有两种：一种是匿名页、一种是文件页，对应的映射也就分为两种，匿名映射、文件映射，mmap所映射的物理内存能否在多进程间共享又分为共享映射、私有映射两种映射方式，共享映射是多进程共享的，一个进程修改了共享映射的内存其他进程是可以看到的，用于多进程间的通信；私有映射是进程私有的，其他进程看不到，多进程修改同一映射文件将不会回写到磁盘文件上。 按照匿名、文件与共享、私有进行组合，就有四种映射方式。 私有匿名映射：MAP_PRIVATE | MAP_ANONYMOUS，glib库中封装的malloc函数申请内存大于128KB时使用mmap私有匿名映射方式来申请堆内存。这里mmap只是先申请一段VMA，还没有真正分配物理内存，PTE是空的，只有读写此区域时，才会触发缺页异常分配物理内存。私有匿名映射除了用于申请虚拟内存之外，还会用于execve系统调用中，内核需要删除释放旧的虚拟内存空间并情况页表，然后打开可执行文件，解析文件头，判断可执行文件的格式，对应函数进行加载。这个过程就需要私有匿名映射创建新的虚拟内存空间中的BSS段、堆和栈。 私有匿名映射 私有文件映射：MAP_PRIVATE，mmap内存文件映射的本质是vm_area_struct结构体中传入参数fd所对应的file结构体指针，指向file结构体，file结构体中存有inode结构体成员，即该文件对应的inode索引，从而找到对应的磁盘块block。和私有匿名映射一样，它不会立即分配物理页面，而是利用缺页异常，进程1在建立PTE之后，再次访问这段文件内存映射时就相当于直接访问文件的page cache，这个过程是在用户态的，没有切态。进程2同样在访问这段文件时也触发缺页异常，建立PTE，但会直接指向进程1中的page cache。所有进程的PTE都设为只读，当任一进程试图写入时，又会触发缺页中断，会申请一个内存页，然后将page cache的内容拷贝到新内存页中，并更新PTE。当进程都有各自专属的物理内存页时，就和page cache脱离关系了，各自的修改在进程之间时互不可见的，且均不会回写到磁盘文件中。可执行文件的.text、.data就使用私有文件映射到了进程虚拟内存空间中的代码段和数据段中。 私有文件映射多进程 私有文件映射进程空间 共享文件映射：MAP_SHARED，和私有文件映射过程一样，唯一不同的点是，多进程中的虚拟内存映射区通过缺页中断会映射到同一page cache中，且是可读可写，不会再次触发缺页中断去各自分配新的物理页。多进程对共享映射区的任何修改都会通过内核回写线程pdflush刷新到磁盘文件中。根据 mmap 共享文件映射多进程之间读写共享（不会发生写时复制）的特点，常用于多进程之间共享内存（page cache），多进程之间的通讯。 共享文件映射 共享匿名映射：MAP_SHARED | MAP_ANONYMOUS，将fd设为-1来实现共享匿名映射，这种映射方式常用于父子进程之间共享内存，父子进程之间的通讯。这个思路如果按照上述的话，就是进程1缺页中断分配一个物理页，进程2同样缺页中断要分配进程1所指的物理页，但是进程2怎么找到这个物理页？找不到的。但这对于共享文件映射很容易，因为有文件的page cache存在，进程2可以根据offset从page cache中查找是否已经有其他进程把映射的文件内容加载到文件页中。如果文件页已经存在 page cache 中了，进程 2 直接映射这个文件页就可以了。共享匿名映射在内核中是通过一个叫做 tmpfs 的虚拟文件系统来实现的，tmpfs 不是传统意义上的文件系统，它是基于内存实现的，挂载在 dev/zero 目录下。当多个进程通过 mmap 进行共享匿名映射的时候，内核会在 tmpfs 文件系统中创建一个匿名文件，这个匿名文件并不是真实存在于磁盘上的，它是内核为了共享匿名映射而模拟出来的，匿名文件也有自己的 inode 结构以及 page cache。在 mmap 进行共享匿名映射的时候，内核会把这个匿名文件关联到进程的虚拟映射区 VMA 中。这样一来，当进程虚拟映射区域与 tmpfs 文件系统中的这个匿名文件映射起来之后，后面的流程就和共享文件映射一模一样了。由于是基于内存实现的虚拟文件系统，在其他进程是不可见的，而子进程是可见的，适用于父子进程间的共享匿名映射。 总结： 私有匿名映射，其主要用于进程申请虚拟内存，以及初始化进程虚拟内存空间中的 BSS 段，堆，栈这些虚拟内存区域。 私有文件映射，其核心特点是背后映射的文件页在多进程之间是读共享的，多个进程对各自虚拟内存区的修改只能反应到各自对应的文件页上，而且各自的修改在进程之间是互不可见的，最重要的一点是这些修改均不会回写到磁盘文件中。我们可以利用这些特点来加载二进制可执行文件的 .text , .data section 到进程虚拟内存空间中的代码段和数据段中。 共享文件映射，多进程之间读写共享（不会发生写时复制），常用于多进程之间共享内存（page cache），多进程之间的通讯。 共享匿名映射，用于父子进程之间共享内存，父子进程之间的通讯。父子进程之间需要依赖 tmpfs 中的匿名文件来实现共享内存。是一种特殊的共享文件映射。 xv6的mmap函数声明： void* mmap(void *addr, size_t len, int prot, int flags, int fd, off_t offset); 可以看出和上述Linux的mmap函数声明是一致的，本实验就是要实现类似于Unix的上述简化实现。只实现文件映射部分，没有匿名映射。实验说明： 内核自己决定文件映射的虚拟地址，mmap返回该地址，失败返回0xffffffffffffffff。 len是映射的字节数。 prot有可读、可写、可执行三种。 flags是MAP_PRIVATE或MAP_SHARED，即私有文件映射或共享文件映射，就是上述所提的2、3两种情况。 在usertrap()中page fault进行处理分配物理内存。 MAP_SHARED可以不分配物理内存。 munmap函数声明： int munmap(void *addr, size_t len); 移除指定地址范围内的mmap映射。 MAP_SHARED要将修改写入文件，进程退出时也要对MAP_SHARED的任何修改写入文件。 hints： 按照之前的老方法添加mmap和munmap系统调用。 定义VMA结构体，记录映射的虚拟地址空间的起始地址、长度、权限、文件等信息。 声明一个大小为16的VMA数组。 把VMA数组对应到一个未使用的区域。 VMA结构体中应包含一个指向file结构体的指针。 mmap应增加文件的引用计数。 VMA区域引发的page fault进行处理，分配一页物理内存，使用readi","date":"2025-08-26","objectID":"/posts/mit-lab-mmap/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Mmap","uri":"/posts/mit-lab-mmap/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 fs Large files 增加xv6文件大小，一个文件的大小是由inode决定的，inode记录文件的元信息，其中的address数组记录磁盘的块号。我们知道，文件就是一堆磁盘块，磁盘块的数量越多文件越大，所以理论上address数组越大，文件越大。但inode也是占用一个块大小（xv6是1024字节）的，那么address的大小会被限制在二百多，很明显是不够的。这里inode限制address数组大小为13，前12个元素是直接块，即address索引（逻辑块号）与其值（磁盘块号）是一一对应的，第13个元素是间接块，即指向了一个磁盘块，那个磁盘块里又划分了（BSIZE / sizeof(uint)）大小的数组，每个元素指向一个磁盘块，这样就由1个磁盘块扩充为了256个磁盘块。 间接块扩充文件 按照类似的思路，我们可以将上述间接块指向的磁盘块改为间接块，这样建立二级间接块，在二级间接块上指向实际的磁盘块，从而将1个磁盘块扩充为了256*256个磁盘块（这里其实牺牲了部分磁盘块作为间接块而不是文件块）。具体实现代码： static uint bmap(struct inode *ip, uint bn) { uint addr, *a; struct buf *bp; if(bn \u003c NDIRECT){ // 小于直接块数目 if((addr = ip-\u003eaddrs[bn]) == 0){ addr = balloc(ip-\u003edev); // 分配一个磁盘块给文件 if(addr == 0) return 0; ip-\u003eaddrs[bn] = addr; } return addr; } bn -= NDIRECT; if(bn \u003c NINDIRECT){ // 小于间接块数目 // Load indirect block, allocating if necessary. if((addr = ip-\u003eaddrs[NDIRECT]) == 0){ addr = balloc(ip-\u003edev); // 分配一个磁盘块作为间接块 if(addr == 0) return 0; ip-\u003eaddrs[NDIRECT] = addr; } bp = bread(ip-\u003edev, addr); // 从间接块里读 a = (uint*)bp-\u003edata; // 这里转为了uint* 一个指针指向其分配的磁盘块 if((addr = a[bn]) == 0){ // bp的size就是NINDIRECT BSIZE/sizeof(uint) addr = balloc(ip-\u003edev); // 分配一个磁盘块给文件 if(addr){ a[bn] = addr; log_write(bp); } } brelse(bp); return addr; } bn -= NINDIRECT; // 0-256*256 // 0-255 if (bn \u003c NDBINDIRECT) { // 小于二级间接块数目 if((addr = ip-\u003eaddrs[NDIRECT + 1]) == 0){ addr = balloc(ip-\u003edev); // 分配一个磁盘块作为一级间接块 if(addr == 0) return 0; ip-\u003eaddrs[NDIRECT + 1] = addr; } uint cn = bn / NINDIRECT; // 在一级间接块的index bp = bread(ip-\u003edev, addr); a = (uint*)bp-\u003edata; if((addr = a[cn]) == 0) { addr = balloc(ip-\u003edev); // 再分配一个磁盘块作为二级间接块 if(addr){ a[cn] = addr; log_write(bp); } } brelse(bp); uint index = bn % NINDIRECT; // 在二级间接块的index bp = bread(ip-\u003edev, addr); a = (uint*)bp-\u003edata; if((addr = a[index]) == 0) { addr = balloc(ip-\u003edev); // 再分配一个磁盘块给文件 if(addr) { a[index] = addr; log_write(bp); } } brelse(bp); return addr; } panic(\"bmap: out of range\"); } 注意更改fs.h中的对应宏定义。 #define NDIRECT 11 #define NINDIRECT (BSIZE / sizeof(uint)) #define NDBINDIRECT (NINDIRECT * NINDIRECT) #define MAXFILE (NDIRECT + NINDIRECT + NDBINDIRECT) struct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT+2]; // Data block addresses }; 以及file.h中inode的address数组的大小，这里要和dinode的address数组的大小相对应，不然fs.img构建会出错。 // in-memory copy of an inode struct inode { uint dev; // Device number uint inum; // Inode number int ref; // Reference count struct sleeplock lock; // protects everything below here int valid; // inode has been read from disk? short type; // copy of disk inode short major; short minor; short nlink; uint size; uint addrs[NDIRECT+2]; }; 还有根据提示要确保itrunc释放文件的所有块，类似的进行更改，至此能通过usertests。 void itrunc(struct inode *ip) { int i, j; struct buf *bp; uint *a; for(i = 0; i \u003c NDIRECT; i++){ // 释放所有的直接块 if(ip-\u003eaddrs[i]){ bfree(ip-\u003edev, ip-\u003eaddrs[i]); ip-\u003eaddrs[i] = 0; } } if(ip-\u003eaddrs[NDIRECT]){ // 如果一级间接块存在 bp = bread(ip-\u003edev, ip-\u003eaddrs[NDIRECT]); a = (uint*)bp-\u003edata; for(j = 0; j \u003c NINDIRECT; j++){ // 释放所有的文件块 if(a[j]) bfree(ip-\u003edev, a[j]); } brelse(bp); bfree(ip-\u003edev, ip-\u003eaddrs[NDIRECT]); // 释放一级间接块 ip-\u003eaddrs[NDIRECT] = 0; } if (ip-\u003eaddrs[NDIRECT + 1]){ // 如果二级间接块存在 bp = bread(ip-\u003edev, ip-\u003eaddrs[NDIRECT + 1]); a = (uint*)bp-\u003edata; for(j = 0; j \u003c NINDIRECT; j++) { if(a[j]) { struct buf *bp2 = bread(ip-\u003edev, a[j]); uint* c = (uint*)bp2-\u003edata; for (int k = 0; k \u003c NINDIRECT; k++) { if(c[k]) bfree(ip-\u003edev, c[k]); } brelse(bp2); bfree(ip-\u003edev, a[j]); } } brelse(bp); bfree(ip-\u003edev, ip-\u003eaddrs[NDIRECT + 1]); // 释放一级间接块 ip-\u003eaddrs[NDIRECT + 1] = 0; } ip-\u003esize = 0; iupdate(ip); } Symbolic links 这个实验如果明白了符号链接的原理过程就能迎刃而解，本质上就是在path的路径创建了一个新文件（代码以inode表达），在文件里写入target路径，同时标记此文件为T_SYMLINK类型，遇到这种文件要读文件内容再一次以文件内容为path去找对应的inode，从而得到所指向文件的内容。代码具体实现： 增加symlink系统调用，在path处创建一个T_SYMLINK类型的文件，writei写入target路径为文件内容。 uint64 sys_symlink(void) { char target[MAXPATH]; cha","date":"2025-08-26","objectID":"/posts/mit-lab-fs/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:File system","uri":"/posts/mit-lab-fs/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 locks Buffer cache 如果多个进程密集使用文件系统，它们可能会争用bcache.lock，这是一个大锁，保护bcache中的buffers。修改bcache、bget、brelse使不同块中的并发查找和释放不太可能发生锁冲突。不能增加buffer的数量，也就是依旧30个buffer。不需要实现LRU缓存替换策略，但必须当缓存未命中时，能使用任何refcnt为零的buffer。这个refcnt表示当前有多少进程在使用这个buffer，每次bget()得到一个buffer后，会对这个buffer的refcnt加一，只有当refcnt变为0时，才会被回收。 $ bcachetest start test0 test0 results: --- lock kmem/bcache stats lock: kmem: #test-and-set 0 #acquire() 33030 lock: kmem: #test-and-set 0 #acquire() 28 lock: kmem: #test-and-set 0 #acquire() 73 lock: bcache: #test-and-set 0 #acquire() 96 lock: bcache.bucket: #test-and-set 0 #acquire() 6229 lock: bcache.bucket: #test-and-set 0 #acquire() 6204 lock: bcache.bucket: #test-and-set 0 #acquire() 4298 lock: bcache.bucket: #test-and-set 0 #acquire() 4286 lock: bcache.bucket: #test-and-set 0 #acquire() 2302 lock: bcache.bucket: #test-and-set 0 #acquire() 4272 lock: bcache.bucket: #test-and-set 0 #acquire() 2695 lock: bcache.bucket: #test-and-set 0 #acquire() 4709 lock: bcache.bucket: #test-and-set 0 #acquire() 6512 lock: bcache.bucket: #test-and-set 0 #acquire() 6197 lock: bcache.bucket: #test-and-set 0 #acquire() 6196 lock: bcache.bucket: #test-and-set 0 #acquire() 6201 lock: bcache.bucket: #test-and-set 0 #acquire() 6201 --- top 5 contended locks: lock: virtio_disk: #test-and-set 1483888 #acquire() 1221 lock: proc: #test-and-set 38718 #acquire() 76050 lock: proc: #test-and-set 34460 #acquire() 76039 lock: proc: #test-and-set 31663 #acquire() 75963 lock: wait_lock: #test-and-set 11794 #acquire() 16 tot= 0 test0: OK 实验所期望的输出可以看出，lock: bcache的锁争用次数变为了0，且多了13个lock: bcache.bucket，每个bucket的锁争用次数也为0。 需要以“bcache”开头为每个锁调用initlock方法。bcache是多个进程共享的，不能像kalloc一样将所有的buffer切割为每个进程所独享，很明显buffer是固定30个大小，多进程需要都能够访问的。建议使用哈希表来查找缓存中的块号，每个哈希桶持有一把锁。哈希表就是key-value对，那我的key就是hash(块号)，value就是存储的buffer，每个桶还要持有一把锁。 这些锁冲突情况是可以接受的： 两个进程去使用相同的块号。 两个进程查bcache中的buffer（遍历找对应的块号），发现没有，需要查找未使用的块（refcnt=0）进行替换。 两个进程同时使用哈希到一个bucket的块号。 hints： 哈希表大小设为13，固定值，印证了上述结果打印可以通过test。 在哈希表中维护buffer操作必须是原子性的。 删除双向链表形式的bcache且不实现LRU，在bget()中可以选择任何refcnt==0的buffer，在brelse()中也无需获取bcache.lock。 遍历哈希表以及遍历bucket的元素去查找未使用的buffer是可以的。 在驱逐一个buffer时需要持有bcache锁和每个bucket的锁。 正常来说，驱逐一个buffer会将其搬到另一个槽位上（块号hash后得到不同的key），如果在同一个槽位上需要避免死锁。 调试时需要保留全局bcache.lock的acquie/relse在bget()的开头和结尾，一旦代码正确remove这个全局锁。 使用xv6的竞争检测器来找潜在的竞争。 实验思路： 建立大小为13的哈希表，要访问的buffer块哈希到桶上，每个桶一把锁，这样就将原来的一把大锁化为了13把小锁。当缓存命中时就直接拿取对应的锁进行更新操作，很容易处理，就是拿到自己桶对应的小锁即可；但当缓存未命中时，就要遍历buffer去找最久未使用的buffer进行驱逐替换处理。这里分为两种情况，如果要驱逐的buffer和新加入的buffer哈希到一个桶上，以及未哈希到一个桶上。不在同一个桶上就涉及链表断开、重分配的操作，在同一个桶上就不需要链表操作。 实验要处理的上述代码逻辑主要体现在bget函数上，在bget中就是先获取桶A的锁，去遍历所有桶找最久未使用的buffer，然后拿到对应桶B的锁进行链表断开驱逐buffer操作，释放桶B的锁，将buffer挂在桶A上，再释放桶A的锁。这是我最开始的实现方式，运行xv6后会发现系统卡住不动了，分析是发生了死锁。这里由于是遍历所有桶，就有可能出现这种情况，即CPU1拿到桶A的锁，遍历发现要驱逐的buffer在桶B，于是申请桶B的锁，而同时CPU2拿到桶B的锁，同样遍历发现要驱逐的buffer在桶A，去申请桶A的锁。这样就是CPU1拿着桶A的锁去申请桶B的锁，CPU2拿着桶B的锁去申请桶A的锁，造成死锁。死锁的发生条件：互斥、请求保持、不可剥夺、循环等待。破坏上述4个条件之一就可以避免死锁，这里可以破坏请求保持条件，即在遍历查找要驱逐的buffer时释放自己的桶锁，后边重分配操作时再获取。 上述处理方式虽然解决了死锁的问题，但又会带来一个问题，就是在释放桶A的锁之后，拿到桶B的锁之前，这个间隙可能会同时有一个线程也在对同一个要添加的块进行bget，那么就会同样判断缓存未命中，又去重复进行驱逐替换操作，导致同一区块有多份缓存的情况。那么怎么解决这个问题呢？这里采取的办法是使用一把驱逐锁保护这个间隙，让桶A释放自己的锁之后，拿到桶B的锁之前这个过程串行化。并且在获取这个驱逐锁后再次判断是否缓存命中，这样就保证只有第一个进入的线程对该块进行了缓存操作，以下是具体的代码实现。 在buf结构体中新增lastuse成员变量，维护一个时间戳，方便查找最久未使用的buffer。 struct buf { int valid; // has data been read from disk? int disk; // does disk \"own\" buf? uint dev; uint blockno; struct sleeplock lock; uint refcnt; uint lastuse; //struct buf *prev; // LRU cache list struct buf *next; uchar data[BSIZE]; }; bio.c代码： #include \"types.h\" #include \"param.h\" #include \"spinlock.h\" #include \"sleeplock.h\" #include \"riscv.h\" #include \"defs.h\" #include \"fs.h\" #include \"buf.h\" #define NBUCKET 13 #define HASH(dev, blockno) (((dev) \u003c\u003c 27 | (blockno)) % NBUCKET) struct { struct buf buf[NBUF]; struct buf bufmap[NBUCKET]; // 哈希桶 struct spinlock bucketlocks[NBUCKET]; // 每个桶一把锁 struct spinlock evictionlocks[NBUCKET]; // 每个桶一把驱逐锁 } bcache; void binit(void) { for (int i = 0; i \u003c NBUCKET; i++) { initlock(\u0026bcache.bucketlocks[i], \"bcache_bucket_lock\"); init","date":"2025-08-26","objectID":"/posts/mit-lab-lock/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Locks","uri":"/posts/mit-lab-lock/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 cow 写时复制版的fork，思路是在fork时，uvmcopy会copy父进程的页表到子进程，建立对应的映射，但不去分配物理内存，也就是建立映射时，指向的物理内存是父进程的物理页，父子进程的虚拟地址同时映射到同一物理页。设置PTE为不可写，设置COW标志位。这样父进程或子进程其中一个试图写时，触发缺页故障，陷入内核，在usertrap中根据trap的类型处理trap，这里就是解除原PTE的映射，新分配一个物理页，建立到该物理页的映射，并且对应的PTE清除COW标志位，设为可写。 此外，还需要考虑的点是，因为同一物理页可能被多个进程映射了，那么kfree就需要判断什么时候正确释放物理页。这里使用引用计数，建立一个引用计数数组，维护每个物理页的引用计数（有多少进程虚拟地址映射到了该物理页）。kalloc时设置所分配的物理页引用计数为1，每当fork时uvmcopy复制映射就会将引用计数加一，调用kfree时先将引用计数减一，判断引用计数是否为0，从而再释放物理页面。（这里和智能指针的引用计数机制如出一辙） 按照这个思路simple的样例是可以通过的，但之后的就不会通过了。是因为多进程下对全局共享的引用计数数组需要加锁避免竞态条件下的错误。更改之后，file样例又不能通过，出现pipe() panic，是因为还需更改copyout()，这个函数是xv6实现的软件访问页表，将内核数据拷贝到用户态，这里不会触发缺页异常，而显然，如果要拷贝到用户态不可写的物理页上，就需要实现上述COW，所以在此函数增加检测COW物理页，并对应进程COW处理。至此，COW所有的test都可以通过。以下是代码实现的具体细节。 更改uvmcopy()： int uvmcopy(pagetable_t old, pagetable_t new, uint64 sz) { pte_t *pte; uint64 pa, i; uint flags; //char *mem; for(i = 0; i \u003c sz; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) panic(\"uvmcopy: pte should exist\"); if((*pte \u0026 PTE_V) == 0) panic(\"uvmcopy: page not present\"); pa = PTE2PA(*pte); // 可读可写 不可读可写 // 可读不可写 不可读不可写 // 对于可写页进行COW if (*pte \u0026 PTE_W) { *pte = *pte \u0026 ~PTE_W; *pte = *pte | PTE_COW; } // 把父进程的权限标志位复制到子进程 flags = PTE_FLAGS(*pte); // 建立到原物理页的映射 if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){ goto err; } // 原物理页的引用计数加一 acquire(\u0026refcntlock); refcnt[GETPAINDEX((uint64)pa)]++; release(\u0026refcntlock); // if((mem = kalloc()) == 0) // goto err; // memmove(mem, (char*)pa, PGSIZE); // if(mappages(new, i, PGSIZE, (uint64)mem, flags) != 0){ // kfree(mem); // goto err; // } } return 0; err: uvmunmap(new, 0, i / PGSIZE, 1); return -1; } 检查COW物理页并进行COW处理函数，注意这里va不能等于MAXVA，这是个大坑，usertests一直MAXVA样例不通过就是没有加=这个原因。 int uvmcheckcow(uint64 va) { // 虚拟地址vm对应的页面是否是COW页 if (va \u003e= MAXVA) return 0; pte_t *pte; struct proc *p = myproc(); if((pte = walk(p-\u003epagetable, va, 0)) == 0) return 0; return va \u003c p-\u003esz \u0026\u0026 (*pte \u0026 PTE_V) \u0026\u0026 (*pte \u0026 PTE_COW); } int uvmcopycow(uint64 va) { if (va \u003e= MAXVA) return -1; pte_t *pte; uint flags; uint64 pa; uint64 mem; struct proc *p = myproc(); if((pte = walk(p-\u003epagetable, va, 0)) == 0) return -1; // 分配一个新的物理页面 并把原物理页面引用计数减1 pa = PTE2PA(*pte); if ((mem = (uint64)cowalloc((void *)pa)) == 0) return -1; // 清除COW 设置为可写 flags = PTE_FLAGS(*pte); if (*pte \u0026 PTE_COW) { flags = flags \u0026 ~PTE_COW; flags = flags | PTE_W; } // 解除原页面映射 会把PTE设为0 uvmunmap(p-\u003epagetable, PGROUNDDOWN(va), 1, 0); // 这里不做dofree // 建立新页面映射 if(mappages(p-\u003epagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, flags) != 0){ return -1; } return 0; } usertrap增加对应的page fault处理： if(r_scause() == 8){ // system call if(killed(p)) exit(-1); // sepc points to the ecall instruction, // but we want to return to the next instruction. p-\u003etrapframe-\u003eepc += 4; // an interrupt will change sepc, scause, and sstatus, // so enable only now that we're done with those registers. intr_on(); syscall(); } else if((which_dev = devintr()) != 0){ // ok } else if ((r_scause() == 13 || r_scause() == 15) \u0026\u0026 uvmcheckcow(r_stval())) { // 发生页面错误 且是COW页 // 进行写时复制 if (uvmcopycow(r_stval()) == -1) setkilled(p); // 没有可用的物理内存 杀死进程 } else { printf(\"usertrap(): unexpected scause 0x%lx pid=%d\\n\", r_scause(), p-\u003epid); printf(\" sepc=0x%lx stval=0x%lx\\n\", r_sepc(), r_stval()); setkilled(p); } 引用计数需要维护的数组及锁： struct spinlock refcntlock; int refcnt[(PHYSTOP - KERNBASE) / PGSIZE]; #define GETPAINDEX(p) (p - KERNBASE) / PGSIZE // 放在defs.h 不要放在kalloc.c void kinit() { initlock(\u0026kmem.lock, \"kmem\"); initlock(\u0026refcntlock, \"refcnt\"); freerange(end, (void*)PHYSTOP); } kalloc和kfree增加引用计数判断： void kfree(void *pa) { struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa \u003c end || (uint64)pa \u003e= PHYSTOP) panic(\"kfree\"); // 页面引用计数 \u003c= 0 时释放 acquire(\u0026refcntlock); if (--refcnt[GETPAINDEX((uint64)pa)] \u003c= 0) { // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); r = (struct run*)pa; acquire(\u0026kmem.lock); r-\u003enext = kmem.freelist; kmem.freelist = r; release(\u0026kmem.lock); } release(\u0026refcntlock); } void * kalloc(void) { struct run *r; acquire(\u0026kmem.lock); r = kmem.freelist; if(r)","date":"2025-08-26","objectID":"/posts/mit-lab-cow/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Copy-on-Write Fork for xv6","uri":"/posts/mit-lab-cow/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 trap RISC-V assembly 哪些寄存器包含函数的参数？比如，哪个寄存器保存printf函数中的13？ a0、a1、a2寄存器保存函数参数。a2寄存器保存printf函数中的13。 f函数调用在哪里？g函数调用在哪里？ 没有具体的调用，实际都做了函数内联优化，直接执行x+3。 函数printf位于哪个地址？ 位于0x6bc地址。 30: 68c000ef jal 6bc \u003cprintf\u003e 在printf的jalr之后，ra寄存器的值是多少？ 0x34，执行main中下一条指令。 代码输出是什么？如果RISC-V是大端序，你会将i设为多少才能得到相同的输出？你会将57616改为其他不同的值吗？ unsigned int i = 0x00646c72; printf(\"H%x Wo%s\", 57616, (char *) \u0026i); 输出：HE110 World。强转为char*是指从变量i的地址处按字节读取变量i，如果不取i的地址，那就是在0x00646c72地址处按字节读取变量i，这是错的。当前是小端序，低字节低地址，所以按0x72、0x6c、0x64、0x00的顺序读，转换为rld。若是大端序，则将i设为0x726c6400才能得到相同的输出。57616不需要变，因为对应的十六进制不会改变。 6. 代码“y=”之后会打印什么？ printf(\"x=%d y=%d\", 3); 打印的是不确定的值，printf会错误读取栈上某个随机的值，是未定义行为。 Backtrace error发生后定位其函数调用链进行回溯。编译器生成了包含当前调用链每个函数栈帧的机器码，每个栈帧包含返回地址和指向调用它的栈帧指针。s0寄存器包含一个指向当前栈帧的指针。讲义中的这张图清晰的展示了栈帧结构，从s0寄存器得到栈帧指针fp，然后在fp-8的位置是返回地址，在fp-16的位置是保存的上一个调用它的fp。 栈帧结构 思路就是循环遍历fp的值，从而读取对应栈帧的返回地址并打印。什么时候循环停止，提示说所有栈帧都在同一页上，所以超出该页范围即停止，实现代码： void backtrace(void) { uint64 fp = r_fp(); uint64 fpmin = PGROUNDDOWN(fp); uint64 fpmax = PGROUNDDOWN(fp) + PGSIZE; while (fp \u003c fpmax \u0026\u0026 fp \u003e= fpmin) { printf(\"%p\\n\", (uint64 *)*(uint64 *)(fp-8)); fp = *(uint64*)(fp-16); } } Alarm 添加一个新的sigalarm(interval, handler)系统调用，应用程序调用sigalarm(n, fn)时，则在程序消耗CPU时间的每n个ticks时调用应用程序函数fn。这里alarmtest划分了四个test，一个一个来做。 首先就是添加sigalarm和sigreturn两个系统调用，参照syscall实验，主要是得到用户参数，传递到proc结构中，新建的计数变量ticks和中断处理程序handler函数指针。sys_sigreturn在这个test先直接return 0;即可。 uint64 sys_sigalarm(void) { int ticks; uint64 handler_addr; argint(0, \u0026ticks); argaddr(1, \u0026handler_addr); myproc()-\u003eticks = ticks; myproc()-\u003ehandler = (void (*)())handler_addr; return 0; } 因为每次定时器中断都会在usertrap中进行处理，所以在处理函数中增加统计tick数量，若达到ticks即执行用户程序handler，这里需要将sepc寄存器的值设为handler的地址，从而在trap返回时指向handler程序的地址。这样就完成了test0。 if(which_dev == 2) { p-\u003epassticks ++; if (p-\u003eticks \u003e 0 \u0026\u0026 p-\u003epassticks == p-\u003eticks) { p-\u003epassticks = 0; // 返回用户程序handler地址处 p-\u003etrapframe-\u003eepc = (uint64)p-\u003ehandler; } yield(); } test1要求在从handler返回时，要恢复中断之前的寄存器的值，类比于trapframe保存寄存器的值，这里新增另一个atrapframe来保存中断之前的寄存器，在sigreturn时再进行恢复。 if(which_dev == 2) { p-\u003epassticks ++; if (p-\u003eticks \u003e 0 \u0026\u0026 p-\u003epassticks == p-\u003eticks) { // 把定时器中断前的寄存器保存到atrpframe中 memmove(p-\u003eatrapframe,p-\u003etrapframe,sizeof(struct trapframe)); p-\u003epassticks = 0; // 返回用户程序handler地址处 p-\u003etrapframe-\u003eepc = (uint64)p-\u003ehandler; } yield(); } uint64 sys_sigreturn(void) { struct proc* p = myproc(); // 恢复定时器中断前的寄存器 如果不恢复此时寄存器值是用户handler程序的寄存器值 原返回程序的寄存器值被覆盖所以寄 memmove(p-\u003etrapframe,p-\u003eatrapframe,sizeof(struct trapframe)); return 0; } test2要求不能重复执行中断处理程序，即在进程结构体中增加标志位表示当前是否在执行handler，初始为0，在调用handler时设为1，sigreturn时设为0。 if(which_dev == 2) { p-\u003epassticks ++; if (p-\u003eticks \u003e 0 \u0026\u0026 p-\u003epassticks == p-\u003eticks \u0026\u0026 p-\u003ealarmflag == 0) { // 把定时器中断前的寄存器保存到atrpframe中 memmove(p-\u003eatrapframe,p-\u003etrapframe,sizeof(struct trapframe)); p-\u003epassticks = 0; // 返回用户程序handler地址处 p-\u003etrapframe-\u003eepc = (uint64)p-\u003ehandler; // 此进程在执行handler p-\u003ealarmflag = 1; } yield(); } uint64 sys_sigreturn(void) { struct proc* p = myproc(); // 恢复定时器中断前的寄存器 如果不恢复此时寄存器值是用户handler程序的寄存器值 原返回程序的寄存器值被覆盖所以寄 memmove(p-\u003etrapframe,p-\u003eatrapframe,sizeof(struct trapframe)); p-\u003ealarmflag = 0; return 0; } test3要求sigreturn返回时a0寄存器的值要是中断之前的a0寄存器的值，sigreturn返回0会将a0设为0，所以这里直接返回保存的trapframe中的a0的值。 uint64 sys_sigreturn(void) { struct proc* p = myproc(); // 恢复定时器中断前的寄存器 如果不恢复此时寄存器值是用户handler程序的寄存器值 原返回程序的寄存器值被覆盖所以寄 memmove(p-\u003etrapframe,p-\u003eatrapframe,sizeof(struct trapframe)); p-\u003ealarmflag = 0; return p-\u003etrapframe-\u003ea0; } 以上所有在proc.h中新增的结构体成员，其生命周期需要在proc.c中添加对应代码，主要体现在allocproc.c函数及freeproc.c函数中，这里没有给出。至此，alarmtest所有test都通过。 ","date":"2025-08-26","objectID":"/posts/mit-lab-traps/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Traps","uri":"/posts/mit-lab-traps/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 pgtbl Speed up system calls 加速系统调用：在内核空间和用户空间共享只读区域的数据，这样不需要跨内核，减少了用户态和内核态切换的开销。这里就是指频繁的系统调用会带来内核态与用户态的切换开销，为了减少这个开销，做法有： 可以减少系统调用的次数（如设置用户buffer和内核buffer缓存系统调用请求数据，到达一定量时再一次性执行系统调用，典型的就是read()、write()） 使用协程，用户态的线程，不会跨核 本实验做法，内核空间与用户空间虚拟映射到同一个只读的物理页面，避免内核态和用户态切换（Linux称为VDSO） 本实验通过fork子进程，实现用户态程序ugetpid()和系统调用getpid()一样的效果。ugetpid()实现已经给出来，是读虚拟地址USYSCALL处的内容。所以需要在fork创建子进程时（内核态），向USYSCALL地址（用户态虚拟地址空间）写入struct usyscall，对应分配了一个物理页，那么之后用户态ugetpid就能直接在USYSCALL处读数据，而不跨核。 int ugetpid(void) { struct usyscall *u = (struct usyscall *)USYSCALL; return u-\u003epid; } 代码实现参照trapframe生命周期代码，在进程分配trapframe内存、建立trapframe映射、释放trapframe内存这三部分，实现usyscall，存入当前进程的pid。 // kernel/proc.c // static struct proc* allocproc(void) // Allocate a trapframe page. if((p-\u003etrapframe = (struct trapframe *)kalloc()) == 0){ freeproc(p); release(\u0026p-\u003elock); return 0; } // Allocate a usyscall page if ((p-\u003eusyscall = (struct usyscall *)kalloc()) == 0) { freeproc(p); release(\u0026p-\u003elock); return 0; } p-\u003eusyscall-\u003epid = p-\u003epid; // kernel/proc.c // pagetable_t proc_pagetable(struct proc *p) // map the trapframe page just below the trampoline page, for // trampoline.S. if(mappages(pagetable, TRAPFRAME, PGSIZE, (uint64)(p-\u003etrapframe), PTE_R | PTE_W) \u003c 0){ uvmunmap(pagetable, TRAMPOLINE, 1, 0); uvmfree(pagetable, 0); return 0; } if(mappages(pagetable, USYSCALL, PGSIZE, (uint64)(p-\u003eusyscall), PTE_R | PTE_U) \u003c 0){ uvmunmap(pagetable, TRAPFRAME, 1, 0); uvmunmap(pagetable, TRAMPOLINE, 1, 0); uvmfree(pagetable, 0); return 0; } // kernel/proc.c static void freeproc(struct proc *p) { if(p-\u003etrapframe) kfree((void*)p-\u003etrapframe); p-\u003etrapframe = 0; if(p-\u003eusyscall) kfree((void*)p-\u003eusyscall); p-\u003eusyscall = 0; if(p-\u003epagetable) proc_freepagetable(p-\u003epagetable, p-\u003esz); p-\u003epagetable = 0; p-\u003esz = 0; p-\u003epid = 0; p-\u003eparent = 0; p-\u003ename[0] = 0; p-\u003echan = 0; p-\u003ekilled = 0; p-\u003exstate = 0; p-\u003estate = UNUSED; } 总结： 这个实验深刻认识到，内核空间是可以操作用户空间的，不要有固有思维内核空间只操作内核，用户空间只操作用户程序。是内核的权限高，可以操作任何代码，这里就可以拿到p-\u003etrapframe，你会说这不是进程用户空间的吗？实际上kernel代码是可以操作这页的。所以这个实验开始困惑的是用户空间和内核共享只读区域是什么，这里的答案就是用户空间内核都可以读写，是将内核操作的结果放到了用户可以读的用户地址空间，仅此而已，不是什么用户虚拟地址、内核虚拟地址映射到一个物理内存，这里没有涉及内核虚拟地址。 回答最后可以speed up的syscall，很明显就是只涉及读取内核态的结果的系统调用，不对内核态进行修改，如getpid()、uptime()、getuid()、sbrk(0)等。再次强调，这里的加速是指不进行跨核。 Print a page table 编写一个函数打印页表的内容。xv6是三级页表，打印的形式是..2级页表PTE，.. ..2级PTE下的所有1级PTE，.. .. ..1级PTE下的所有0级PTE。根据提示，可以参考freewalk递归遍历所有PTE。 freewalk这段代码递归实现了所有页表的释放，是理解递归实现的很好示例，它的出口就是到了叶子页（L0级页表）不再递归，然后交由上层递归kfree本级页表。 void freewalk(pagetable_t pagetable) { // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i \u003c 512; i++){ pte_t pte = pagetable[i]; // PTE有效 \u0026\u0026 PTE不是叶子页（没有RWX位） if((pte \u0026 PTE_V) \u0026\u0026 (pte \u0026 (PTE_R|PTE_W|PTE_X)) == 0){ // this PTE points to a lower-level page table. uint64 child = PTE2PA(pte); freewalk((pagetable_t)child); pagetable[i] = 0; } else if(pte \u0026 PTE_V){ // freewalk之前要uvmunmap叶子页 panic(\"freewalk: leaf\"); } } kfree((void*)pagetable); // kfree的位置很重要 } 参考freewalk实现的，就是用递归深度控制遍历层级，这里遍历的depth从0到2，对应着2、1、0三级页表（freewalk的depth相当于是从0到1），然后去打印对应的二进制就行。注意使用void *转换，能输出对应%p格式，以及这里的虚拟地址，是依赖上层pte的基地址的，所以也需要传递上层pte的虚拟地址到下层递归中。 void vmprintsub(pagetable_t pagetable, int depth, uint64 base) { if (depth \u003e 2) return; for(int i = 0; i \u003c 512; i++){ pte_t pte = pagetable[i]; if(pte \u0026 PTE_V){ uint64 child = PTE2PA(pte); printf(\"..\"); // 2 .. int count = depth; while (count--) { // 1 .. .. printf(\" ..\"); // 0 .. .. .. } uint64 va = base + (i \u003c\u003c PXSHIFT(2-depth)); printf(\"%p: pte %p pa %p\\n\", (void *)va, (void *)pte, (void *)PTE2PA(pte)); vmprintsub((pagetable_t)child, depth + 1, va); } } } void vmprint(pagetable_t pagetable) { // your code here printf(\"page table %p\\n\", pagetable); // 打印页表项 // ..va: pte pa // .. ..va: pte pa // .. .. ..va: pte pa vmprintsub(pagetable, 0, 0); } Use superpages 使用超级页，RISC-V分页硬件支持2M的页面，比普通4KB更大的页面称为超级页，2M的页面称为兆页面。OS设置L1级的PTE中的PTE_V和PTE_R位，并设置物理页号为指向2MB物理内存的区域起始位置。使用超级页可以减少页表使用的物理内存量，并可以减少TLB缓冲中未命中的次数，从而大幅提升性能。 题目中提示了，通过设置L1级的PTE直接映射到物理地址，因为2M的页面对应的就是2^(9+12)。所以代码就是在所有有关分配超级页内存时页表相关的操作。 首先在kalloc.c","date":"2025-08-26","objectID":"/posts/mit-lab-pgtbl/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Page tables","uri":"/posts/mit-lab-pgtbl/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 syscall System call tracing 实现系统调用trace，trace接受一个参数（整数掩码），如果系统调用号在掩码中设置，需要修改xv6内核，使其在每个系统调用即将返回时打印一行（进程ID、系统调用名称、返回值）。 用户程序需要调用system call，那么所提供的system call的函数声明在user.h，而实际的定义实现却是在kernel中，要知道用户态的代码和内核态的代码是隔离的，那么是用户程序是如何调用内核的代码实现的呢？答案是user.pl，user.pl是在用户空间的脚本，作为桥梁，在makefile时生成user.S，这个汇编代码include了syscall.h，syscall.h声明了系统调用号（对应kernel system call 函数），把这个编号放到寄存器a7中，然后ecall进内核，在ecall时会将a7寄存器的值存到trapframe中，从而传到内核中，这样内核从trapframe中加载a7寄存器的值，从而调用对应的system call。 本实验是要实现system call，但是需要shell去测试验证，所以依旧需要先实现一个用户程序trace，在这个程序中调用system call，这里源代码已经提供了trace用户程序。需要增加的是上述关于调用内核system call的操作。 ecall会调用trampoline.S代码，这段代码会执行内核的usertrap函数，当r_scause() == 8时会调用syscall()，syscall()会取a7寄存器的值，然后调用对应syscall函数数组索引的系统调用函数，返回值存在trapframe的a0寄存器里。所以就在syscall()里判断是否是要跟踪的系统调用号，如果是，打印进程ID、系统调用名称和返回值。 整体实现过程就是：1. 添加新增的与syscall有关的函数存根、声明等。2. 在syscall()中拿到用户参数掩码，与当前系统调用号进行按位与运算，如果是1则打印。3. 注意根据提示，增加系统调用名称数组以及在fork时将父进程的mask也copy到子进程中。 主要的代码实现： // kernel/syscall.c // ... // Prototypes for the functions that handle system calls. extern uint64 sys_fork(void); extern uint64 sys_exit(void); extern uint64 sys_wait(void); extern uint64 sys_pipe(void); extern uint64 sys_read(void); extern uint64 sys_kill(void); extern uint64 sys_exec(void); extern uint64 sys_fstat(void); extern uint64 sys_chdir(void); extern uint64 sys_dup(void); extern uint64 sys_getpid(void); extern uint64 sys_sbrk(void); extern uint64 sys_sleep(void); extern uint64 sys_uptime(void); extern uint64 sys_open(void); extern uint64 sys_write(void); extern uint64 sys_mknod(void); extern uint64 sys_unlink(void); extern uint64 sys_link(void); extern uint64 sys_mkdir(void); extern uint64 sys_close(void); extern uint64 sys_trace(void); // 在外部.c文件中的sys_trace // An array mapping syscall numbers from syscall.h // to the function that handles the system call. static uint64 (*syscalls[])(void) = { // 函数指针数组 [SYS_fork] sys_fork, [SYS_exit] sys_exit, [SYS_wait] sys_wait, [SYS_pipe] sys_pipe, [SYS_read] sys_read, [SYS_kill] sys_kill, [SYS_exec] sys_exec, [SYS_fstat] sys_fstat, [SYS_chdir] sys_chdir, [SYS_dup] sys_dup, [SYS_getpid] sys_getpid, [SYS_sbrk] sys_sbrk, [SYS_sleep] sys_sleep, [SYS_uptime] sys_uptime, [SYS_open] sys_open, [SYS_write] sys_write, [SYS_mknod] sys_mknod, [SYS_unlink] sys_unlink, [SYS_link] sys_link, [SYS_mkdir] sys_mkdir, [SYS_close] sys_close, [SYS_trace] sys_trace, }; char* sysnames[] = { [SYS_fork] \"fork\", [SYS_exit] \"exit\", [SYS_wait] \"wait\", [SYS_pipe] \"pipe\", [SYS_read] \"read\", [SYS_kill] \"kill\", [SYS_exec] \"exec\", [SYS_fstat] \"fstat\", [SYS_chdir] \"chdir\", [SYS_dup] \"dup\", [SYS_getpid] \"getpid\", [SYS_sbrk] \"sbrk\", [SYS_sleep] \"sleep\", [SYS_uptime] \"uptime\", [SYS_open] \"open\", [SYS_write] \"write\", [SYS_mknod] \"mknod\", [SYS_unlink] \"unlink\", [SYS_link] \"link\", [SYS_mkdir] \"mkdir\", [SYS_close] \"close\", [SYS_trace] \"trace\", }; void syscall(void) { int num; struct proc *p = myproc(); num = p-\u003etrapframe-\u003ea7; if(num \u003e 0 \u0026\u0026 num \u003c NELEM(syscalls) \u0026\u0026 syscalls[num]) { // Use num to lookup the system call function for num, call it, // and store its return value in p-\u003etrapframe-\u003ea0 p-\u003etrapframe-\u003ea0 = syscalls[num](); if ((p-\u003emask \u0026 (1 \u003c\u003c num)) == (1 \u003c\u003c num)) { // 代码中对uint64会赋值-1 导致打印a0时出现问题 make grade Test trace children过不了 if (p-\u003etrapframe-\u003ea0 == -1) { // 这里单独增加了判断 printf(\"%d: syscall %s -\u003e %d\\n\", p-\u003epid, sysnames[num], -1); }else { printf(\"%d: syscall %s -\u003e %lu\\n\", p-\u003epid, sysnames[num], p-\u003etrapframe-\u003ea0); } } } else { printf(\"%d %s: unknown sys call %d\\n\", p-\u003epid, p-\u003ename, num); p-\u003etrapframe-\u003ea0 = -1; } } // kernel/sysproc.c // ... // 和进程相关的系统调用实现放在此文件里 uint64 sys_trace(void) { // 注意这里的传入参数全是void // 因为使用argint内核函数来获取放到trampframe中的寄存器的值 // 用户程序trace有传入一个参数 int int n; argint(0, \u0026n); // 提取第一个用户参数 struct proc *p = myproc(); p-\u003emask = n; return 0; } // kernel/proc.c // ... // Create a new process, copying the parent. // Sets up child kernel stack to return as if from fork() system call. int fork(void) { int i, pid; struct proc *np; struct proc *p = myproc(); // Allocate process. if((np =","date":"2025-08-26","objectID":"/posts/mit-lab-syscall/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:System calls","uri":"/posts/mit-lab-syscall/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 util Boot xv6 首先是搭建环境，这里使用CLion + WSL2的环境进行开发，git克隆代码仓库： git clone git://g.csail.mit.edu/xv6-labs-2024 将项目放到了github中进行版本管理，remove掉了原远程仓库，ssh连接到自己新建的仓库，push上去就可以。 git remote remove origin 安装 lab tools ，我这里使用的是Ubuntu-22.04，安装的QEMU是6.0系列，这里用不了。提示说需要QEMU 7.2+、GDB 8.3+，对应的Ubuntu-24之后，汗流浃背了。搜了一下WSL2还能同时安装不同版本的Ubuntu，于是下了Ubuntu-24.04，make qemu后正常运行，使用ctrl-a x退出终端。 此外，CLion在make后会提示没有all这个目标的构建错误，使用WSL为默认的编译工具链，并修改makefile设置，将target设为qemu即可，解析完后此时是能实现代码跳转的。 CLion配置 配置CLion gdb debug参考这个链接的方法。 注意请勿随意修改.git目录和原origin指向的链接，所以上述remove原origin仓库是不合适的，应该是添加github仓库，然后分支push，这样就能将原仓库代码保存到自己github上进行版本管理。 git remote add github git@github.com:UAreFree/xv6-labs-2024.git git push github util:util Sleep 题意：实现用户级别的程序sleep，即暂停指定的时间，这里的时间刻度是定时器中断时间间隔。 提示：第一反应是这个定时器中断时间间隔怎么实现，这里提示使用sleep系统调用，那就不需要考虑了，整体应该是考察如何实现user中的shell程序。 在Makefile中添加用户级程序，$U 是用户程序的目录，每个程序对应一个.c或.S文件，经过编译后生成对应的.o文件。 UPROGS=\\ $U/_cat\\ $U/_echo\\ $U/_forktest\\ $U/_grep\\ $U/_init\\ $U/_kill\\ $U/_ln\\ $U/_ls\\ $U/_mkdir\\ $U/_rm\\ $U/_sh\\ $U/_stressfs\\ $U/_usertests\\ $U/_grind\\ $U/_wc\\ $U/_zombie\\ $U/_sleep\\ 新建sleep.c文件，代码实现： #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" int main(int argc, char *argv[]) { // 用户忘记传参数 打印错误信息 if(argc \u003c= 1){ fprintf(2, \"usage: sleep [int]\\n\"); exit(1); } // 使用sleep系统调用 sleep(atoi(argv[1])); exit(0); } 运行./grade-lab-util sleep进行代码测试。 Pingpong 依旧用户程序，通过一对管道在两个进程间“乒乓”发送一个字节，父进程向子进程发送一个字节，子进程打印\": received ping\"，并将该字节通过管道写入父进程，然后退出；父进程从子进程读取该字节，打印\": received pong\"。 pipe管道p[0]是读，p[1]是写，在读管道时，关闭写端；在写管道时，关闭读端。这里用了两个管道（一对），对每个管道按照上述原则进行读写就可以了，以及注意exit和wait在父子进程中的使用。 新建pingpong.c文件，代码实现： #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" int main(int argc, char *argv[]) { int pipe1[2], pipe2[2]; pipe(pipe1); pipe(pipe2); char c = 'p'; // 父进程向子进程发送一个字节 通过管道 int pid = fork(); if (pid \u003c 0) { fprintf(2, \"fork failed\\n\"); exit(1); } if (pid == 0) { // 从pipe1里读 close(pipe1[1]); read(pipe1[0], \u0026c, 1); close(pipe1[0]); fprintf(1,\"%d: received ping\\n\", getpid()); // 向pipe2里写 close(pipe2[0]); write(pipe2[1], \u0026c, 1); close(pipe2[1]); exit(0); }else { // 向pipe1里写p close(pipe1[0]); write(pipe1[1], \u0026c, 1); close(pipe1[1]); // 等子进程退出 wait(0); // 从pipe2里读p close(pipe2[1]); read(pipe2[0], \u0026c, 1); close(pipe2[0]); fprintf(1,\"%d: received pong\\n\", getpid()); } // 子进程向父进程发送上述同一个字节 通过另一个管道 exit(0); } Find 查找目录树中所有具有指定名称的文件，向子目录递归查询。 示例是输入了两个参数，第一个是“.”，是根目录；第二个是要查找的文件名，也就是在所有目录中查找。下边是阅读ls实现的代码注释，按照这个思路，find就是ls目录树所有文件，找到与文件名相同的文件路径，所以就是当ls遇到目录时继续ls进行递归实现，再加上文件名判断打印即可。 首先阅读ls代码，使用open()得到文件描述符fd，再使用fstat()将fd的文件信息存入st结构体中。根据st中所示文件的类型，进行对应处理，如是文件直接打印，如是目录还需打印目录项所有条目。 void ls(char *path) { char buf[512], *p; int fd; struct dirent de; struct stat st; if((fd = open(path, O_RDONLY)) \u003c 0){ fprintf(2, \"ls: cannot open %s\\n\", path); return; } if(fstat(fd, \u0026st) \u003c 0){ // fstat系统调用 把fd文件的信息存入st结构体中 fprintf(2, \"ls: cannot stat %s\\n\", path); close(fd); return; } switch(st.type){ case T_DEVICE: case T_FILE: printf(\"%s %d %d %d\\n\", fmtname(path), st.type, st.ino, (int) st.size); // 文件直接打印 路径名 文件类型 inode号 文件大小 break; case T_DIR: if(strlen(path) + 1 + DIRSIZ + 1 \u003e sizeof buf){ printf(\"ls: path too long\\n\"); break; } strcpy(buf, path); // 路径名copy到buf中 p = buf+strlen(buf); // 指向buf末尾 *p++ = '/'; // 当前目录末尾+/ 继续指向末尾 while(read(fd, \u0026de, sizeof(de)) == sizeof(de)){ // 目录也是文件 可以read 目录里包含一系列dirent结构体数据（存有inode及对应的路径名） if(de.inum == 0) continue; memmove(p, de.name, DIRSIZ); // 复制目录项的路径名到p中 p[DIRSIZ] = 0; if(stat(buf, \u0026st) \u003c 0){ printf(\"ls: cannot stat %s\\n\", buf); continue; } printf(\"%s %d %d %d\\n\", fmtname(buf), st.type, st.ino, (int) st.size); // 打印该文件信息 } break; } close(fd); } 新建find.c文件，代码实现： #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" #include \"kernel/fs.h\" #include \"kernel/fcntl.h\" void lsa(char *path, char* filename) { char buf[512], *p; int fd; struct dirent de; struct stat st; if((fd = open(path, O_RDONLY)) \u003c 0){ fprintf(2, \"ls: cannot open %s\\n\", path); return; } if(fstat(fd, \u0026st) \u003c 0){ // fstat系统调用 把fd文件","date":"2025-08-26","objectID":"/posts/mit-lab-util/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Xv6 and Unix utilities","uri":"/posts/mit-lab-util/"},{"categories":null,"content":"about me","date":"2025-08-25","objectID":"/about/","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"社会稳定一份子 退耕还林倡导者 控制科学自由人 电子制作门外汉 智能小车梦想家 三昧真火炼丹师 C++受虐狂 Linux键盘侠 EarthOnline差等生 ","date":"2025-08-25","objectID":"/about/:0:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":["操作系统"],"content":"本文是作者自己总结的《Operating Systems:Three Easy Pieces》读书笔记","date":"2025-08-23","objectID":"/posts/ostep-note/","tags":["操作系统导论","读书笔记"],"title":"操作系统导论读书笔记","uri":"/posts/ostep-note/"},{"categories":["操作系统"],"content":"《Operating Systems:Three Easy Pieces》是Arpaci-Dusseau教授夫妇所著，向Richard Feynman的《费曼物理学讲义》致敬。本书围绕虚拟化、并发和持久化三个主题展开。“教育过程的真正意义在于：前进，学习许多新的和引人入胜的主题，通过学习不断成熟，最重要的是，找到能为你点火的东西。” 操作系统介绍 冯诺依曼计算模型的基本概念，运行的程序会做一件非常简单的事情：执行指令（CPU是一个无情执行指令的机器，程序是状态机）。处理器不断取指（内存中）、译码、执行、写回。操作系统是一类软件，负责让程序运行变得容易。 操作系统主要利用虚拟化技术将物理资源转换为虚拟形式，虚拟化CPU将单个CPU转换为看似无限数量的CPU，从而让许多程序看似同时运行；虚拟化内存，每个进程访问自己的私有虚拟地址空间，操作系统以某种方式映射到机器的物理内存上，物理内存是操作系统管理的共享资源。 并发问题在同时（并发地）处理很多事情时出现且必须解决。多线程程序对共享计数器必须进行处理，对于这种增加计数值时需要三条指令：1. 将计数器的值从内存加载到寄存器；2. 递增值；3. 将寄存器的值保存回内存。这三条指令不是原子执行（所有指令一次性执行）的，所以会有怪事发生。 断电或系统崩溃，内存中的数据就会丢失，需要硬件（磁盘）和软件（文件系统）持久地存储数据。操作系统不会虚拟化磁盘而是假设用户经常需要共享文件中的信息。文件系统部分的系统调用有open()、write()、close()等，首先确定新数据放到磁盘哪个位置，然后在文件系统所维护的各种结构中进行记录，需要向底层存储设备发出I/O请求。出于性能方面考虑，首先延迟写操作一段时间，将其批量分组为较大的组。为了处理写入期间系统崩溃的问题，文件系统包含某种复杂的写入协议，如日志或写时复制，仔细排序写入磁盘的操作，以确保恢复到合理的状态。 设计一个操作系统的目标：1. 建立一些抽象；2. 提供高性能；3. 在OS和应用程序之间提供隔离；4. 可靠。 虚拟化 进程 进程：运行中的程序，OS为运行的程序所提供的抽象。进程的机器状态：内存和寄存器。所有OS需要提供的API：创建、销毁、等待、其他控制、状态。OS运行程序必须做的第一件事：将磁盘中的代码和所有静态数据加载到内存中，加载到进程的地址空间中。OS在加载时也是惰性加载，仅在程序执行期间需要加载的代码和数据才会加载。第二件事，为程序运行时栈分配内存空间，也可能分配堆空间。还会执行一些其他初始化任务，特别是I/O相关。默认每个进程有三个文件描述符fd，标准输入、输出和错误。 进程状态：运行、就绪、阻塞。运行和就绪可以相互转换，通过CPU调度和取消调度。阻塞（被某种事件阻塞）需要等待某种事件发生。初始：进程刚创建；最终：已退出但尚未清理（僵尸状态）。这里僵尸进程是指子进程先于父进程退出后，父进程没有释放子进程的PCB，如果父进程不结束，那么一直保持僵尸进程状态，占用系统资源，如果父进程结束了，init进程会自动接手子进程并收尸。与之相对的是孤儿进程，孤儿进程是父进程先于子进程退出，那么会被init进程收养，不会占用系统资源。僵尸进程的处理办法：1. 杀死父进程，大多数情况不可取。2. SIGCHLD信号处理，实际上当子进程终止时，内核会向它的父进程发送一个SIGCHLD信号，父进程可以选择忽略（默认）还是提供处理函数（调用wait或waitpid函数释放子进程占用的资源）。 fork()：这个系统调用确实比较奇怪，初学时很难接受是怎么做到同一个程序同时运行相同的代码的。fork的话会创建一个子进程，子进程copy了父进程（内存和寄存器值），但是返回值不同（子进程返回值为0）。fork的子进程不会执行fork之前的代码。 wait()：父进程可以延迟执行，调用wait等待子进程执行完毕再执行，这个系统调用也会回收子进程的资源。 exec()：可以让子进程执行与父进程不同的程序，exec会从可执行程序中加载代码和静态数据，并覆盖自己的代码段和静态数据，堆、栈及其他内存空间也会被重新初始化，然后OS就会执行该程序，将参数通过argv传给该进程。这里说明exec是完全转头调用所指定的可执行程序。这种分离fork和exec的做法在构建shell时非常有用，因为这给了shell在fork之后exec之前运行代码的机会。比如shell作为主进程在fork了一个子进程后通过exec执行wc系统调用统计文件字符数，可以使用重定向将输出从标准输出改为某个文件，而这实现原理就是在fork之后exec之前关闭标准输出的文件描述符，打开文件，新打开的文件将成为第一个可用的文件描述符，从而向文件中写入。 受限直接执行 时分共享CPU实现虚拟化：看起来同时运行多个进程。在保持控制权的同时获得高性能是构建操作系统的主要挑战之一。 用户态和内核态切换：划分用户空间和内核空间区分受限操作执行，内核空间可以做everything，执行受限的指令；用户空间需要执行这些受限的指令就需要执行系统调用，而要执行系统调用就必须执行trap指令，该指令同时跳转入内核，从而执行特权指令，完成后，OS调用一个trap返回指令返回到用户空间。这涉及到内核态到用户态的转换，在trap时处理器需要将一些寄存器保存到每个进程的内核栈上，trap返回时从栈中弹出这些值。open()系统调用和C的过程调用看起来差不多，是的，open()是一个过程调用但内部有trap指令，在执行open()时会用汇编处理参数和返回值以及执行trap指令。然而需要考虑的一点是，trap进内核的代码地址不能是任意的（出于安全考虑），那么每个系统调用或中断在trap进内核后都会被指定对应的处理程序地址，这个对应关系保存在trap表中。 进程间切换：OS作为一个进程（类似的）也要运行在CPU上，如果子进程去执行一个死循环而不主动让出CPU的控制权那么OS就失去了CPU的控制权，唯一办法是重启大法。这显然不合理，需要其他办法获得CPU的控制权。时钟中断，和系统调用一样，必须有对应的中断处理函数，以及在系统开始时也必须启动时钟。 上下文切换：系统开启时钟中断，中断产生后，发生进程切换，也就有进程间的上下文切换。上下文切换就是保存当前执行进程的寄存器的值，恢复即将执行进程的寄存器的值。为了保存当前进程的上下文，OS会执行一些汇编代码来保存通用寄存器、程序计数器、内核栈指针，然后恢复通用寄存器、程序计数器、切换内核栈，以供即将执行进程使用。通过切换栈，内核在进入切换代码调用时，是一个进程的上下文，返回时，是另一个进程的上下文。当OS最终执行从陷阱返回指令时，即将执行的进程变成了当前运行的进程，至此上下文切换完成。 这里可以这样理解，进程进行上下文切换，那么什么是上下文？上下文肯定是描述进程的东西，那么进程由什么来描述？就是PCB，简单来说主要包括寄存器和内存，每个进程有自己运行时寄存器的值，每个进程有自己看起来独有的内存空间（包括栈和堆）。那么上下文就包括这些，切换的也就是寄存器的值、内存空间。具体过程就是保存正在执行的进程A的寄存器（通用寄存器、程序计数器、栈指针等）到内核空间的一块内存中（cpu_context结构体保存），把即将要执行的进程B所保存的上下文内存块中（cpu_context）的值恢复到寄存器中，这是硬件上下文的切换；也别忘了还有内存空间的切换，具体就是切换进程B的pgd（页全局目录的虚拟地址）所对应的页全局目录的物理地址到页表基址寄存器中，当访问用户空间时MMU会通过页表基址寄存器来遍历页表得到物理地址。 而下边表中保存和恢复了两次寄存器，容易误导。实际上下文切换的就是调用switch()例程的一次寄存器的保存和恢复，而另一次是中断所伴随的，即不论是否是上下文切换的场景，只要中断产生都会将用户态的寄存器的值保存到内核栈，然后恢复，这里称为保存现场，不要与进程上下文搞混。 受限直接执行协议（时钟中断） 所谓的受限直接执行的含义是：OS在启动时设置陷阱处理程序并启动时钟中断，然后仅在受限模式下运行进程，以此为CPU提供宝宝防护。这是虚拟化CPU的基本机制。 进程调度 调度指标：周转时间，周转时间=完成时间-到达时间，这是一个性能指标。另一个有趣的目标是公平，两者往往是矛盾的。 先进先出 FIFO 最短任务优先 SJF 同时到达的任务先运行短时间的 最短完成时间优先 STCF 向SJF添加抢占则为最短完成时间优先，随时到达随时评估 新的调度指标：响应时间，响应时间=首次运行时间-到达时间。其实就对应了任务的响应优先级，考虑到了某些运行时间长的任务一直得不到响应的情况。 轮转 RR 时间片轮转，划分固定的时间片给每个任务，按任务队列顺序依次占有CPU 时间片时间越短响应时间越好，但是需要摊销上下文切换成本，让上下文切换时间占比不要太高 上下文切换的成本不仅来自上述所说的操作，在CPU cache、TLB等硬件上建立了大量的状态，这些切换也可能导致显著的性能成本。 这样会导致周转时间指标很差，因为作为一个公平的策略 结合I/O 这里主要是指实际场景中可以利用I/O操作耗时的间隙进行CPU操作从而更好利用CPU 多级反馈队列 MLFQ Corbato提出，获得图灵奖 需要联合考虑周转时间和响应时间两个指标 是用历史经验预测未来的一个经典例子，以史为鉴，更多的是基于行为制定规则，而不是先验知识 划分不同优先级的不同队列，每个队列之间用RR，I/O密集型的优先级高（占用CPU时间短），CPU密集型的优先级低（占用CPU时间长），优先级大的先运行，","date":"2025-08-23","objectID":"/posts/ostep-note/:0:0","tags":["操作系统导论","读书笔记"],"title":"操作系统导论读书笔记","uri":"/posts/ostep-note/"},{"categories":["测试分类"],"content":"这是一个用于测试的文章，帮助验证Hugo-FixIt配置的正确性。","date":"2025-01-25","objectID":"/posts/article-test/","tags":["测试","Hugo","FixIt"],"title":"文章测试","uri":"/posts/article-test/"},{"categories":["测试分类"],"content":"标题 h3 标题 h4 标题 h5 标题 h6 标题 段落 这是第一段话。 这是第二段话。 换行 这是第一行。 这是第二行。 强调 这是粗体。 这是斜体。 这是既粗又斜体。 块引用 块引用前有空白行！ 这是块引用。 这是嵌套块引用。 块引用后有空白行！ 列表 第一 第二 第三 无序一 无序二 在此项里要空一个Tab。 无序三 代码块 #include \u003ciostream\u003e\rint main(){\rstd::cout \u003c\u003c \"这是缩进方式的代码块\";\r}\r#include \u003ciostream\u003e int main(){ std::cout\u003c\u003c \"这是围栏方式的代码块\"; } 图片 代码 这里是Hugo生成的静态网站。 分割线 我在分割线之上。 我在分割线之下。 链接 这篇文章参考了Markdown Guide。 这篇文章参考了Markdown Guide（引用式链接）。 表格 语法 描述 示例 标题 # # h1 强调 ** 这是强调 脚注 这是一个简单的脚注，1还有个更复杂的脚注。2 定义 Markdown 一种轻量级标记语言。 删除线 Markdown我不会现在我会了 任务列表 选中 未选中 Emoji 输入法打出来的😂 表情符号的短代码 :tent: 高亮 这是高亮。 上下标 下标：H2O 上标：x2 警示 注意 突出显示用户应考虑的信息，即使只是浏览也应考虑。 提示 可选信息，可帮助用户取得更大的成功。 重要 用户成功所需的关键信息。 警告 由于存在潜在风险，需要用户立即关注的关键内容。 小心 操作的潜在负面后果。 FixIt 一个简洁、优雅且高效的 Hugo 主题。 任务列表 未完成 已完成 进行中 已取消 已计划 已重新计划 重要 问题 下划线 这是下划线。 数学公式 这是行内公式：$c = \\pm\\sqrt{a^2 + b^2}$ 这是公式块： $$ c = \\pm\\sqrt{a^2 + b^2} $$ 字符注音 Markdown一种轻量级标记语言 分数 99/100 简单的脚注 ↩︎ 复杂的脚注 ↩︎ ","date":"2025-01-25","objectID":"/posts/article-test/:0:0","tags":["测试","Hugo","FixIt"],"title":"文章测试","uri":"/posts/article-test/"}]