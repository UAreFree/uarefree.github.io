[{"categories":null,"content":" 🧱 社会稳定一份子 🌲 退耕还林倡导者 ⚙️ 控制科学自由人 🔧 电子制作门外汉 🚗 智能小车梦想家 🔥 三昧真火炼丹师 💻 C++ 受虐狂 🐧 Linux 键盘侠 ","date":"2025-09-09","objectID":"/about/:0:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":["操作系统"],"content":"xv6: a simple, Unix-like teaching operating system (book-riscv-rev4) 操作系统原型——xv6分析与实验 (罗秋明) xv6 book chapter 1 xv6实现了Unix所引入的接口，还复刻了Unix的内部架构设计。 当进程需要调用内核服务时，会调用系统调用（OS的一个接口调用），进入内核，内核执行完服务后再返回。当用户调用系统调用时，硬件会提升特权级别并启动内核中预先安排的功能（这里应该指的是trap）。 xv6进程由用户内存空间（指令、数据和堆栈）以及进程状态组成。内核为每个进程分配一个进程标识符（PID），fork创建新进程，会将原进程的内存（包括指令、数据和堆栈）完全复制到新进程中，返回两个参数：在原进程中返回新进程的PID、在新进程中返回零。 xv6 syscall 这里system call表中列出了sleep的描述，传入int类型整数，会暂停n个时钟周期。exit终止当前进程，释放资源，传入0表示成功，1表示失败；wait返回子进程exit后的PID，并将子进程的退出状态复制到传入的status参数，如果有子进程则等待一个退出，如果没有子进程立即返回-1;exec会从文件系统中加载一个文件替换调用进程的内存，该文件需为ELF格式，通常是编译程序后的结果，指令会从ELF声明的入口点开始执行，第二参数是字符串参数，通常字符串数组第一个元素是程序名称（被忽略掉），注意exec要求最后一个元素必须是0（NULL）表示参数列表的结束；sbrk将进程内存扩展n个字节，返回新内存的地址；read从文件描述符fd中读取n个字节复制到buf中，并返回已读取的字节数，每个文件描述符都关联一个偏移量，read会从当前文件描述符对应的文件偏移量读取数据，无法读取时返回0表示结束；write会将n个字节从缓冲区写入fd，也会从文件偏移量开始；dup复制一个现有文件描述符，返回一个指向原描述符对象的新描述符，两个描述符共享一个偏移量；pipe创建一个管道，p[0]放读fd，p[1]放写fd。 xv6 book chapter 2 OS必须满足三大核心要求：多路复用、隔离与交互。xv6运行在多核RISC-V处理器上，RISC-V是一款64位CPU，xv6专为qemu的\"-machine virt\"选项模拟硬件环境设计，包括内存、ROM、串口、磁盘驱动器等。 某些嵌入式设备或实时OS系统实现的方式是：将系统调用实现为库，供应用程序调用，应用程序可以以这种方式直接与硬件资源交互。当有多个应用程序时，必须定期让出CPU权，这种协作分时机制需要应用程序之间是相互信任的，但更多的是需要隔离的。实现隔离就应避免直接访问硬件资源，而将其抽象为服务。 为实现强隔离机制，OS需要确保：应用程序无法修改OS的数据结构和指令，应用程序无法访问其他进程的内存。硬件上的强隔离：RISC-V具有三种指令模式：机器模式、监督模式（特权指令、内核空间）和用户模式（用户指令、用户空间）。若应用程序需要调用内核函数（系统调用）必须切换至内核模式。为此，RISC-V提供了ecall指令，将CPU切换至内核模式，并通过内核指定的入口点（入口点位置很重要，内核必须能够控制）进入内核。 为了减少内核代码出错导致内核崩溃的可能性，微内核架构尽量减少了内核空间的代码，仅包含启动应用程序、发送消息、访问硬件设备等少数底层功能，其他大部分功能驻留到用户空间中。 将所有系统调用的实现都在监督模式下运行，这种架构是宏内核，整个OS有一个拥有完整硬件特权的单一程序构成，各个组件之间的协作更为便捷。Linux采用的是宏内核，采用微内核架构的OS在嵌入式领域得到广泛使用。xv6采用的是宏内核，虽然整体功能量要比现实工业级的微内核还小。 xv6中隔离的基本单元是进程，每个进程为程序提供看似私有的地址空间以及看似独立的CPU来执行指令。xv6进程虚拟地址最大是0x3fffffffff（MAXVA），在顶端放了一个4096字节的跳转页（trampoline，用户态转为内核态）和陷阱页（trapframe，保存进程用户寄存器），进程最重要的内核状态包括页表、内核栈和运行状态，这些状态信息由proc结构体维护。每个进程又有一个线程，用于保存执行进程所需的状态信息。线程的大部分状态信息（局部变量、函数调用返回地址等）都存储在线程栈中。每个进程有两个栈：用户栈和内核栈，用户态使用用户栈，内核态使用内核栈。可以通过ecall发起系统调用，ecall提升硬件级别，并将PC指向内核定义的入口点，执行入口点的代码，入口点的代码会切换进程的内核栈，并执行实现系统调用的内核指令，执行完成后切换回用户栈，并通过sret指令返回用户空间。 进程通过两种设计思路实现上述两种虚拟化（内存虚拟化和CPU虚拟化），内存虚拟化：虚拟地址空间，CPU虚拟化：线程。在xv6中，进程仅包含一个地址空间和一个线程，实际为充分利用多核CPU会配置多个线程。 xv6内核如何启动并运行第一个进程？当RISC-V开机时，会初始化硬件并运行存储在ROM中的引导加载程序，该程序将xv6内核加载到内存中，然后在机器模式下，从入口点（entry.S）开始执行xv6指令，设置堆栈以便运行start.c代码。函数start会执行一些仅在机器模式下运行的配置操作，然后切换至监督模式（寄存器mstatus），将返回地址设为主程序地址（写入主程序地址到寄存器mepc），禁用监督模式下的虚拟地址转换（向页表寄存器satp写入0），并将所有中断和异常都委托给监督模式处理。主程序main.c初始化多个设备后调用用户初始化函数proc.c创建首个进程，该进程首先执行一段汇编代码initcode.S，加载exec系统调用，将exec系统调用号SYS_EXEC加载到寄存器a7中，然后调用ecall进入内核。内核通过系统调用中a7寄存器的值来调用所需的系统调用函数，exec系统调用会执行exec(\"/init\")替换当前进程的内存和寄存器，init进程会在需要时创建一个新的控制台设备文件（开启文件描述符0、1、2），接着在控制台上启动一个shell，系统就启动了。 用户调用如何传递到内核中的exec系统调用实现？初始化代码.S将exec的参数存入寄存器a0和a1，并将系统调用号存入a7，ecall进入内核，依次执行uservec、usertrap和system call操作，system call从陷阱页中保存的a7读取系统调用号，从而执行对应的system call。返回时，将返回值记录在p-\u003etrapframe-\u003ea0中，就导致用户空间的exec()调用返回该值。 内核中系统调用参数是用户代码传递的参数，这些参数会放在寄存器中，内核trap代码会将用户寄存器保存到当前进程的陷阱页中，以便内核代码进行提取。内核函数argint、argaddr、argfd会从陷阱页中提取系统调用参第n个数，同时调用argraw来获取相应的用户寄存器保存值。 xv6 book chapter 3 xv6主要用了两种技巧：1. 将相同内存（trampoline）映射到多个地址空间；2. 用未映射页来保护内核堆栈和用户堆栈。 xv6基于Sv39 RISC-V运行，使用了低39位的虚拟地址。而这39位又划分为低12位及剩余的27位，27位可构成2^27个页表项（PTE），对应转换为44位的物理地址，低12位则为偏移量，直接复制到物理地址的低12位。 地址转换 RISC-V CPU具体将虚拟地址转为物理地址的步骤： 页表以三级树状结构存储在物理内存中（注意，页表存在物理内存中，没存在虚拟内存，也没存在MMU中），每一级页表大小是4KB，占一页，内含512（2^9）个页表项。 将上述27位虚拟地址划分为3个9位，每9位选择每级页表中的页表项。如果所需的三个PTE中任何一个不存在，分页硬件会引发分页错误异常，并由内核处理该异常。与单级页表相比，三级页表能节省较大的内存空间，比如当应用程序仅使用从地址零开始的少数页面时，顶层目录中的1至511号条目将失效，内核无需为这些条目分配中间目录，也就无需分配对应的底层页面，从而为中间目录节省了511个页面，为底层目录节省了511*512个页面。 这样虽然节省了页面，但增加了CPU加载PTE的开销，每次要读取3个PTE，为避免开销，会将PTE缓存在TLB中。 每个PTE的低10位是标志位，用于告知分页硬件如何使用相关虚拟地址，如是否有效、是否可读、是否可写等。 要让CPU使用页表，就要将顶层页目录的物理地址写到satp寄存器中，从而读取到顶层页目录的内容进行后续查表。每个CPU有自己的satp寄存器，从而实现拥有自己的虚拟地址空间，进而同时运行不同的进程。 多级页表 内核页表只有一个，大部分直接映射到物理内存，从KERNBASE（0x80000000）开始到PHYSTOP（0x88000000）这部分是QEMU模拟的地址空间范围，主要存放kernel的代码段、数据段、堆区；在KERNBASE之下将IO设备接口以内存映射寄存器的方式暴露给软件，直接与硬件通信；trampoline和栈页不是直接映射的，由于栈页放在了虚拟地址的高地址，因此保留了一个未映射的保护页，当内核栈溢出时会触发异常。 地址空间映射 代码层面上，以kvm开头的操作内核页表，以uvm开头的操作用户页表，其他同时用于两者。RISC-V中包含一个名为sfence.vma的指令，用于刷新当前CPU的TLB，xv6会在重新加载satp寄存器之后，trampoline切换用户页","date":"2025-08-30","objectID":"/posts/xv6-note/:0:0","tags":["xv6","读书笔记"],"title":"xv6读书笔记","uri":"/posts/xv6-note/"},{"categories":["操作系统"],"content":"本文是作者自己总结的《Operating Systems:Three Easy Pieces》读书笔记","date":"2025-08-23","objectID":"/posts/ostep-note/","tags":["操作系统导论","读书笔记"],"title":"操作系统导论读书笔记","uri":"/posts/ostep-note/"},{"categories":["操作系统"],"content":"《Operating Systems:Three Easy Pieces》是Arpaci-Dusseau教授夫妇所著，向Richard Feynman的《费曼物理学讲义》致敬。本书围绕虚拟化、并发和持久化三个主题展开。“教育过程的真正意义在于：前进，学习许多新的和引人入胜的主题，通过学习不断成熟，最重要的是，找到能为你点火的东西。” 操作系统介绍 冯诺依曼计算模型的基本概念，运行的程序会做一件非常简单的事情：执行指令（CPU是一个无情执行指令的机器，程序是状态机）。处理器不断取指（内存中）、译码、执行、写回。操作系统是一类软件，负责让程序运行变得容易。 操作系统主要利用虚拟化技术将物理资源转换为虚拟形式，虚拟化CPU将单个CPU转换为看似无限数量的CPU，从而让许多程序看似同时运行；虚拟化内存，每个进程访问自己的私有虚拟地址空间，操作系统以某种方式映射到机器的物理内存上，物理内存是操作系统管理的共享资源。 并发问题在同时（并发地）处理很多事情时出现且必须解决。多线程程序对共享计数器必须进行处理，对于这种增加计数值时需要三条指令：1. 将计数器的值从内存加载到寄存器；2. 递增值；3. 将寄存器的值保存回内存。这三条指令不是原子执行（所有指令一次性执行）的，所以会有怪事发生。 断电或系统崩溃，内存中的数据就会丢失，需要硬件（磁盘）和软件（文件系统）持久地存储数据。操作系统不会虚拟化磁盘而是假设用户经常需要共享文件中的信息。文件系统部分的系统调用有open()、write()、close()等，首先确定新数据放到磁盘哪个位置，然后在文件系统所维护的各种结构中进行记录，需要向底层存储设备发出I/O请求。出于性能方面考虑，首先延迟写操作一段时间，将其批量分组为较大的组。为了处理写入期间系统崩溃的问题，文件系统包含某种复杂的写入协议，如日志或写时复制，仔细排序写入磁盘的操作，以确保恢复到合理的状态。 设计一个操作系统的目标：1. 建立一些抽象；2. 提供高性能；3. 在OS和应用程序之间提供隔离；4. 可靠。 虚拟化 进程 进程：运行中的程序，OS为运行的程序所提供的抽象。进程的机器状态：内存和寄存器。所有OS需要提供的API：创建、销毁、等待、其他控制、状态。OS运行程序必须做的第一件事：将磁盘中的代码和所有静态数据加载到内存中，加载到进程的地址空间中。OS在加载时也是惰性加载，仅在程序执行期间需要加载的代码和数据才会加载。第二件事，为程序运行时栈分配内存空间，也可能分配堆空间。还会执行一些其他初始化任务，特别是I/O相关。默认每个进程有三个文件描述符fd，标准输入、输出和错误。 进程状态：运行、就绪、阻塞。运行和就绪可以相互转换，通过CPU调度和取消调度。阻塞（被某种事件阻塞）需要等待某种事件发生。初始：进程刚创建；最终：已退出但尚未清理（僵尸状态）。这里僵尸进程是指子进程先于父进程退出后，父进程没有释放子进程的PCB，如果父进程不结束，那么一直保持僵尸进程状态，占用系统资源，如果父进程结束了，init进程会自动接手子进程并收尸。与之相对的是孤儿进程，孤儿进程是父进程先于子进程退出，那么会被init进程收养，不会占用系统资源。僵尸进程的处理办法：1. 杀死父进程，大多数情况不可取。2. SIGCHLD信号处理，实际上当子进程终止时，内核会向它的父进程发送一个SIGCHLD信号，父进程可以选择忽略（默认）还是提供处理函数（调用wait或waitpid函数释放子进程占用的资源）。 fork()：这个系统调用确实比较奇怪，初学时很难接受是怎么做到同一个程序同时运行相同的代码的。fork的话会创建一个子进程，子进程copy了父进程（内存和寄存器值），但是返回值不同（子进程返回值为0）。fork的子进程不会执行fork之前的代码。 wait()：父进程可以延迟执行，调用wait等待子进程执行完毕再执行，这个系统调用也会回收子进程的资源。 exec()：可以让子进程执行与父进程不同的程序，exec会从可执行程序中加载代码和静态数据，并覆盖自己的代码段和静态数据，堆、栈及其他内存空间也会被重新初始化，然后OS就会执行该程序，将参数通过argv传给该进程。这里说明exec是完全转头调用所指定的可执行程序。这种分离fork和exec的做法在构建shell时非常有用，因为这给了shell在fork之后exec之前运行代码的机会。比如shell作为主进程在fork了一个子进程后通过exec执行wc系统调用统计文件字符数，可以使用重定向将输出从标准输出改为某个文件，而这实现原理就是在fork之后exec之前关闭标准输出的文件描述符，打开文件，新打开的文件将成为第一个可用的文件描述符，从而向文件中写入。 受限直接执行 时分共享CPU实现虚拟化：看起来同时运行多个进程。在保持控制权的同时获得高性能是构建操作系统的主要挑战之一。 用户态和内核态切换：划分用户空间和内核空间区分受限操作执行，内核空间可以做everything，执行受限的指令；用户空间需要执行这些受限的指令就需要执行系统调用，而要执行系统调用就必须执行trap指令，该指令同时跳转入内核，从而执行特权指令，完成后，OS调用一个trap返回指令返回到用户空间。这涉及到内核态到用户态的转换，在trap时处理器需要将一些寄存器保存到每个进程的内核栈上，trap返回时从栈中弹出这些值。open()系统调用和C的过程调用看起来差不多，是的，open()是一个过程调用但内部有trap指令，在执行open()时会用汇编处理参数和返回值以及执行trap指令。然而需要考虑的一点是，trap进内核的代码地址不能是任意的（出于安全考虑），那么每个系统调用或中断在trap进内核后都会被指定对应的处理程序地址，这个对应关系保存在trap表中。 进程间切换：OS作为一个进程（类似的）也要运行在CPU上，如果子进程去执行一个死循环而不主动让出CPU的控制权那么OS就失去了CPU的控制权，唯一办法是重启大法。这显然不合理，需要其他办法获得CPU的控制权。时钟中断，和系统调用一样，必须有对应的中断处理函数，以及在系统开始时也必须启动时钟。 上下文切换：系统开启时钟中断，中断产生后，发生进程切换，也就有进程间的上下文切换。上下文切换就是保存当前执行进程的寄存器的值，恢复即将执行进程的寄存器的值。为了保存当前进程的上下文，OS会执行一些汇编代码来保存通用寄存器、程序计数器、内核栈指针，然后恢复通用寄存器、程序计数器、切换内核栈，以供即将执行进程使用。通过切换栈，内核在进入切换代码调用时，是一个进程的上下文，返回时，是另一个进程的上下文。当OS最终执行从陷阱返回指令时，即将执行的进程变成了当前运行的进程，至此上下文切换完成。 这里可以这样理解，进程进行上下文切换，那么什么是上下文？上下文肯定是描述进程的东西，那么进程由什么来描述？就是PCB，简单来说主要包括寄存器和内存，每个进程有自己运行时寄存器的值，每个进程有自己看起来独有的内存空间（包括栈和堆）。那么上下文就包括这些，切换的也就是寄存器的值、内存空间。具体过程就是保存正在执行的进程A的寄存器（通用寄存器、程序计数器、栈指针等）到内核空间的一块内存中（cpu_context结构体保存），把即将要执行的进程B所保存的上下文内存块中（cpu_context）的值恢复到寄存器中，这是硬件上下文的切换；也别忘了还有内存空间的切换，具体就是切换进程B的pgd（页全局目录的虚拟地址）所对应的页全局目录的物理地址到页表基址寄存器中，当访问用户空间时MMU会通过页表基址寄存器来遍历页表得到物理地址。 而下边表中保存和恢复了两次寄存器，容易误导。实际上下文切换的就是调用switch()例程的一次寄存器的保存和恢复，而另一次是中断所伴随的，即不论是否是上下文切换的场景，只要中断产生都会将用户态的寄存器的值保存到内核栈，然后恢复，这里称为保存现场，不要与进程上下文搞混。 受限直接执行协议（时钟中断） 所谓的受限直接执行的含义是：OS在启动时设置陷阱处理程序并启动时钟中断，然后仅在受限模式下运行进程，以此为CPU提供宝宝防护。这是虚拟化CPU的基本机制。 进程调度 调度指标：周转时间，周转时间=完成时间-到达时间，这是一个性能指标。另一个有趣的目标是公平，两者往往是矛盾的。 先进先出 FIFO 最短任务优先 SJF 同时到达的任务先运行短时间的 最短完成时间优先 STCF 向SJF添加抢占则为最短完成时间优先，随时到达随时评估 新的调度指标：响应时间，响应时间=首次运行时间-到达时间。其实就对应了任务的响应优先级，考虑到了某些运行时间长的任务一直得不到响应的情况。 轮转 RR 时间片轮转，划分固定的时间片给每个任务，按任务队列顺序依次占有CPU 时间片时间越短响应时间越好，但是需要摊销上下文切换成本，让上下文切换时间占比不要太高 上下文切换的成本不仅来自上述所说的操作，在CPU cache、TLB等硬件上建立了大量的状态，这些切换也可能导致显著的性能成本。 这样会导致周转时间指标很差，因为作为一个公平的策略 结合I/O 这里主要是指实际场景中可以利用I/O操作耗时的间隙进行CPU操作从而更好利用CPU 多级反馈队列 MLFQ Corbato提出，获得图灵奖 需要联合考虑周转时间和响应时间两个指标 是用历史经验预测未来的一个经典例子，以史为鉴，更多的是基于行为制定规则，而不是先验知识 划分不同优先级的不同队列，每个队列之间用RR，I/O密集型的优先级高（占用CPU时间短），CPU密集型的优先级低（占用CPU时间长），优先级大的先运行，","date":"2025-08-23","objectID":"/posts/ostep-note/:0:0","tags":["操作系统导论","读书笔记"],"title":"操作系统导论读书笔记","uri":"/posts/ostep-note/"},{"categories":["轮子"],"content":"spdlog是一个快速的C++日志库。使用有两个版本，headr-only的版本和编译版本。header-only版本只需要把项目include文件夹放到自己项目中即可构建，用起来比较方便。官方给出的特点：很快；header-only 或编译版本；使用 fmt 库实现丰富的格式化功能；异步模式；自定义格式化；多线程/单线程记录器；多种日志目标；日志过滤；支持从argv或环境变量中加载日志级别；回溯支持 - 将调试消息存储在环形缓冲区中并按需显示。 spdlog是一个日志库，调用其提供的接口实现日志打印功能，它本身引用到了fmt库，fmt是格式化的库。初级的日志打印的定义是： spdlog::info(\"Positional args are {1} {0}..\", \"too\", \"supported\"); 上述函数通过调用spdlog命名空间的info接口，打印结果： [2025-09-04 15:49:34.163] [info] Positional args are supported too.. spdlog::set_level(spdlog::level::info); 我们通过spdlog::info接口来向下探索，该接口定义在spdlog.h中，说明所有源文件只要include了spdlog.h，即可调用spdlog库所提供的接口。该函数是一个模版函数定义如下： template \u003ctypename... Args\u003e inline void info(format_string_t\u003cArgs...\u003e fmt, Args \u0026\u0026...args) { default_logger_raw()-\u003einfo(fmt, std::forward\u003cArgs\u003e(args)...); } Args…是可变参数包类型，format_string_t\u003cArgs…\u003e fmt是fmt库接口fmt::format_string\u003cArgs…\u003e，Args \u0026\u0026…args将可变参数包转为右值用于完美转发。还要需要注意的是spdlog.h中的所有函数都是inline的，这也是为什么spdlog能实现header-only的原因。 default_logger_raw()的函数定义如下： SPDLOG_INLINE spdlog::logger *default_logger_raw() { return details::registry::instance().get_default_raw(); } 该函数使用单例模式，使用static创建全局唯一registry实例，调用get_default_raw()拿到默认logger。然后调用logger类的成员函数info，该函数定义如下，其中log函数是重载函数，支持多种参数类型和数量。不过最终都会调用log_it_函数。 template \u003ctypename... Args\u003e void info(format_string_t\u003cArgs...\u003e fmt, Args \u0026\u0026...args) { log(level::info, fmt, std::forward\u003cArgs\u003e(args)...); } SPDLOG_INLINE void logger::log_it_(const spdlog::details::log_msg \u0026log_msg, bool log_enabled, bool traceback_enabled) { if (log_enabled) { sink_it_(log_msg); } if (traceback_enabled) { tracer_.push_back(log_msg); } } log_it_函数主要就是接收log函数根据日志级别判断是否日志打印的参数，调用sink_it_函数。 SPDLOG_INLINE void logger::sink_it_(const details::log_msg \u0026msg) { for (auto \u0026sink : sinks_) { if (sink-\u003eshould_log(msg.level)) { SPDLOG_TRY { sink-\u003elog(msg); } SPDLOG_LOGGER_CATCH(msg.source) } } if (should_flush_(msg)) { flush_(); } } sink_it_函数遍历所有sink，调用sink的log函数。这里sinks_的类型是std::vector\u003csink_ptr\u003e，即shared_ptr的容器，每个元素是指向sink对象的智能指针。而log是虚函数，交由继承sink的子类对象来实现，这里sink-\u003elog(msg);很好的展现了多态的实现机制。base_sink继承了sink，且其他sink继承了base_sink，spdlog中定义了多种sink，如daily_file_sink、rotating_file_sink、stdout_sinks等，这些sink实现的头文件放到了sinks中。以daily_file_sink为例，最后写日志的函数实现为： void sink_it_(const details::log_msg \u0026msg) override { auto time = msg.time; bool should_rotate = time \u003e= rotation_tp_; if (should_rotate) { auto filename = FileNameCalc::calc_filename(base_filename_, now_tm(time)); file_helper_.open(filename, truncate_); rotation_tp_ = next_rotation_tp_(); } memory_buf_t formatted; base_sink\u003cMutex\u003e::formatter_-\u003eformat(msg, formatted); file_helper_.write(formatted); // Do the cleaning only at the end because it might throw on failure. if (should_rotate \u0026\u0026 max_files_ \u003e 0) { delete_old_(); } } 根据msg的时间判断是否大于轮转时间，如果大于就要新建一个文件，打开、写文件是通过抽象的file层实现。写操作是最后调用stdio.h中标准库POSIX接口fwrite_unlocked来实现文件写数据，这是单线程下不加锁的实现。 SPDLOG_INLINE void file_helper::write(const memory_buf_t \u0026buf) { if (fd_ == nullptr) return; size_t msg_size = buf.size(); auto data = buf.data(); if (!details::os::fwrite_bytes(data, msg_size, fd_)) { throw_spdlog_ex(\"Failed writing to file \" + os::filename_to_str(filename_), errno); } } extern size_t fread_unlocked (void *__restrict __ptr, size_t __size, size_t __n, FILE *__restrict __stream) __wur __nonnull ((4)); extern size_t fwrite_unlocked (const void *__restrict __ptr, size_t __size, size_t __n, FILE *__restrict __stream) __nonnull ((4)); #endif 以上就是spdlog提供的spdlog::info实现流程，整体来看就是创建全局唯一单例registry，然后创建logger，logger构造时可以绑定sink输出槽，将日志消息传递到sink中调用标准库API进行写日志，其中还用到了fmt格式化。这是在单线程下单一logger单一sink的最简单实现。在example/example.cpp中还给出了其他使用示例，比如我们在使用时可以直接创建一个shared_ptr类型的对象指向spdlog所提供的sink，创建sink时一般是名字（字符串）和filename绑定的方式。一个logger是可以对应多个sink的，在logger的构造函数中提供了迭代器复制构造的实现： template \u003ctypename It\u003e logger(std::string name, It begin, It end) : name_(std::move(name)), sinks_(begin, end) {} 本篇文章从示例出发，一层一层地解析了spdlog的实现。其实更深层次的解析应该从架构出发，给出整个库实现的各个模块关系，这里先挖个坑，有时间再来填。 ","date":"2025-06-22","objectID":"/posts/spdlog-analysis/:0:0","tags":["spdlog","日志库"],"title":"spdlog源码解析","uri":"/posts/spdlog-analysis/"},{"categories":["计算机系统"],"content":"链接：将代码和数据收集并组合成一个可执行文件，加载到内存中执行。 学习链接可以解决的问题： 构造大型程序，遇到由于缺少库文件或者库文件版本不兼容而导致的链接错误。 避免一些难以发现的编程错误。 理解编程语言中的作用域规则是如何实现的。 理解重要的系统概念。 更好的利用共享库。 编译器驱动程序 GCC GNU 编译器集合 GCC 会调用语言预处理器、编译器、汇编器和链接器。gcc -Og -o prog main.c sum.c调用gcc编译，-Og启动适中优化、-o生成可执行文件。./prog调用操作系统中的加载器函数，将可执行文件prog中的代码和数据复制到内存，然后将控制转移到这个程序的开头。 GNU是一个自由软件项目，旨在开发完全自由的操作系统。GCC 是 GNU 项目下的一组编译工具的集合。 命令 作用 gcc -Og -E prog main.c sum.c 预处理生成 .i gcc -Og -S prog main.c sum.c 编译成汇编文件 .s gcc -Og -c prog main.c sum.c 汇编成可重定位目标文件 .o gcc -Og -v prog main.c sum.c 链接成可执行目标文件 .out 目标文件 注意不只是.o文件，目标文件有以下三种： 可重定位目标文件：包含二进制代码和数据，.o文件可与其他.o文件链接 可执行目标文件：包含二进制代码和数据，.out文件可直接复制到内存并执行 共享目标文件：特殊的可重定位目标文件，可以在加载或者运行时被动态地加载进内存并链接 目标文件是按照特定的格式来组织的，各个系统的目标文件格式都不相同。现代 x86-64 Linux 和 Unix 系统使用可执行可链接格式（Executable and Linkable Format，ELF）。 可重定位目标文件 ELF 格式的可重定位目标文件由 ELF header、Sections、Section header table组成。 ELF文件格式 ELF header ELF头 ELF header 的长度是64个字节。 Sections .text：已编译程序的机器代码。 .data：已初始化 的全局和静态变量。 .bss：未初始化的全局和静态变量，以及所有被 初始化为0 的全局或静态变量。在目标文件中这个节不占据实际的空间，它仅仅是一个占位符（better save space）。目标文件格式区分已初始化和未初始化变量是为了空间效率：在目标文件中，未初始化变量不需要占据任何实际的磁盘空间。运行 时，在内存中分配这些变量，初始值为 0。COMMON存放未初始化的全局变量。 .rodata:存放 只读 数据。如printf中的格式串和switch语句中的跳转表。 .symtab：符号表，存放在程序中定义和引用的函数和全局变量的信息。 Section header table 存放 sections 中每个 section 的起始位置、大小、类型等信息。 符号和符号表 链接的本质是把多个不同的目标文件粘合在一起，而符号是链接中的粘合剂。符号表就是 GOT (Global Offset Table)。 使用readelf -S file.o查看段头信息（包含每个段的名称、大小、类型等），使用readelf -s file.o查看符号表（包括符号的名称、类型、值等），使用readelf -r file.o查看重定位信息（指示代码中哪些地方需要在链接时调整）。 yaf@yaf-VMware-Virtual-Platform:~/projects/csapp$ readelf -s main.o Symbol table '.symtab' contains 6 entries: Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FILE LOCAL DEFAULT ABS main.c 2: 0000000000000000 0 SECTION LOCAL DEFAULT 1 .text 3: 0000000000000000 30 FUNC GLOBAL DEFAULT 1 main 4: 0000000000000000 8 OBJECT GLOBAL DEFAULT 3 array 5: 0000000000000000 0 NOTYPE GLOBAL DEFAULT UND sum 符号main是位于.text节（Ndx = 1）偏移量为 0 （value = 0）处的 30 字节函数；符号array是位于.data节（Ndx = 3）偏移量为 0 （value = 0）处的 8 字节对象；符号sum是对外部符号的引用（Ndx = UND） #include \u003cstdio.h\u003e int count = 10; // 初始化的全局变量 .data int value; // 未初始化的全局变量 .bss void func(int sum){ // 全局函数 printf(\"sum = %d\\n\", sum); } int main() { static int a = 1; // 初始化的静态变量 .data static int b = 0; // 初始化的静态变量 .bss int x = 1; // 局部变量 func(a + b + x); return 0; } Num: Value Size Type Bind Vis Ndx Name 0: 0000000000000000 0 NOTYPE LOCAL DEFAULT UND 1: 0000000000000000 0 FILE LOCAL DEFAULT ABS main.c 2: 0000000000000000 0 SECTION LOCAL DEFAULT 1 .text 3: 0000000000000000 0 SECTION LOCAL DEFAULT 3 .data 4: 0000000000000000 0 SECTION LOCAL DEFAULT 4 .bss 5: 0000000000000000 0 SECTION LOCAL DEFAULT 5 .rodata 6: 0000000000000004 4 OBJECT LOCAL DEFAULT 3 a.1 7: 0000000000000004 4 OBJECT LOCAL DEFAULT 4 b.0 8: 0000000000000000 4 OBJECT GLOBAL DEFAULT 3 count 9: 0000000000000000 4 OBJECT GLOBAL DEFAULT 4 value 10: 0000000000000000 43 FUNC GLOBAL DEFAULT 1 func 11: 0000000000000000 0 NOTYPE GLOBAL DEFAULT UND printf 12: 000000000000002b 52 FUNC GLOBAL DEFAULT 1 main a.1和b.0是名称修饰，为了防止静态变量的名字冲突。局部变量符号表里是没有的，由于在栈上，符号表并不关心。 符号类型： 全局符号，由该模块定义，同时能被其他模块引用。 外部符号，被其他模块定义，同时被该模块引用。 局部符号，只能被该模块定义和引用。 区别局部符号和全局符号：static属性。 符号解析 将程序中的符号（如变量名、函数名等）与它们在内存或程序中的实际地址进行关联的过程。 符号同名冲突 多个同名的强符号 找死，出 error 一个强符号和多个同名的弱符号 没事，隐藏 bug 多个同名的弱符号 warning，不易察觉的运行时 error 编译时添加-fno -common的编译选项，多个重名触发错误；或-Werror把所有警告变成错误 强符号：函数和已初始化的全局变量。 弱符号：未初始化的全局变量。 静态链接 printf、atoi等函数存放在libc.a中，静态库以一种称为archive的特殊文件格式存放在磁盘上。使用ar rcs libvector.a addvec.o multvec.o构造静态库，使用gcc -static -o prog main.o ./libvector.a链接静态库。根据main.o中的符号，将所用到的符号对应的.o文件复制进可执行文件。 静态链接过程 目标文件是字节块的集合，有代码块、数据块、引导链接器和加载器的数据结构。 链接器必须完成的两个主要任务： 符号解析： 按命令行从左到右扫描可重定位文件和静态库文件 可重定位文件放到集合 E 中，最后合并起来 引用了当尚未定义的符号放到集合 U 中 文件中已定义的符号放到集合 D 中 使用静态链接，命令行文件的输入顺序十分重要，一般将库放到命令行结尾。而且如果库是独立的，必须对它们进行排序 重定位 重定位 链接器合并输入模块，并为每个符号分配运行时地址。 重定位节和符号定义 把所有相同类型的 section 合并为一个新的 section 每条指令和全局变量都有了唯一的运行时内存地址 重定位节中的符号引用 修改对符号的引用使其指向正确的运行时地址 重定位条目：告诉链接器在合成可执行文件时应该如何修改这个引用，.rel.text是代码的重定位条目，.rel.data是已初始化数据的重定位条目 typedef st","date":"2025-05-11","objectID":"/posts/link/:0:0","tags":["链接"],"title":"链接","uri":"/posts/link/"},{"categories":["计算机系统"],"content":"《程序员的自我修养——链接、装载与库》 简介 对于系统程序开发者来说，计算机硬件设备中有三个部件最为关键，分别是中央处理器CPU、内存和I/O控制芯片。连接所有高速芯片的是北桥，专门处理低速设备的是南桥，北桥连接着PCI总线，南桥连接着ISA总线。南桥挂在PCI总线上，从而与北桥交换数据。人们希望计算机越来越快，有两种方法，一是提升CPU频率，一是增加CPU数量。 “计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决”，计算机软件体系结构就是分层的： 计算机软件体系结构 层与层之间通信的协议称为接口，接口下面那层是接口的提供者，接口上面那层是接口的使用者，每个中间层都是对它下面那层的包装和扩展。 多任务系统：OS以进程的方式运行在比系统权限更低的级别，抢占式分配CPU资源，CPU在多个进程间快速切换，造成很多进程都在同时运行的假象。目前几乎所有现代OS都采用这种方式。 设备驱动：OS作为硬件的上层，是对硬件的管理和抽象。我们希望不论硬件型号如何，对应用程序开发者来说，都只需要调用一个统一的函数接口，而不是针对每个型号的硬件都去单独实现。硬件驱动程序是OS的一部分，但交由硬件生产厂商完成。 之后的有关内存的讲述在有了OS基础后都可以看懂，参考OSTEP读书笔记文章。 线程：轻量级进程，是程序执行流的最小单元。一个线程由线程ID、PC指针、寄存器和堆栈组成。使用线程的原因： 等待网络响应，等待的线程会进入睡眠状态，无法继续执行，而多线程执行可以有效利用等待的时间。 耗时长的计算操作，多线程可以让一个线程负责交互，另一个线程负责计算。 程序逻辑本身要求并发操作。 多核计算机本身具备同时执行多个线程的能力，有必要充分发挥。 相对于多进程，多线程在数据共享方面效率很高。 线程的私有存储空间： 栈 线程局部存储（TLS），某些OS为线程单独提供的私有空间，但通常只有有限的容量 寄存器，执行流的基本数据 现代OS基本使用多任务系统，多个进程看起来同时运行，每个进程有自己独立的内存资源，CPU在多个进程间快速切换。线程调度自上述多任务系统问世来就不断被提出不同的方案和算法，主流的都带有优先级调度和时间片轮转调度的痕迹。系统会根据线程的表现自动调整优先级，通常情况下，相对于CPU密集型的现程，CPU更喜欢先运行IO密集型的线程，IO密集型的线程涉及频繁的等待。 Windows内核有明确的线程和进程的概念，可以使用明确的API：CreateProcess和CreateThread来创建进程和线程，并有一系列的API来操纵它们。但Linux对多线程的支持颇为贫乏，Linux内核中并不存在真正的线程概念，将所有执行实体都称为任务（Task），不同task之间可以选择共享内存空间，那么这些task就构成了类似的一个进程的多个线程。 过度优化： 编译器为了提高速度将一个变量缓存到寄存器中而不写回。 x = 0; Thread1 Thread2 lock(); lock(); x++; x++; unlock(); unlock(); 这里两个线程使用了锁来保护共享变量x，但也不一定能保证线程安全。如果Thread1读取x的值到寄存器中，进行了x++操作，但并没有立即写回到内存中，释放了锁；Thread2去读x的值，依旧是0，读的是之前在内存中的值。 编译器调整指令顺序。 x = y = 0; Thread1 Thread2 r1 = y; y = 1; x = 1; r2 = x; 显然，r1和r2至少一个为1，逻辑上不可能为0。然而，编译器在优化时，可能为了效率而不交换毫不相干的两条相邻指令的执行顺序，那么r1=r2=0就完全可能了。 解决办法：使用volatile关键字，volatile可以完美解决第一种情况。但第二种情况，虽然可以阻止编译器调整volatile变量的指令顺序，但无法阻止CPU动态调度换序。 单例模式的double-check： volatile T* pInst = 0; T* GetInstance(){ if(pInst == NULL){ lock(); if(pInst == NULL) pInst = new T; unlock(); } return pInst; } 这里插一下为什么需要double-check，如果不double-check的话，代码是这样的： volatile T* pInst = 0; T* GetInstance(){ if(pInst == NULL){ lock(); pInst = new T; unlock(); } return pInst; } 如果当前实例为空，就上锁new一个对象，防止多线程同时new多个对象。但是如果两个线程同时判断了pInst == NULL，线程A先拿到锁，new了对象，线程B依旧会执行条件判断里的语句，new一个对象，所以要再次判断。 书中指出，这样的代码依然是有问题的，C++ 的new包含两个步骤：1. 分配内存；2. 调用构造函数。所以pInst = new T;包含三步：1. 分配内存；2. 在内存的位置上调用构造函数; 3. 将内存地址赋值给pInst。CPU乱序执行会导致2、3步骤交换，如果先将地址赋值给pInst，而构造函数还没调用完成，对象没有构造出来，此时并发的调用GetInstance()会发现pInst不为空，从而直接将未构造完成的对象返回给用户使用。对于阻止CPU乱序执行的方法，通常是调用CPU提供的一条指令barrier来保证线程安全。 volatile T* pInst = 0; T* GetInstance(){ if(pInst == NULL){ lock(); if(pInst == NULL){ T* temp = new T; barrier(); // 保证对象构造完成 再返回地址 pInst = temp; } unlock(); } return pInst; } 这里题外话，推荐使用局部静态变量（自带线程安全初始化）来实现单例模式，而不用双检查锁。 T* GetInstance(){ static T pInst; return \u0026pInst; } 多线程内部情况：用户使用的线程并不是内核线程，而是存在于用户态的用户线程。用户线程并不一定在OS内核里对应同等数量的内核线程，例如某些轻量级的线程库，三个用户线程对内核来说可能只有一个线程。 一对一模型，一个用户线程对应一个内核线程，线程之间是真正的并发。一般直接使用API或系统调用创建的线程均为一对一的线程。缺点：用户线程数量受OS限制；内核线程调度上下文开销大，导致用户线程执行效率下降。 多对一模型，将多个用户线程映射到一个内核线程，线程切换由用户态代码进行，快速很多，高效的上下文切换和几乎无限制的线程数量。缺点：其中一个用户线程阻塞，所有线程都将无法执行；处理器增多也不会有明显的性能提升。 多对多模型，将多个用户线程映射到少数但不止一个内核线程上，上述两者模型的折中，优缺点也对应折中。 静态链接 被隐藏了的过程 静态链接过程 预处理 命令：gcc -E hello.c -o hello.i 删除所有的\"#define\"，并展开所有宏定义 处理所有条件预编译指令，如\"#if\"、\"#ifdef\"等 处理\"#include\"预编译指令，将被包含的文件插入到该预编译指令的位置 删除所有的注释\"//“和”/**/\" 添加行号和文件名标识 保留所有的#pragma编译器指令 预处理后的.i文件所有宏已经被展开，且包含的文件也已经被插入到.i文件中。当无法判断宏定义是否正确或头文件是否正确时，可以查看预编译后的文件来确定问题。 编译 命令：gcc -S hello.i -o hello.s 具体步骤在下一节简单介绍。 gcc命令只是后台程序的包装，它会根据不同的参数要求去调用预编译编译程序cc1、汇编器as、连接器ld。 汇编 命令：gcc -c hello.s -o hello.o 将汇编代码转变成机器可以执行的指令，每一个汇编语句几乎对应一条机器指令，调用汇编器as来完成。 链接 ld -static crt1.o crti.o crtbeginT.o hello.o -start-group -lgcc -lgcc_eh -lc -end-group crtend.o crtn.o 本书主要介绍上述链接的内容。 编译器做了什么 源代码通过词法分析、语法分析、语义分析构建语法树，并进行优化，转为中间代码，它是语法树的顺序表示。中间代码使得编译器可以被分为前端和后端，前端负责产生机器无关的中间代码，后端将中间代码转换成目标机器代码，实现跨平台。代码生成器将中间代码转换为目标代码，这样源代码被编译成了目标代码。 以C语言的源代码为例：array[index] = (index + 4) * (2 + 6)为例，它会编译成为目标代码，但目标代码中的index和array的地址还没确定，要把目标代码编译成能在机器上执行的指令，必须要确定index和array的地址。如果index和array跟源代码在同一个编译单元，那么编译器可以为它们分配空间，确定它们的地址。但是如果定义在其他程序模块，最终运行时的绝对地址都要在最终链接的时候才能确定。所以现代编译器可以将一个源代码文件编译成一个未链接的目标文件，然后由链接器将这些目标文件链接起来形成可执行文件。 链接器年龄比编译器长 目标文件的指令可能不会一成不变，对应的地址也会发生改变，重新计算各个目标的地址过程叫做重定位。汇编语言将机器指令难以记住","date":"2025-04-06","objectID":"/posts/programmer-note/:0:0","tags":["读书笔记"],"title":"程序员的自我修养读书笔记","uri":"/posts/programmer-note/"},{"categories":["编程语言"],"content":"智能指针（Smart Pointer）是C++ 中用来自动管理动态内存分配的工具，它是一种类模板，能够自动释放内存，避免手动管理动态分配内存时可能发生的内存泄漏和指针悬空等问题。 智能指针主要包含以下几个特性： 自动内存管理：当智能指针超出作用域时，会自动释放其管理的动态内存。 RAII（资源获取即初始化）：智能指针在构造时绑定动态内存，并在析构时自动释放资源。 引用计数（部分智能指针支持）：追踪共享对象的引用次数，当引用计数变为0时自动销毁对象。 以上是GPT给出的概念解释，道理大家都懂，具体怎么使用以及怎么实现的呢？ 动态内存分配就是我们手动new()或malloc()一块堆上的存储区，最经典的就是定义一个指针，指向新开辟的数组，用完后delete掉，如果忘记delete就会导致内存泄漏，或者重复delete就会导致未定义行为，delete之后再次访问对象还可能出现悬空指针的问题。所以总结一句话，new的时候能用智能指针就用智能指针。 #include \u003ciostream\u003e int main() { // 动态分配一个大小为5的int数组 int* arr = new int[5]; // 初始化数组 for (int i = 0; i \u003c 5; ++i) { arr[i] = i * 10; } // 使用数组 std::cout \u003c\u003c \"Array elements: \"; for (int i = 0; i \u003c 5; ++i) { std::cout \u003c\u003c arr[i] \u003c\u003c \" \"; } std::cout \u003c\u003c std::endl; // 释放内存 delete[] arr; return 0; } 使用智能指针操心的就比较少，只需引入#include ，写一行智能指针的代码即可。 #include \u003ciostream\u003e #include \u003cmemory\u003e // 包含智能指针头文件 int main() { // 使用 unique_ptr 管理动态分配的数组 std::unique_ptr\u003cint[]\u003e arr(new int[5]); // 初始化数组 for (int i = 0; i \u003c 5; ++i) { arr[i] = i * 10; } // 使用数组 std::cout \u003c\u003c \"Array elements: \"; for (int i = 0; i \u003c 5; ++i) { std::cout \u003c\u003c arr[i] \u003c\u003c \" \"; } std::cout \u003c\u003c std::endl; // 离开作用域时，unique_ptr 会自动释放数组内存 return 0; } 智能指针有以下三种：std::unique_ptr、std::shared_ptr、std::weak_ptr，它们的使用就是将模板参数替换为对应new的数据类型。 std::unique_ptr 独占所有权，只能一个指针指向一块动态内存，无法复制，只能move转移所有权。 #include \u003cmemory\u003e #include \u003ciostream\u003e class MyClass { public: MyClass() { std::cout \u003c\u003c \"Constructor\\n\"; } ~MyClass() { std::cout \u003c\u003c \"Destructor\\n\"; } }; int main() { // 创建一个 unique_ptr，管理 MyClass 的对象 std::unique_ptr\u003cMyClass\u003e ptr1 = std::make_unique\u003cMyClass\u003e(); // 无法复制 unique_ptr // std::unique_ptr\u003cMyClass\u003e ptr2 = ptr1; // 错误 // 转移所有权 std::unique_ptr\u003cMyClass\u003e ptr2 = std::move(ptr1); // 此时 ptr1 不再拥有对象，ptr2 拥有对象 if (!ptr1) { std::cout \u003c\u003c \"ptr1 is null\\n\"; } return 0; } std::shared_ptr 共享所有权，允许多个智能指针共享同一块动态内存，使用引用计数来追踪有多少指针引用同一个对象。 每次 shared_ptr 被复制时，引用计数增加。每次 shared_ptr 被销毁或超出作用域时，引用计数减少。当引用计数变为0时，管理的对象被销毁。 #include \u003cmemory\u003e #include \u003ciostream\u003e class MyClass { public: MyClass() { std::cout \u003c\u003c \"Constructor\\n\"; } ~MyClass() { std::cout \u003c\u003c \"Destructor\\n\"; } }; int main() { // 创建一个 shared_ptr std::shared_ptr\u003cMyClass\u003e ptr1 = std::make_shared\u003cMyClass\u003e(); { // 复制 shared_ptr，引用计数增加 std::shared_ptr\u003cMyClass\u003e ptr2 = ptr1; std::cout \u003c\u003c \"Shared ownership count: \" \u003c\u003c ptr1.use_count() \u003c\u003c \"\\n\"; // 输出 2 } // 离开作用域，ptr2 被销毁，引用计数减少 std::cout \u003c\u003c \"Shared ownership count: \" \u003c\u003c ptr1.use_count() \u003c\u003c \"\\n\"; // 输出 1 return 0; } 但是shared_ptr会存在循环引用问题，导致引用计数永不为0。那么下面讲讲什么是循环引用问题。 很类似于死锁，死锁是A在等B释放锁，而B在等A释放锁，从而导致A与B互相等待的情况。这里的循环应用就是A里的shared_ptr指向了B，而B里的shared_ptr指向了A，两个shared_ptr就算离开作用域，引用计数也都为1，导致A和B的内存无法释放，形成内存泄漏。 死锁的解决方法有：资源（锁）按相同顺序获取；线程主动释放资源；那么循环引用的解决方法是？引入weak_ptr，weak_ptr并不会增加引用计数，那这样weak_ptr在作用域结束时一定会被释放，指向weak_ptr的shared_ptr自然会将引用计数减一，从而正常释放自己。 使用shared_ptr还会出现一个问题，shared_ptr如果直接指向this指针时，可能会出现double free的情况，因为shared_ptr指向this指针会指向一个新的控制块，这样对同一对象就会同时有两个控制块（引用计数），当shared_ptr生命周期结束时所指向的对象会被释放两次。解决方法是：使用std::enable_shared_from_this，它内部实现是一个weak_ptr指向共享的控制块，保证了引用计数的唯一。 std::weak_ptr 不多说了，上边说了weak_ptr存在的意义。值得注意的是，weak_ptr在观察shared_ptr所管理的对象，当需要访问对象时，可以通过 weak_ptr.lock() 获取一个临时的 std::shared_ptr。如果对象已经被释放，则返回空指针。 #include \u003ciostream\u003e #include \u003cmemory\u003e class B; // 前向声明 class A { public: std::shared_ptr\u003cB\u003e b_ptr; // A 持有 B 的 shared_ptr ~A() { std::cout \u003c\u003c \"A destroyed\\n\"; } }; class B { public: std::weak_ptr\u003cA\u003e a_ptr; // B 持有 A 的 weak_ptr ~B() { std::cout \u003c\u003c \"B destroyed\\n\"; } }; int main() { std::shared_ptr\u003cA\u003e a = std::make_shared\u003cA\u003e(); std::shared_ptr\u003cB\u003e b = std::make_shared\u003cB\u003e(); a-\u003eb_ptr = b; // A 持有 B b-\u003ea_ptr = a; // B 持有 A（通过 weak_ptr） // 离开作用域时，不会发生内存泄漏 return 0; } 总的来说，智能指针是对原始指针的封装，提供资源管理和安全访问功能的模板类。对于共享型智能指针（如 std::shared_ptr 和 std::weak_ptr），通常引入控制块来管理资源的共享状态。控制块包含强引用计数器、弱引用计数器、资源删除器以及与资源绑定的原始对象指针等信息。多个智能指针实例可以共享一个控制块来管理同一个资源。 ","date":"2025-03-02","objectID":"/posts/cpp-smart-pointer/:0:0","tags":["C++","智能指针"],"title":"C++ 智能指针","uri":"/posts/cpp-smart-pointer/"},{"categories":["编程语言"],"content":"static是C++中非常常见的关键字，以下总结static关键字的使用场景，即什么时候使用static。 首先说static是静态的意思，什么是静态呢？你可能会听过静态存储区，一段内存的区域，我们来看看C程序的内存分布。 程序内存分布 C程序内存地址从低到高依次为代码区、初始化的数据区（.data段）、未被初始化的数据区(.bss段)、堆和栈，静态存储区就位于数据区，这里的数据区主要有全局变量、static变量，而一般的变量会被放在堆或栈区，栈主要存函数中的局部变量、堆主要存new的内存。 内存区域 存储内容 特点 代码区 程序指令 1. 只读 2. 所有线程共享 初始化数据区 1. 全局变量 2. static变量 1. 编译时显示赋值 2. 程序加载时分配内存，读取可执行文件存储的初始值 未初始化的数据区 1. 未初始化的全局变量 2. 未初始化的static变量 1. 程序加载时被自动初始化为零（数值类型为 0，指针类型为 NULL） 2. 程序加载时分配内存，但可执行文件不会存储未初始化的数据 堆区 动态分配函数（如malloc、calloc或new）分配的内存 1. 动态分配，由程序员管理 2. 生存周期由程序控制，可以跨函数代码块使用 栈区 局部变量、函数参数、返回地址 1. 由编译器自动分配，遵循“先进后出”的结构 2. 生命周期为函数调用期间的执行周期 initialized data 和 uninitialized data 有什么区别？ 初始化状态，顾名思义，初始化区的变量会显示赋值初始化，未被初始化区的变量未被显示赋值初始化。 可执行文件存储，未被初始化区变量的数据不会存储到可执行文件中，而是直接将分配的内存赋值为零。 所以static变量的生命周期就和全局变量一样是程序运行的整个生命周期，那static变量和全局变量又有什么区别呢？我们想用多个函数操作同一个变量就用全局变量不就可以了？为什么还要有static变量呢？ 对于static局部变量，它和全局变量的区别是： 作用域不同，局部变量限制其作用域在函数内，仅在函数内可见，外部无法访问，提高了封装性。 static局部变量的动态初始化在第一次执行到声明语句时初始化，而全局变量在程序启动时就初始化，避免了资源浪费，静态初始化与全局变量一致。 对于static全局变量，它就是在全局变量的基础上限制了作用域为当前文件，即使使用extern外部文件也无法使用。这可以减少同命名变量冲突，减少模块的耦合度。 在这里对static变量有了初步的认识，那我们加上类。 类static变量，顾名思义声明于某个类内的static变量，那它和普通成员变量有什么区别呢？类比上述非类的情况，类static变量的生命周期是整个程序，也就不像对象的成员变量随对象创建消亡，那么它的初始化就要在类外定义和初始化，所有对象共享一个static成员变量。 class A { public: static int count; //类内声明 } int A::count = 0; //类外定义初始化 接下来是static函数，对于类外的static函数，其作用是限制其作用域为本文件内；对于类内的static成员函数，类似于类内的static成员变量，它不属于具体的对象，而是属于类本身，没有具体的this指针，不能访问非static成员变量和非static成员函数。 class A { public: static int count; //类内声明 static void display(){ //static成员函数 std::cout \u003c\u003c \"A::count: \" \u003c\u003c count \u003c\u003c std::endl; } } int A::count = 0; //类外定义初始化 int main(){ A::display(); //直接通过类名调用 return 0; } static成员函数的作用其实更像是提供类级别的函数，实现与类具体对象无关的通用功能。它避免了实例化对象，减少了this指针的开销。 class A { public: static int count; //类内声明 static void display(){ //static成员函数 std::cout \u003c\u003c \"A::count: \" \u003c\u003c count \u003c\u003c std::endl; } static int add(int a, int b){ return a + b; } } int A::count = 0; //类外定义初始化 int main(){ A::display(); //直接通过类名调用 std::cout \u003c\u003c A::add(1, 2) \u003c\u003c std::endl; //实现通用功能 return 0; } 扩展，C++中有四种存储周期： 存储周期 声明周期 示例 用法 静态 static 程序开始到程序结束 全局变量，static局部变量，类的静态成员 用于全局状态变量或需要跨函数的持久状态 动态 dynamic 显示分配到显示释放 使用 new 或 malloc 分配的变量 用于在运行时需要灵活分配内存的场景 自动 automatic 定义时离开作用域 函数内的普通局部变量 用于临时计算或局部操作，效率高 线程 thread_local 线程启动到线程结束 thread_local 修饰的变量 用于线程独立的变量存储，避免多线程冲突 参考：Memory Layout of C Programs ","date":"2025-02-23","objectID":"/posts/cpp-static/:0:0","tags":["C++","static"],"title":"C++ Static","uri":"/posts/cpp-static/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 mmap 先读一下从内核世界透视mmap内存映射的本质（原理篇）这篇文章。 #include \u003csys/mman.h\u003e void* mmap(void* addr, size_t length, int prot, int flags, int fd, off_t offset); 系统调用mmap映射的是虚拟内存中的文件映射与匿名映射区，在这段虚拟内存区域中，包含了一段一段的虚拟映射区，每调用一次mmap就会在文件映射与匿名映射区划分一段作为申请的虚拟内存。 addr：指定映射的虚拟内存起始地址，一般设为NULL，交由内核决定起始地址。 length：申请的内存有多大，决定匿名映射的物理内存有多大或文件映射的文件区域有多大。addr和length必须按照PAGE_SIZE对齐。 如果是文件映射，就要通过参数fd指定要映射文件的描述符，通过offset指定文件映射区在文件的偏移。Linux中内存页和磁盘块大小一般情况都是4KB，这里的offset也必须按照4KB对齐。 mmap映射的虚拟内存在内核中用struct vm_area_struct结构表示，进程中的VMA有两种组织形式，双向链表和红黑树。mmap系统调用本质是先要在虚拟内存空间中划分出一段VMA出来，这段VMA区域的大小由vm_start，vm_end表示，而它们由mmap参数addr、length决定；随后内核会对这段VMA进行相关的映射，如果是文件映射，内核会将映射的文件以及要映射文件区域在文件中的offset，与VMA中的vm_file、vm_pgoff关联映射起来，他们由mmap参数fd、offset决定；mmap映射的这段VMA中的相关权限和标志位，是由mmap参数的prot、flags决定的，最终会映射到VMA中的vm_page_prot、vm_flags中，指定进程对这块VMA的访问权限和相关标志位。PS：进程所依赖的动态链接库.so文件也是通过mmap文件映射的方式将代码段、数据段映射到文件映射与匿名映射区中。 mmap中参数prot指定VMA的访问权限，取值有四种： #define PROT_READ 0x1 /* page can be read */ #define PROT_WRITE 0x2 /* page can be written */ #define PROT_EXEC 0x4 /* page can be executed */ #define PROT_NONE 0x0 /* page can not be accessed */ mmap中参数flags指定VMA映射方式，OS对物理页的管理类型有两种：一种是匿名页、一种是文件页，对应的映射也就分为两种，匿名映射、文件映射，mmap所映射的物理内存能否在多进程间共享又分为共享映射、私有映射两种映射方式，共享映射是多进程共享的，一个进程修改了共享映射的内存其他进程是可以看到的，用于多进程间的通信；私有映射是进程私有的，其他进程看不到，多进程修改同一映射文件将不会回写到磁盘文件上。 按照匿名、文件与共享、私有进行组合，就有四种映射方式。 私有匿名映射：MAP_PRIVATE | MAP_ANONYMOUS，glib库中封装的malloc函数申请内存大于128KB时使用mmap私有匿名映射方式来申请堆内存。这里mmap只是先申请一段VMA，还没有真正分配物理内存，PTE是空的，只有读写此区域时，才会触发缺页异常分配物理内存。私有匿名映射除了用于申请虚拟内存之外，还会用于execve系统调用中，内核需要删除释放旧的虚拟内存空间并情况页表，然后打开可执行文件，解析文件头，判断可执行文件的格式，对应函数进行加载。这个过程就需要私有匿名映射创建新的虚拟内存空间中的BSS段、堆和栈。 私有匿名映射 私有文件映射：MAP_PRIVATE，mmap内存文件映射的本质是vm_area_struct结构体中传入参数fd所对应的file结构体指针，指向file结构体，file结构体中存有inode结构体成员，即该文件对应的inode索引，从而找到对应的磁盘块block。和私有匿名映射一样，它不会立即分配物理页面，而是利用缺页异常，进程1在建立PTE之后，再次访问这段文件内存映射时就相当于直接访问文件的page cache，这个过程是在用户态的，没有切态。进程2同样在访问这段文件时也触发缺页异常，建立PTE，但会直接指向进程1中的page cache。所有进程的PTE都设为只读，当任一进程试图写入时，又会触发缺页中断，会申请一个内存页，然后将page cache的内容拷贝到新内存页中，并更新PTE。当进程都有各自专属的物理内存页时，就和page cache脱离关系了，各自的修改在进程之间时互不可见的，且均不会回写到磁盘文件中。可执行文件的.text、.data就使用私有文件映射到了进程虚拟内存空间中的代码段和数据段中。 私有文件映射多进程 私有文件映射进程空间 共享文件映射：MAP_SHARED，和私有文件映射过程一样，唯一不同的点是，多进程中的虚拟内存映射区通过缺页中断会映射到同一page cache中，且是可读可写，不会再次触发缺页中断去各自分配新的物理页。多进程对共享映射区的任何修改都会通过内核回写线程pdflush刷新到磁盘文件中。根据 mmap 共享文件映射多进程之间读写共享（不会发生写时复制）的特点，常用于多进程之间共享内存（page cache），多进程之间的通讯。 共享文件映射 共享匿名映射：MAP_SHARED | MAP_ANONYMOUS，将fd设为-1来实现共享匿名映射，这种映射方式常用于父子进程之间共享内存，父子进程之间的通讯。这个思路如果按照上述的话，就是进程1缺页中断分配一个物理页，进程2同样缺页中断要分配进程1所指的物理页，但是进程2怎么找到这个物理页？找不到的。但这对于共享文件映射很容易，因为有文件的page cache存在，进程2可以根据offset从page cache中查找是否已经有其他进程把映射的文件内容加载到文件页中。如果文件页已经存在 page cache 中了，进程 2 直接映射这个文件页就可以了。共享匿名映射在内核中是通过一个叫做 tmpfs 的虚拟文件系统来实现的，tmpfs 不是传统意义上的文件系统，它是基于内存实现的，挂载在 dev/zero 目录下。当多个进程通过 mmap 进行共享匿名映射的时候，内核会在 tmpfs 文件系统中创建一个匿名文件，这个匿名文件并不是真实存在于磁盘上的，它是内核为了共享匿名映射而模拟出来的，匿名文件也有自己的 inode 结构以及 page cache。在 mmap 进行共享匿名映射的时候，内核会把这个匿名文件关联到进程的虚拟映射区 VMA 中。这样一来，当进程虚拟映射区域与 tmpfs 文件系统中的这个匿名文件映射起来之后，后面的流程就和共享文件映射一模一样了。由于是基于内存实现的虚拟文件系统，在其他进程是不可见的，而子进程是可见的，适用于父子进程间的共享匿名映射。 总结： 私有匿名映射，其主要用于进程申请虚拟内存，以及初始化进程虚拟内存空间中的 BSS 段，堆，栈这些虚拟内存区域。 私有文件映射，其核心特点是背后映射的文件页在多进程之间是读共享的，多个进程对各自虚拟内存区的修改只能反应到各自对应的文件页上，而且各自的修改在进程之间是互不可见的，最重要的一点是这些修改均不会回写到磁盘文件中。我们可以利用这些特点来加载二进制可执行文件的 .text , .data section 到进程虚拟内存空间中的代码段和数据段中。 共享文件映射，多进程之间读写共享（不会发生写时复制），常用于多进程之间共享内存（page cache），多进程之间的通讯。 共享匿名映射，用于父子进程之间共享内存，父子进程之间的通讯。父子进程之间需要依赖 tmpfs 中的匿名文件来实现共享内存。是一种特殊的共享文件映射。 xv6的mmap函数声明： void* mmap(void *addr, size_t len, int prot, int flags, int fd, off_t offset); 可以看出和上述Linux的mmap函数声明是一致的，本实验就是要实现类似于Unix的上述简化实现。只实现文件映射部分，没有匿名映射。实验说明： 内核自己决定文件映射的虚拟地址，mmap返回该地址，失败返回0xffffffffffffffff。 len是映射的字节数。 prot有可读、可写、可执行三种。 flags是MAP_PRIVATE或MAP_SHARED，即私有文件映射或共享文件映射，就是上述所提的2、3两种情况。 在usertrap()中page fault进行处理分配物理内存。 MAP_SHARED可以不分配物理内存。 munmap函数声明： int munmap(void *addr, size_t len); 移除指定地址范围内的mmap映射。 MAP_SHARED要将修改写入文件，进程退出时也要对MAP_SHARED的任何修改写入文件。 hints： 按照之前的老方法添加mmap和munmap系统调用。 定义VMA结构体，记录映射的虚拟地址空间的起始地址、长度、权限、文件等信息。 声明一个大小为16的VMA数组。 把VMA数组对应到一个未使用的区域。 VMA结构体中应包含一个指向file结构体的指针。 mmap应增加文件的引用计数。 VMA区域引发的page fault进行处理，分配一页物理内存，使用readi","date":"2024-11-24","objectID":"/posts/mit-lab-mmap/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Mmap","uri":"/posts/mit-lab-mmap/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 fs Large files 增加xv6文件大小，一个文件的大小是由inode决定的，inode记录文件的元信息，其中的address数组记录磁盘的块号。我们知道，文件就是一堆磁盘块，磁盘块的数量越多文件越大，所以理论上address数组越大，文件越大。但inode也是占用一个块大小（xv6是1024字节）的，那么address的大小会被限制在二百多，很明显是不够的。这里inode限制address数组大小为13，前12个元素是直接块，即address索引（逻辑块号）与其值（磁盘块号）是一一对应的，第13个元素是间接块，即指向了一个磁盘块，那个磁盘块里又划分了（BSIZE / sizeof(uint)）大小的数组，每个元素指向一个磁盘块，这样就由1个磁盘块扩充为了256个磁盘块。 间接块扩充文件 按照类似的思路，我们可以将上述间接块指向的磁盘块改为间接块，这样建立二级间接块，在二级间接块上指向实际的磁盘块，从而将1个磁盘块扩充为了256*256个磁盘块（这里其实牺牲了部分磁盘块作为间接块而不是文件块）。具体实现代码： static uint bmap(struct inode *ip, uint bn) { uint addr, *a; struct buf *bp; if(bn \u003c NDIRECT){ // 小于直接块数目 if((addr = ip-\u003eaddrs[bn]) == 0){ addr = balloc(ip-\u003edev); // 分配一个磁盘块给文件 if(addr == 0) return 0; ip-\u003eaddrs[bn] = addr; } return addr; } bn -= NDIRECT; if(bn \u003c NINDIRECT){ // 小于间接块数目 // Load indirect block, allocating if necessary. if((addr = ip-\u003eaddrs[NDIRECT]) == 0){ addr = balloc(ip-\u003edev); // 分配一个磁盘块作为间接块 if(addr == 0) return 0; ip-\u003eaddrs[NDIRECT] = addr; } bp = bread(ip-\u003edev, addr); // 从间接块里读 a = (uint*)bp-\u003edata; // 这里转为了uint* 一个指针指向其分配的磁盘块 if((addr = a[bn]) == 0){ // bp的size就是NINDIRECT BSIZE/sizeof(uint) addr = balloc(ip-\u003edev); // 分配一个磁盘块给文件 if(addr){ a[bn] = addr; log_write(bp); } } brelse(bp); return addr; } bn -= NINDIRECT; // 0-256*256 // 0-255 if (bn \u003c NDBINDIRECT) { // 小于二级间接块数目 if((addr = ip-\u003eaddrs[NDIRECT + 1]) == 0){ addr = balloc(ip-\u003edev); // 分配一个磁盘块作为一级间接块 if(addr == 0) return 0; ip-\u003eaddrs[NDIRECT + 1] = addr; } uint cn = bn / NINDIRECT; // 在一级间接块的index bp = bread(ip-\u003edev, addr); a = (uint*)bp-\u003edata; if((addr = a[cn]) == 0) { addr = balloc(ip-\u003edev); // 再分配一个磁盘块作为二级间接块 if(addr){ a[cn] = addr; log_write(bp); } } brelse(bp); uint index = bn % NINDIRECT; // 在二级间接块的index bp = bread(ip-\u003edev, addr); a = (uint*)bp-\u003edata; if((addr = a[index]) == 0) { addr = balloc(ip-\u003edev); // 再分配一个磁盘块给文件 if(addr) { a[index] = addr; log_write(bp); } } brelse(bp); return addr; } panic(\"bmap: out of range\"); } 注意更改fs.h中的对应宏定义。 #define NDIRECT 11 #define NINDIRECT (BSIZE / sizeof(uint)) #define NDBINDIRECT (NINDIRECT * NINDIRECT) #define MAXFILE (NDIRECT + NINDIRECT + NDBINDIRECT) struct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT+2]; // Data block addresses }; 以及file.h中inode的address数组的大小，这里要和dinode的address数组的大小相对应，不然fs.img构建会出错。 // in-memory copy of an inode struct inode { uint dev; // Device number uint inum; // Inode number int ref; // Reference count struct sleeplock lock; // protects everything below here int valid; // inode has been read from disk? short type; // copy of disk inode short major; short minor; short nlink; uint size; uint addrs[NDIRECT+2]; }; 还有根据提示要确保itrunc释放文件的所有块，类似的进行更改，至此能通过usertests。 void itrunc(struct inode *ip) { int i, j; struct buf *bp; uint *a; for(i = 0; i \u003c NDIRECT; i++){ // 释放所有的直接块 if(ip-\u003eaddrs[i]){ bfree(ip-\u003edev, ip-\u003eaddrs[i]); ip-\u003eaddrs[i] = 0; } } if(ip-\u003eaddrs[NDIRECT]){ // 如果一级间接块存在 bp = bread(ip-\u003edev, ip-\u003eaddrs[NDIRECT]); a = (uint*)bp-\u003edata; for(j = 0; j \u003c NINDIRECT; j++){ // 释放所有的文件块 if(a[j]) bfree(ip-\u003edev, a[j]); } brelse(bp); bfree(ip-\u003edev, ip-\u003eaddrs[NDIRECT]); // 释放一级间接块 ip-\u003eaddrs[NDIRECT] = 0; } if (ip-\u003eaddrs[NDIRECT + 1]){ // 如果二级间接块存在 bp = bread(ip-\u003edev, ip-\u003eaddrs[NDIRECT + 1]); a = (uint*)bp-\u003edata; for(j = 0; j \u003c NINDIRECT; j++) { if(a[j]) { struct buf *bp2 = bread(ip-\u003edev, a[j]); uint* c = (uint*)bp2-\u003edata; for (int k = 0; k \u003c NINDIRECT; k++) { if(c[k]) bfree(ip-\u003edev, c[k]); } brelse(bp2); bfree(ip-\u003edev, a[j]); } } brelse(bp); bfree(ip-\u003edev, ip-\u003eaddrs[NDIRECT + 1]); // 释放一级间接块 ip-\u003eaddrs[NDIRECT + 1] = 0; } ip-\u003esize = 0; iupdate(ip); } Symbolic links 这个实验如果明白了符号链接的原理过程就能迎刃而解，本质上就是在path的路径创建了一个新文件（代码以inode表达），在文件里写入target路径，同时标记此文件为T_SYMLINK类型，遇到这种文件要读文件内容再一次以文件内容为path去找对应的inode，从而得到所指向文件的内容。代码具体实现： 增加symlink系统调用，在path处创建一个T_SYMLINK类型的文件，writei写入target路径为文件内容。 uint64 sys_symlink(void) { char target[MAXPATH]; cha","date":"2024-11-17","objectID":"/posts/mit-lab-fs/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:File system","uri":"/posts/mit-lab-fs/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 locks Buffer cache 如果多个进程密集使用文件系统，它们可能会争用bcache.lock，这是一个大锁，保护bcache中的buffers。修改bcache、bget、brelse使不同块中的并发查找和释放不太可能发生锁冲突。不能增加buffer的数量，也就是依旧30个buffer。不需要实现LRU缓存替换策略，但必须当缓存未命中时，能使用任何refcnt为零的buffer。这个refcnt表示当前有多少进程在使用这个buffer，每次bget()得到一个buffer后，会对这个buffer的refcnt加一，只有当refcnt变为0时，才会被回收。 $ bcachetest start test0 test0 results: --- lock kmem/bcache stats lock: kmem: #test-and-set 0 #acquire() 33030 lock: kmem: #test-and-set 0 #acquire() 28 lock: kmem: #test-and-set 0 #acquire() 73 lock: bcache: #test-and-set 0 #acquire() 96 lock: bcache.bucket: #test-and-set 0 #acquire() 6229 lock: bcache.bucket: #test-and-set 0 #acquire() 6204 lock: bcache.bucket: #test-and-set 0 #acquire() 4298 lock: bcache.bucket: #test-and-set 0 #acquire() 4286 lock: bcache.bucket: #test-and-set 0 #acquire() 2302 lock: bcache.bucket: #test-and-set 0 #acquire() 4272 lock: bcache.bucket: #test-and-set 0 #acquire() 2695 lock: bcache.bucket: #test-and-set 0 #acquire() 4709 lock: bcache.bucket: #test-and-set 0 #acquire() 6512 lock: bcache.bucket: #test-and-set 0 #acquire() 6197 lock: bcache.bucket: #test-and-set 0 #acquire() 6196 lock: bcache.bucket: #test-and-set 0 #acquire() 6201 lock: bcache.bucket: #test-and-set 0 #acquire() 6201 --- top 5 contended locks: lock: virtio_disk: #test-and-set 1483888 #acquire() 1221 lock: proc: #test-and-set 38718 #acquire() 76050 lock: proc: #test-and-set 34460 #acquire() 76039 lock: proc: #test-and-set 31663 #acquire() 75963 lock: wait_lock: #test-and-set 11794 #acquire() 16 tot= 0 test0: OK 实验所期望的输出可以看出，lock: bcache的锁争用次数变为了0，且多了13个lock: bcache.bucket，每个bucket的锁争用次数也为0。 需要以“bcache”开头为每个锁调用initlock方法。bcache是多个进程共享的，不能像kalloc一样将所有的buffer切割为每个进程所独享，很明显buffer是固定30个大小，多进程需要都能够访问的。建议使用哈希表来查找缓存中的块号，每个哈希桶持有一把锁。哈希表就是key-value对，那我的key就是hash(块号)，value就是存储的buffer，每个桶还要持有一把锁。 这些锁冲突情况是可以接受的： 两个进程去使用相同的块号。 两个进程查bcache中的buffer（遍历找对应的块号），发现没有，需要查找未使用的块（refcnt=0）进行替换。 两个进程同时使用哈希到一个bucket的块号。 hints： 哈希表大小设为13，固定值，印证了上述结果打印可以通过test。 在哈希表中维护buffer操作必须是原子性的。 删除双向链表形式的bcache且不实现LRU，在bget()中可以选择任何refcnt==0的buffer，在brelse()中也无需获取bcache.lock。 遍历哈希表以及遍历bucket的元素去查找未使用的buffer是可以的。 在驱逐一个buffer时需要持有bcache锁和每个bucket的锁。 正常来说，驱逐一个buffer会将其搬到另一个槽位上（块号hash后得到不同的key），如果在同一个槽位上需要避免死锁。 调试时需要保留全局bcache.lock的acquie/relse在bget()的开头和结尾，一旦代码正确remove这个全局锁。 使用xv6的竞争检测器来找潜在的竞争。 实验思路： 建立大小为13的哈希表，要访问的buffer块哈希到桶上，每个桶一把锁，这样就将原来的一把大锁化为了13把小锁。当缓存命中时就直接拿取对应的锁进行更新操作，很容易处理，就是拿到自己桶对应的小锁即可；但当缓存未命中时，就要遍历buffer去找最久未使用的buffer进行驱逐替换处理。这里分为两种情况，如果要驱逐的buffer和新加入的buffer哈希到一个桶上，以及未哈希到一个桶上。不在同一个桶上就涉及链表断开、重分配的操作，在同一个桶上就不需要链表操作。 实验要处理的上述代码逻辑主要体现在bget函数上，在bget中就是先获取桶A的锁，去遍历所有桶找最久未使用的buffer，然后拿到对应桶B的锁进行链表断开驱逐buffer操作，释放桶B的锁，将buffer挂在桶A上，再释放桶A的锁。这是我最开始的实现方式，运行xv6后会发现系统卡住不动了，分析是发生了死锁。这里由于是遍历所有桶，就有可能出现这种情况，即CPU1拿到桶A的锁，遍历发现要驱逐的buffer在桶B，于是申请桶B的锁，而同时CPU2拿到桶B的锁，同样遍历发现要驱逐的buffer在桶A，去申请桶A的锁。这样就是CPU1拿着桶A的锁去申请桶B的锁，CPU2拿着桶B的锁去申请桶A的锁，造成死锁。死锁的发生条件：互斥、请求保持、不可剥夺、循环等待。破坏上述4个条件之一就可以避免死锁，这里可以破坏请求保持条件，即在遍历查找要驱逐的buffer时释放自己的桶锁，后边重分配操作时再获取。 上述处理方式虽然解决了死锁的问题，但又会带来一个问题，就是在释放桶A的锁之后，拿到桶B的锁之前，这个间隙可能会同时有一个线程也在对同一个要添加的块进行bget，那么就会同样判断缓存未命中，又去重复进行驱逐替换操作，导致同一区块有多份缓存的情况。那么怎么解决这个问题呢？这里采取的办法是使用一把驱逐锁保护这个间隙，让桶A释放自己的锁之后，拿到桶B的锁之前这个过程串行化。并且在获取这个驱逐锁后再次判断是否缓存命中，这样就保证只有第一个进入的线程对该块进行了缓存操作，以下是具体的代码实现。 在buf结构体中新增lastuse成员变量，维护一个时间戳，方便查找最久未使用的buffer。 struct buf { int valid; // has data been read from disk? int disk; // does disk \"own\" buf? uint dev; uint blockno; struct sleeplock lock; uint refcnt; uint lastuse; //struct buf *prev; // LRU cache list struct buf *next; uchar data[BSIZE]; }; bio.c代码： #include \"types.h\" #include \"param.h\" #include \"spinlock.h\" #include \"sleeplock.h\" #include \"riscv.h\" #include \"defs.h\" #include \"fs.h\" #include \"buf.h\" #define NBUCKET 13 #define HASH(dev, blockno) (((dev) \u003c\u003c 27 | (blockno)) % NBUCKET) struct { struct buf buf[NBUF]; struct buf bufmap[NBUCKET]; // 哈希桶 struct spinlock bucketlocks[NBUCKET]; // 每个桶一把锁 struct spinlock evictionlocks[NBUCKET]; // 每个桶一把驱逐锁 } bcache; void binit(void) { for (int i = 0; i \u003c NBUCKET; i++) { initlock(\u0026bcache.bucketlocks[i], \"bcache_bucket_lock\"); init","date":"2024-11-10","objectID":"/posts/mit-lab-lock/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Locks","uri":"/posts/mit-lab-lock/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 cow 写时复制版的fork，思路是在fork时，uvmcopy会copy父进程的页表到子进程，建立对应的映射，但不去分配物理内存，也就是建立映射时，指向的物理内存是父进程的物理页，父子进程的虚拟地址同时映射到同一物理页。设置PTE为不可写，设置COW标志位。这样父进程或子进程其中一个试图写时，触发缺页故障，陷入内核，在usertrap中根据trap的类型处理trap，这里就是解除原PTE的映射，新分配一个物理页，建立到该物理页的映射，并且对应的PTE清除COW标志位，设为可写。 此外，还需要考虑的点是，因为同一物理页可能被多个进程映射了，那么kfree就需要判断什么时候正确释放物理页。这里使用引用计数，建立一个引用计数数组，维护每个物理页的引用计数（有多少进程虚拟地址映射到了该物理页）。kalloc时设置所分配的物理页引用计数为1，每当fork时uvmcopy复制映射就会将引用计数加一，调用kfree时先将引用计数减一，判断引用计数是否为0，从而再释放物理页面。（这里和智能指针的引用计数机制如出一辙） 按照这个思路simple的样例是可以通过的，但之后的就不会通过了。是因为多进程下对全局共享的引用计数数组需要加锁避免竞态条件下的错误。更改之后，file样例又不能通过，出现pipe() panic，是因为还需更改copyout()，这个函数是xv6实现的软件访问页表，将内核数据拷贝到用户态，这里不会触发缺页异常，而显然，如果要拷贝到用户态不可写的物理页上，就需要实现上述COW，所以在此函数增加检测COW物理页，并对应进程COW处理。至此，COW所有的test都可以通过。以下是代码实现的具体细节。 更改uvmcopy()： int uvmcopy(pagetable_t old, pagetable_t new, uint64 sz) { pte_t *pte; uint64 pa, i; uint flags; //char *mem; for(i = 0; i \u003c sz; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) panic(\"uvmcopy: pte should exist\"); if((*pte \u0026 PTE_V) == 0) panic(\"uvmcopy: page not present\"); pa = PTE2PA(*pte); // 可读可写 不可读可写 // 可读不可写 不可读不可写 // 对于可写页进行COW if (*pte \u0026 PTE_W) { *pte = *pte \u0026 ~PTE_W; *pte = *pte | PTE_COW; } // 把父进程的权限标志位复制到子进程 flags = PTE_FLAGS(*pte); // 建立到原物理页的映射 if(mappages(new, i, PGSIZE, (uint64)pa, flags) != 0){ goto err; } // 原物理页的引用计数加一 acquire(\u0026refcntlock); refcnt[GETPAINDEX((uint64)pa)]++; release(\u0026refcntlock); // if((mem = kalloc()) == 0) // goto err; // memmove(mem, (char*)pa, PGSIZE); // if(mappages(new, i, PGSIZE, (uint64)mem, flags) != 0){ // kfree(mem); // goto err; // } } return 0; err: uvmunmap(new, 0, i / PGSIZE, 1); return -1; } 检查COW物理页并进行COW处理函数，注意这里va不能等于MAXVA，这是个大坑，usertests一直MAXVA样例不通过就是没有加=这个原因。 int uvmcheckcow(uint64 va) { // 虚拟地址vm对应的页面是否是COW页 if (va \u003e= MAXVA) return 0; pte_t *pte; struct proc *p = myproc(); if((pte = walk(p-\u003epagetable, va, 0)) == 0) return 0; return va \u003c p-\u003esz \u0026\u0026 (*pte \u0026 PTE_V) \u0026\u0026 (*pte \u0026 PTE_COW); } int uvmcopycow(uint64 va) { if (va \u003e= MAXVA) return -1; pte_t *pte; uint flags; uint64 pa; uint64 mem; struct proc *p = myproc(); if((pte = walk(p-\u003epagetable, va, 0)) == 0) return -1; // 分配一个新的物理页面 并把原物理页面引用计数减1 pa = PTE2PA(*pte); if ((mem = (uint64)cowalloc((void *)pa)) == 0) return -1; // 清除COW 设置为可写 flags = PTE_FLAGS(*pte); if (*pte \u0026 PTE_COW) { flags = flags \u0026 ~PTE_COW; flags = flags | PTE_W; } // 解除原页面映射 会把PTE设为0 uvmunmap(p-\u003epagetable, PGROUNDDOWN(va), 1, 0); // 这里不做dofree // 建立新页面映射 if(mappages(p-\u003epagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, flags) != 0){ return -1; } return 0; } usertrap增加对应的page fault处理： if(r_scause() == 8){ // system call if(killed(p)) exit(-1); // sepc points to the ecall instruction, // but we want to return to the next instruction. p-\u003etrapframe-\u003eepc += 4; // an interrupt will change sepc, scause, and sstatus, // so enable only now that we're done with those registers. intr_on(); syscall(); } else if((which_dev = devintr()) != 0){ // ok } else if ((r_scause() == 13 || r_scause() == 15) \u0026\u0026 uvmcheckcow(r_stval())) { // 发生页面错误 且是COW页 // 进行写时复制 if (uvmcopycow(r_stval()) == -1) setkilled(p); // 没有可用的物理内存 杀死进程 } else { printf(\"usertrap(): unexpected scause 0x%lx pid=%d\\n\", r_scause(), p-\u003epid); printf(\" sepc=0x%lx stval=0x%lx\\n\", r_sepc(), r_stval()); setkilled(p); } 引用计数需要维护的数组及锁： struct spinlock refcntlock; int refcnt[(PHYSTOP - KERNBASE) / PGSIZE]; #define GETPAINDEX(p) (p - KERNBASE) / PGSIZE // 放在defs.h 不要放在kalloc.c void kinit() { initlock(\u0026kmem.lock, \"kmem\"); initlock(\u0026refcntlock, \"refcnt\"); freerange(end, (void*)PHYSTOP); } kalloc和kfree增加引用计数判断： void kfree(void *pa) { struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa \u003c end || (uint64)pa \u003e= PHYSTOP) panic(\"kfree\"); // 页面引用计数 \u003c= 0 时释放 acquire(\u0026refcntlock); if (--refcnt[GETPAINDEX((uint64)pa)] \u003c= 0) { // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); r = (struct run*)pa; acquire(\u0026kmem.lock); r-\u003enext = kmem.freelist; kmem.freelist = r; release(\u0026kmem.lock); } release(\u0026refcntlock); } void * kalloc(void) { struct run *r; acquire(\u0026kmem.lock); r = kmem.freelist; if(r)","date":"2024-10-06","objectID":"/posts/mit-lab-cow/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Copy-on-Write Fork for xv6","uri":"/posts/mit-lab-cow/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 trap RISC-V assembly 哪些寄存器包含函数的参数？比如，哪个寄存器保存printf函数中的13？ a0、a1、a2寄存器保存函数参数。a2寄存器保存printf函数中的13。 f函数调用在哪里？g函数调用在哪里？ 没有具体的调用，实际都做了函数内联优化，直接执行x+3。 函数printf位于哪个地址？ 位于0x6bc地址。 30: 68c000ef jal 6bc \u003cprintf\u003e 在printf的jalr之后，ra寄存器的值是多少？ 0x34，执行main中下一条指令。 代码输出是什么？如果RISC-V是大端序，你会将i设为多少才能得到相同的输出？你会将57616改为其他不同的值吗？ unsigned int i = 0x00646c72; printf(\"H%x Wo%s\", 57616, (char *) \u0026i); 输出：HE110 World。强转为char*是指从变量i的地址处按字节读取变量i，如果不取i的地址，那就是在0x00646c72地址处按字节读取变量i，这是错的。当前是小端序，低字节低地址，所以按0x72、0x6c、0x64、0x00的顺序读，转换为rld。若是大端序，则将i设为0x726c6400才能得到相同的输出。57616不需要变，因为对应的十六进制不会改变。 代码“y=”之后会打印什么？ printf(\"x=%d y=%d\", 3); 打印的是不确定的值，printf会错误读取栈上某个随机的值，是未定义行为。 Backtrace error发生后定位其函数调用链进行回溯。编译器生成了包含当前调用链每个函数栈帧的机器码，每个栈帧包含返回地址和指向调用它的栈帧指针。s0寄存器包含一个指向当前栈帧的指针。讲义中的这张图清晰的展示了栈帧结构，从s0寄存器得到栈帧指针fp，然后在fp-8的位置是返回地址，在fp-16的位置是保存的上一个调用它的fp。 栈帧结构 思路就是循环遍历fp的值，从而读取对应栈帧的返回地址并打印。什么时候循环停止，提示说所有栈帧都在同一页上，所以超出该页范围即停止，实现代码： void backtrace(void) { uint64 fp = r_fp(); uint64 fpmin = PGROUNDDOWN(fp); uint64 fpmax = PGROUNDDOWN(fp) + PGSIZE; while (fp \u003c fpmax \u0026\u0026 fp \u003e= fpmin) { printf(\"%p\\n\", (uint64 *)*(uint64 *)(fp-8)); fp = *(uint64*)(fp-16); } } Alarm 添加一个新的sigalarm(interval, handler)系统调用，应用程序调用sigalarm(n, fn)时，则在程序消耗CPU时间的每n个ticks时调用应用程序函数fn。这里alarmtest划分了四个test，一个一个来做。 首先就是添加sigalarm和sigreturn两个系统调用，参照syscall实验，主要是得到用户参数，传递到proc结构中，新建的计数变量ticks和中断处理程序handler函数指针。sys_sigreturn在这个test先直接return 0;即可。 uint64 sys_sigalarm(void) { int ticks; uint64 handler_addr; argint(0, \u0026ticks); argaddr(1, \u0026handler_addr); myproc()-\u003eticks = ticks; myproc()-\u003ehandler = (void (*)())handler_addr; return 0; } 因为每次定时器中断都会在usertrap中进行处理，所以在处理函数中增加统计tick数量，若达到ticks即执行用户程序handler，这里需要将sepc寄存器的值设为handler的地址，从而在trap返回时指向handler程序的地址。这样就完成了test0。 if(which_dev == 2) { p-\u003epassticks ++; if (p-\u003eticks \u003e 0 \u0026\u0026 p-\u003epassticks == p-\u003eticks) { p-\u003epassticks = 0; // 返回用户程序handler地址处 p-\u003etrapframe-\u003eepc = (uint64)p-\u003ehandler; } yield(); } test1要求在从handler返回时，要恢复中断之前的寄存器的值，类比于trapframe保存寄存器的值，这里新增另一个atrapframe来保存中断之前的寄存器，在sigreturn时再进行恢复。 if(which_dev == 2) { p-\u003epassticks ++; if (p-\u003eticks \u003e 0 \u0026\u0026 p-\u003epassticks == p-\u003eticks) { // 把定时器中断前的寄存器保存到atrpframe中 memmove(p-\u003eatrapframe,p-\u003etrapframe,sizeof(struct trapframe)); p-\u003epassticks = 0; // 返回用户程序handler地址处 p-\u003etrapframe-\u003eepc = (uint64)p-\u003ehandler; } yield(); } uint64 sys_sigreturn(void) { struct proc* p = myproc(); // 恢复定时器中断前的寄存器 如果不恢复此时寄存器值是用户handler程序的寄存器值 原返回程序的寄存器值被覆盖所以寄 memmove(p-\u003etrapframe,p-\u003eatrapframe,sizeof(struct trapframe)); return 0; } test2要求不能重复执行中断处理程序，即在进程结构体中增加标志位表示当前是否在执行handler，初始为0，在调用handler时设为1，sigreturn时设为0。 if(which_dev == 2) { p-\u003epassticks ++; if (p-\u003eticks \u003e 0 \u0026\u0026 p-\u003epassticks == p-\u003eticks \u0026\u0026 p-\u003ealarmflag == 0) { // 把定时器中断前的寄存器保存到atrpframe中 memmove(p-\u003eatrapframe,p-\u003etrapframe,sizeof(struct trapframe)); p-\u003epassticks = 0; // 返回用户程序handler地址处 p-\u003etrapframe-\u003eepc = (uint64)p-\u003ehandler; // 此进程在执行handler p-\u003ealarmflag = 1; } yield(); } uint64 sys_sigreturn(void) { struct proc* p = myproc(); // 恢复定时器中断前的寄存器 如果不恢复此时寄存器值是用户handler程序的寄存器值 原返回程序的寄存器值被覆盖所以寄 memmove(p-\u003etrapframe,p-\u003eatrapframe,sizeof(struct trapframe)); p-\u003ealarmflag = 0; return 0; } test3要求sigreturn返回时a0寄存器的值要是中断之前的a0寄存器的值，sigreturn返回0会将a0设为0，所以这里直接返回保存的trapframe中的a0的值。 uint64 sys_sigreturn(void) { struct proc* p = myproc(); // 恢复定时器中断前的寄存器 如果不恢复此时寄存器值是用户handler程序的寄存器值 原返回程序的寄存器值被覆盖所以寄 memmove(p-\u003etrapframe,p-\u003eatrapframe,sizeof(struct trapframe)); p-\u003ealarmflag = 0; return p-\u003etrapframe-\u003ea0; } 以上所有在proc.h中新增的结构体成员，其生命周期需要在proc.c中添加对应代码，主要体现在allocproc.c函数及freeproc.c函数中，这里没有给出。至此，alarmtest所有test都通过。 ","date":"2024-09-29","objectID":"/posts/mit-lab-traps/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Traps","uri":"/posts/mit-lab-traps/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 pgtbl Speed up system calls 加速系统调用：在内核空间和用户空间共享只读区域的数据，这样不需要跨内核，减少了用户态和内核态切换的开销。这里就是指频繁的系统调用会带来内核态与用户态的切换开销，为了减少这个开销，做法有： 可以减少系统调用的次数（如设置用户buffer和内核buffer缓存系统调用请求数据，到达一定量时再一次性执行系统调用，典型的就是read()、write()） 使用协程，用户态的线程，不会跨核 本实验做法，内核空间与用户空间虚拟映射到同一个只读的物理页面，避免内核态和用户态切换（Linux称为VDSO） 本实验通过fork子进程，实现用户态程序ugetpid()和系统调用getpid()一样的效果。ugetpid()实现已经给出来，是读虚拟地址USYSCALL处的内容。所以需要在fork创建子进程时（内核态），向USYSCALL地址（用户态虚拟地址空间）写入struct usyscall，对应分配了一个物理页，那么之后用户态ugetpid就能直接在USYSCALL处读数据，而不跨核。 int ugetpid(void) { struct usyscall *u = (struct usyscall *)USYSCALL; return u-\u003epid; } 代码实现参照trapframe生命周期代码，在进程分配trapframe内存、建立trapframe映射、释放trapframe内存这三部分，实现usyscall，存入当前进程的pid。 // kernel/proc.c // static struct proc* allocproc(void) // Allocate a trapframe page. if((p-\u003etrapframe = (struct trapframe *)kalloc()) == 0){ freeproc(p); release(\u0026p-\u003elock); return 0; } // Allocate a usyscall page if ((p-\u003eusyscall = (struct usyscall *)kalloc()) == 0) { freeproc(p); release(\u0026p-\u003elock); return 0; } p-\u003eusyscall-\u003epid = p-\u003epid; // kernel/proc.c // pagetable_t proc_pagetable(struct proc *p) // map the trapframe page just below the trampoline page, for // trampoline.S. if(mappages(pagetable, TRAPFRAME, PGSIZE, (uint64)(p-\u003etrapframe), PTE_R | PTE_W) \u003c 0){ uvmunmap(pagetable, TRAMPOLINE, 1, 0); uvmfree(pagetable, 0); return 0; } if(mappages(pagetable, USYSCALL, PGSIZE, (uint64)(p-\u003eusyscall), PTE_R | PTE_U) \u003c 0){ uvmunmap(pagetable, TRAPFRAME, 1, 0); uvmunmap(pagetable, TRAMPOLINE, 1, 0); uvmfree(pagetable, 0); return 0; } // kernel/proc.c static void freeproc(struct proc *p) { if(p-\u003etrapframe) kfree((void*)p-\u003etrapframe); p-\u003etrapframe = 0; if(p-\u003eusyscall) kfree((void*)p-\u003eusyscall); p-\u003eusyscall = 0; if(p-\u003epagetable) proc_freepagetable(p-\u003epagetable, p-\u003esz); p-\u003epagetable = 0; p-\u003esz = 0; p-\u003epid = 0; p-\u003eparent = 0; p-\u003ename[0] = 0; p-\u003echan = 0; p-\u003ekilled = 0; p-\u003exstate = 0; p-\u003estate = UNUSED; } 总结： 这个实验深刻认识到，内核空间是可以操作用户空间的，不要有固有思维内核空间只操作内核，用户空间只操作用户程序。是内核的权限高，可以操作任何代码，这里就可以拿到p-\u003etrapframe，你会说这不是进程用户空间的吗？实际上kernel代码是可以操作这页的。所以这个实验开始困惑的是用户空间和内核共享只读区域是什么，这里的答案就是用户空间内核都可以读写，是将内核操作的结果放到了用户可以读的用户地址空间，仅此而已，不是什么用户虚拟地址、内核虚拟地址映射到一个物理内存，这里没有涉及内核虚拟地址。 回答最后可以speed up的syscall，很明显就是只涉及读取内核态的结果的系统调用，不对内核态进行修改，如getpid()、uptime()、getuid()、sbrk(0)等。再次强调，这里的加速是指不进行跨核。 Print a page table 编写一个函数打印页表的内容。xv6是三级页表，打印的形式是..2级页表PTE，.. ..2级PTE下的所有1级PTE，.. .. ..1级PTE下的所有0级PTE。根据提示，可以参考freewalk递归遍历所有PTE。 freewalk这段代码递归实现了所有页表的释放，是理解递归实现的很好示例，它的出口就是到了叶子页（L0级页表）不再递归，然后交由上层递归kfree本级页表。 void freewalk(pagetable_t pagetable) { // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i \u003c 512; i++){ pte_t pte = pagetable[i]; // PTE有效 \u0026\u0026 PTE不是叶子页（没有RWX位） if((pte \u0026 PTE_V) \u0026\u0026 (pte \u0026 (PTE_R|PTE_W|PTE_X)) == 0){ // this PTE points to a lower-level page table. uint64 child = PTE2PA(pte); freewalk((pagetable_t)child); pagetable[i] = 0; } else if(pte \u0026 PTE_V){ // freewalk之前要uvmunmap叶子页 panic(\"freewalk: leaf\"); } } kfree((void*)pagetable); // kfree的位置很重要 } 参考freewalk实现的，就是用递归深度控制遍历层级，这里遍历的depth从0到2，对应着2、1、0三级页表（freewalk的depth相当于是从0到1），然后去打印对应的二进制就行。注意使用void *转换，能输出对应%p格式，以及这里的虚拟地址，是依赖上层pte的基地址的，所以也需要传递上层pte的虚拟地址到下层递归中。 void vmprintsub(pagetable_t pagetable, int depth, uint64 base) { if (depth \u003e 2) return; for(int i = 0; i \u003c 512; i++){ pte_t pte = pagetable[i]; if(pte \u0026 PTE_V){ uint64 child = PTE2PA(pte); printf(\"..\"); // 2 .. int count = depth; while (count--) { // 1 .. .. printf(\" ..\"); // 0 .. .. .. } uint64 va = base + (i \u003c\u003c PXSHIFT(2-depth)); printf(\"%p: pte %p pa %p\\n\", (void *)va, (void *)pte, (void *)PTE2PA(pte)); vmprintsub((pagetable_t)child, depth + 1, va); } } } void vmprint(pagetable_t pagetable) { // your code here printf(\"page table %p\\n\", pagetable); // 打印页表项 // ..va: pte pa // .. ..va: pte pa // .. .. ..va: pte pa vmprintsub(pagetable, 0, 0); } Use superpages 使用超级页，RISC-V分页硬件支持2M的页面，比普通4KB更大的页面称为超级页，2M的页面称为兆页面。OS设置L1级的PTE中的PTE_V和PTE_R位，并设置物理页号为指向2MB物理内存的区域起始位置。使用超级页可以减少页表使用的物理内存量，并可以减少TLB缓冲中未命中的次数，从而大幅提升性能。 题目中提示了，通过设置L1级的PTE直接映射到物理地址，因为2M的页面对应的就是2^(9+12)。所以代码就是在所有有关分配超级页内存时页表相关的操作。 首先在kalloc.c","date":"2024-09-22","objectID":"/posts/mit-lab-pgtbl/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Page tables","uri":"/posts/mit-lab-pgtbl/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 syscall System call tracing 实现系统调用trace，trace接受一个参数（整数掩码），如果系统调用号在掩码中设置，需要修改xv6内核，使其在每个系统调用即将返回时打印一行（进程ID、系统调用名称、返回值）。 用户程序需要调用system call，那么所提供的system call的函数声明在user.h，而实际的定义实现却是在kernel中，要知道用户态的代码和内核态的代码是隔离的，那么用户程序是如何调用内核的代码实现的呢？答案是user.pl，user.pl是在用户空间的脚本，作为桥梁，在makefile时生成user.S，这个汇编代码include了syscall.h，syscall.h声明了系统调用号（对应kernel system call 函数），把这个编号放到寄存器a7中，然后ecall进内核，在ecall时会将a7寄存器的值存到trapframe中，从而传到内核中，这样内核从trapframe中加载a7寄存器的值，从而调用对应的system call。 本实验是要实现system call，但是需要shell去测试验证，所以依旧需要先实现一个用户程序trace，在这个程序中调用system call，这里源代码已经提供了trace用户程序。需要增加的是上述关于调用内核system call的操作。 ecall会调用trampoline.S代码，这段代码会执行内核的usertrap函数，当r_scause() == 8时会调用syscall()，syscall()会取a7寄存器的值，然后调用对应syscall函数数组索引的系统调用函数，返回值存在trapframe的a0寄存器里。所以就在syscall()里判断是否是要跟踪的系统调用号，如果是，打印进程ID、系统调用名称和返回值。 整体实现过程就是：1. 添加新增的与syscall有关的函数存根、声明等。2. 在syscall()中拿到用户参数掩码，与当前系统调用号进行按位与运算，如果是1则打印。3. 注意根据提示，增加系统调用名称数组以及在fork时将父进程的mask也copy到子进程中。 主要的代码实现： // kernel/syscall.c // ... // Prototypes for the functions that handle system calls. extern uint64 sys_fork(void); extern uint64 sys_exit(void); extern uint64 sys_wait(void); extern uint64 sys_pipe(void); extern uint64 sys_read(void); extern uint64 sys_kill(void); extern uint64 sys_exec(void); extern uint64 sys_fstat(void); extern uint64 sys_chdir(void); extern uint64 sys_dup(void); extern uint64 sys_getpid(void); extern uint64 sys_sbrk(void); extern uint64 sys_sleep(void); extern uint64 sys_uptime(void); extern uint64 sys_open(void); extern uint64 sys_write(void); extern uint64 sys_mknod(void); extern uint64 sys_unlink(void); extern uint64 sys_link(void); extern uint64 sys_mkdir(void); extern uint64 sys_close(void); extern uint64 sys_trace(void); // 在外部.c文件中的sys_trace // An array mapping syscall numbers from syscall.h // to the function that handles the system call. static uint64 (*syscalls[])(void) = { // 函数指针数组 [SYS_fork] sys_fork, [SYS_exit] sys_exit, [SYS_wait] sys_wait, [SYS_pipe] sys_pipe, [SYS_read] sys_read, [SYS_kill] sys_kill, [SYS_exec] sys_exec, [SYS_fstat] sys_fstat, [SYS_chdir] sys_chdir, [SYS_dup] sys_dup, [SYS_getpid] sys_getpid, [SYS_sbrk] sys_sbrk, [SYS_sleep] sys_sleep, [SYS_uptime] sys_uptime, [SYS_open] sys_open, [SYS_write] sys_write, [SYS_mknod] sys_mknod, [SYS_unlink] sys_unlink, [SYS_link] sys_link, [SYS_mkdir] sys_mkdir, [SYS_close] sys_close, [SYS_trace] sys_trace, }; char* sysnames[] = { [SYS_fork] \"fork\", [SYS_exit] \"exit\", [SYS_wait] \"wait\", [SYS_pipe] \"pipe\", [SYS_read] \"read\", [SYS_kill] \"kill\", [SYS_exec] \"exec\", [SYS_fstat] \"fstat\", [SYS_chdir] \"chdir\", [SYS_dup] \"dup\", [SYS_getpid] \"getpid\", [SYS_sbrk] \"sbrk\", [SYS_sleep] \"sleep\", [SYS_uptime] \"uptime\", [SYS_open] \"open\", [SYS_write] \"write\", [SYS_mknod] \"mknod\", [SYS_unlink] \"unlink\", [SYS_link] \"link\", [SYS_mkdir] \"mkdir\", [SYS_close] \"close\", [SYS_trace] \"trace\", }; void syscall(void) { int num; struct proc *p = myproc(); num = p-\u003etrapframe-\u003ea7; if(num \u003e 0 \u0026\u0026 num \u003c NELEM(syscalls) \u0026\u0026 syscalls[num]) { // Use num to lookup the system call function for num, call it, // and store its return value in p-\u003etrapframe-\u003ea0 p-\u003etrapframe-\u003ea0 = syscalls[num](); if ((p-\u003emask \u0026 (1 \u003c\u003c num)) == (1 \u003c\u003c num)) { // 代码中对uint64会赋值-1 导致打印a0时出现问题 make grade Test trace children过不了 if (p-\u003etrapframe-\u003ea0 == -1) { // 这里单独增加了判断 printf(\"%d: syscall %s -\u003e %d\\n\", p-\u003epid, sysnames[num], -1); }else { printf(\"%d: syscall %s -\u003e %lu\\n\", p-\u003epid, sysnames[num], p-\u003etrapframe-\u003ea0); } } } else { printf(\"%d %s: unknown sys call %d\\n\", p-\u003epid, p-\u003ename, num); p-\u003etrapframe-\u003ea0 = -1; } } // kernel/sysproc.c // ... // 和进程相关的系统调用实现放在此文件里 uint64 sys_trace(void) { // 注意这里的传入参数全是void // 因为使用argint内核函数来获取放到trampframe中的寄存器的值 // 用户程序trace有传入一个参数 int int n; argint(0, \u0026n); // 提取第一个用户参数 struct proc *p = myproc(); p-\u003emask = n; return 0; } // kernel/proc.c // ... // Create a new process, copying the parent. // Sets up child kernel stack to return as if from fork() system call. int fork(void) { int i, pid; struct proc *np; struct proc *p = myproc(); // Allocate process. if((np = ","date":"2024-09-15","objectID":"/posts/mit-lab-syscall/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:System calls","uri":"/posts/mit-lab-syscall/"},{"categories":["操作系统"],"content":"MIT6.1810 lab xv6 2024 util Boot xv6 首先是搭建环境，这里使用CLion + WSL2的环境进行开发，git克隆代码仓库： git clone git://g.csail.mit.edu/xv6-labs-2024 将项目放到了github中进行版本管理，remove掉了原远程仓库，ssh连接到自己新建的仓库，push上去就可以。 git remote remove origin 安装 lab tools ，我这里使用的是Ubuntu-22.04，安装的QEMU是6.0系列，这里用不了。提示说需要QEMU 7.2+、GDB 8.3+，对应的Ubuntu-24之后，汗流浃背了。搜了一下WSL2还能同时安装不同版本的Ubuntu，于是下了Ubuntu-24.04，make qemu后正常运行，使用ctrl-a x退出终端。 此外，CLion在make后会提示没有all这个目标的构建错误，使用WSL为默认的编译工具链，并修改makefile设置，将target设为qemu即可，解析完后此时是能实现代码跳转的。 CLion配置 配置CLion gdb debug参考这个链接的方法。 注意请勿随意修改.git目录和原origin指向的链接，所以上述remove原origin仓库是不合适的，应该是添加github仓库，然后分支push，这样就能将原仓库代码保存到自己github上进行版本管理。 git remote add github git@github.com:UAreFree/xv6-labs-2024.git git push github util:util Sleep 题意：实现用户级别的程序sleep，即暂停指定的时间，这里的时间刻度是定时器中断时间间隔。 提示：第一反应是这个定时器中断时间间隔怎么实现，这里提示使用sleep系统调用，那就不需要考虑了，整体应该是考察如何实现user中的shell程序。 在Makefile中添加用户级程序，$U 是用户程序的目录，每个程序对应一个.c或.S文件，经过编译后生成对应的.o文件。 UPROGS=\\ $U/_cat\\ $U/_echo\\ $U/_forktest\\ $U/_grep\\ $U/_init\\ $U/_kill\\ $U/_ln\\ $U/_ls\\ $U/_mkdir\\ $U/_rm\\ $U/_sh\\ $U/_stressfs\\ $U/_usertests\\ $U/_grind\\ $U/_wc\\ $U/_zombie\\ $U/_sleep\\ 新建sleep.c文件，代码实现： #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" int main(int argc, char *argv[]) { // 用户忘记传参数 打印错误信息 if(argc \u003c= 1){ fprintf(2, \"usage: sleep [int]\\n\"); exit(1); } // 使用sleep系统调用 sleep(atoi(argv[1])); exit(0); } 运行./grade-lab-util sleep进行代码测试。 Pingpong 依旧用户程序，通过一对管道在两个进程间“乒乓”发送一个字节，父进程向子进程发送一个字节，子进程打印\": received ping\"，并将该字节通过管道写入父进程，然后退出；父进程从子进程读取该字节，打印\": received pong\"。 pipe管道p[0]是读，p[1]是写，在读管道时，关闭写端；在写管道时，关闭读端。这里用了两个管道（一对），对每个管道按照上述原则进行读写就可以了，以及注意exit和wait在父子进程中的使用。 新建pingpong.c文件，代码实现： #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" int main(int argc, char *argv[]) { int pipe1[2], pipe2[2]; pipe(pipe1); pipe(pipe2); char c = 'p'; // 父进程向子进程发送一个字节 通过管道 int pid = fork(); if (pid \u003c 0) { fprintf(2, \"fork failed\\n\"); exit(1); } if (pid == 0) { // 从pipe1里读 close(pipe1[1]); read(pipe1[0], \u0026c, 1); close(pipe1[0]); fprintf(1,\"%d: received ping\\n\", getpid()); // 向pipe2里写 close(pipe2[0]); write(pipe2[1], \u0026c, 1); close(pipe2[1]); exit(0); }else { // 向pipe1里写p close(pipe1[0]); write(pipe1[1], \u0026c, 1); close(pipe1[1]); // 等子进程退出 wait(0); // 从pipe2里读p close(pipe2[1]); read(pipe2[0], \u0026c, 1); close(pipe2[0]); fprintf(1,\"%d: received pong\\n\", getpid()); } // 子进程向父进程发送上述同一个字节 通过另一个管道 exit(0); } Find 查找目录树中所有具有指定名称的文件，向子目录递归查询。 示例是输入了两个参数，第一个是“.”，是根目录；第二个是要查找的文件名，也就是在所有目录中查找。下边是阅读ls实现的代码注释，按照这个思路，find就是ls目录树所有文件，找到与文件名相同的文件路径，所以就是当ls遇到目录时继续ls进行递归实现，再加上文件名判断打印即可。 首先阅读ls代码，使用open()得到文件描述符fd，再使用fstat()将fd的文件信息存入st结构体中。根据st中所示文件的类型，进行对应处理，如是文件直接打印，如是目录还需打印目录项所有条目。 void ls(char *path) { char buf[512], *p; int fd; struct dirent de; struct stat st; if((fd = open(path, O_RDONLY)) \u003c 0){ fprintf(2, \"ls: cannot open %s\\n\", path); return; } if(fstat(fd, \u0026st) \u003c 0){ // fstat系统调用 把fd文件的信息存入st结构体中 fprintf(2, \"ls: cannot stat %s\\n\", path); close(fd); return; } switch(st.type){ case T_DEVICE: case T_FILE: printf(\"%s %d %d %d\\n\", fmtname(path), st.type, st.ino, (int) st.size); // 文件直接打印 路径名 文件类型 inode号 文件大小 break; case T_DIR: if(strlen(path) + 1 + DIRSIZ + 1 \u003e sizeof buf){ printf(\"ls: path too long\\n\"); break; } strcpy(buf, path); // 路径名copy到buf中 p = buf+strlen(buf); // 指向buf末尾 *p++ = '/'; // 当前目录末尾+/ 继续指向末尾 while(read(fd, \u0026de, sizeof(de)) == sizeof(de)){ // 目录也是文件 可以read 目录里包含一系列dirent结构体数据（存有inode及对应的路径名） if(de.inum == 0) continue; memmove(p, de.name, DIRSIZ); // 复制目录项的路径名到p中 p[DIRSIZ] = 0; if(stat(buf, \u0026st) \u003c 0){ printf(\"ls: cannot stat %s\\n\", buf); continue; } printf(\"%s %d %d %d\\n\", fmtname(buf), st.type, st.ino, (int) st.size); // 打印该文件信息 } break; } close(fd); } 新建find.c文件，代码实现： #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" #include \"kernel/fs.h\" #include \"kernel/fcntl.h\" void lsa(char *path, char* filename) { char buf[512], *p; int fd; struct dirent de; struct stat st; if((fd = open(path, O_RDONLY)) \u003c 0){ fprintf(2, \"ls: cannot open %s\\n\", path); return; } if(fstat(fd, \u0026st) \u003c 0){ // fstat系统调用 把fd文件","date":"2024-09-08","objectID":"/posts/mit-lab-util/:0:0","tags":["MIT6.1810","Lab"],"title":"MIT6.1810 Lab:Xv6 and Unix utilities","uri":"/posts/mit-lab-util/"}]