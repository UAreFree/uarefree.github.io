# 操作系统导论读书笔记


《Operating Systems:Three Easy Pieces》是Arpaci-Dusseau教授夫妇所著，向Richard Feynman的《费曼物理学讲义》致敬。本书围绕虚拟化、并发和持久化三个主题展开。“教育过程的真正意义在于：前进，学习许多新的和引人入胜的主题，通过学习不断成熟，最重要的是，找到能为你点火的东西。”

&lt;!--more--&gt; 

## 操作系统介绍
冯诺依曼计算模型的基本概念，运行的程序会做一件非常简单的事情：执行指令（CPU是一个无情执行指令的机器，程序是状态机）。处理器不断取指（内存中）、译码、执行、写回。操作系统是一类软件，负责让程序运行变得容易。

操作系统主要利用虚拟化技术将物理资源转换为虚拟形式，虚拟化CPU将单个CPU转换为看似无限数量的CPU，从而让许多程序看似同时运行；虚拟化内存，每个进程访问自己的私有虚拟地址空间，操作系统以某种方式映射到机器的物理内存上，物理内存是操作系统管理的共享资源。

并发问题在同时（并发地）处理很多事情时出现且必须解决。多线程程序对共享计数器必须进行处理，对于这种增加计数值时需要三条指令：1. 将计数器的值从内存加载到寄存器；2. 递增值；3. 将寄存器的值保存回内存。这三条指令不是原子执行（所有指令一次性执行）的，所以会有怪事发生。

断电或系统崩溃，内存中的数据就会丢失，需要硬件（磁盘）和软件（文件系统）持久地存储数据。操作系统不会虚拟化磁盘而是假设用户经常需要共享文件中的信息。文件系统部分的系统调用有open()、write()、close()等，首先确定新数据放到磁盘哪个位置，然后在文件系统所维护的各种结构中进行记录，需要向底层存储设备发出I/O请求。出于性能方面考虑，首先延迟写操作一段时间，将其批量分组为较大的组。为了处理写入期间系统崩溃的问题，文件系统包含某种复杂的写入协议，如日志或写时复制，仔细排序写入磁盘的操作，以确保恢复到合理的状态。

设计一个操作系统的目标：1. 建立一些抽象；2. 提供高性能；3. 在OS和应用程序之间提供隔离；4. 可靠。

## 虚拟化
### 进程
进程：运行中的程序，OS为运行的程序所提供的抽象。进程的机器状态：内存和寄存器。所有OS需要提供的API：创建、销毁、等待、其他控制、状态。OS运行程序必须做的第一件事：将磁盘中的代码和所有静态数据加载到内存中，加载到进程的地址空间中。OS在加载时也是惰性加载，仅在程序执行期间需要加载的代码和数据才会加载。第二件事，为程序运行时栈分配内存空间，也可能分配堆空间。还会执行一些其他初始化任务，特别是I/O相关。默认每个进程有三个文件描述符fd，标准输入、输出和错误。

进程状态：运行、就绪、阻塞。运行和就绪可以相互转换，通过CPU调度和取消调度。阻塞（被某种事件阻塞）需要等待某种事件发生。初始：进程刚创建；最终：已退出但尚未清理（僵尸状态）。这里僵尸进程是指子进程先于父进程退出后，父进程没有释放子进程的PCB，如果父进程不结束，那么一直保持僵尸进程状态，占用系统资源，如果父进程结束了，init进程会自动接手子进程并收尸。与之相对的是孤儿进程，孤儿进程是父进程先于子进程退出，那么会被init进程收养，不会占用系统资源。僵尸进程的处理办法：1. 杀死父进程，大多数情况不可取。2. SIGCHLD信号处理，实际上当子进程终止时，内核会向它的父进程发送一个SIGCHLD信号，父进程可以选择忽略（默认）还是提供处理函数（调用wait或waitpid函数释放子进程占用的资源）。

fork()：这个系统调用确实比较奇怪，初学时很难接受是怎么做到同一个程序同时运行相同的代码的。fork的话会创建一个子进程，子进程copy了父进程（内存和寄存器值），但是返回值不同（子进程返回值为0）。fork的子进程不会执行fork之前的代码。

wait()：父进程可以延迟执行，调用wait等待子进程执行完毕再执行，这个系统调用也会回收子进程的资源。

exec()：可以让子进程执行与父进程不同的程序，exec会从可执行程序中加载代码和静态数据，并覆盖自己的代码段和静态数据，堆、栈及其他内存空间也会被重新初始化，然后OS就会执行该程序，将参数通过argv传给该进程。这里说明exec是完全转头调用所指定的可执行程序。这种分离fork和exec的做法在构建shell时非常有用，因为这给了shell在fork之后exec之前运行代码的机会。比如shell作为主进程在fork了一个子进程后通过exec执行wc系统调用统计文件字符数，可以使用重定向将输出从标准输出改为某个文件，而这实现原理就是在fork之后exec之前关闭标准输出的文件描述符，打开文件，新打开的文件将成为第一个可用的文件描述符，从而向文件中写入。

### 受限直接执行
时分共享CPU实现虚拟化：看起来同时运行多个进程。在保持控制权的同时获得高性能是构建操作系统的主要挑战之一。

用户态和内核态切换：划分用户空间和内核空间区分受限操作执行，内核空间可以做everything，执行受限的指令；用户空间需要执行这些受限的指令就需要执行系统调用，而要执行系统调用就必须执行trap指令，该指令同时跳转入内核，从而执行特权指令，完成后，OS调用一个trap返回指令返回到用户空间。这涉及到内核态到用户态的转换，在trap时处理器需要将一些寄存器保存到每个进程的内核栈上，trap返回时从栈中弹出这些值。open()系统调用和C的过程调用看起来差不多，是的，open()是一个过程调用但内部有trap指令，在执行open()时会用汇编处理参数和返回值以及执行trap指令。然而需要考虑的一点是，trap进内核的代码地址不能是任意的（出于安全考虑），那么每个系统调用或中断在trap进内核后都会被指定对应的处理程序地址，这个对应关系保存在trap表中。

进程间切换：OS作为一个进程（类似的）也要运行在CPU上，如果子进程去执行一个死循环而不主动让出CPU的控制权那么OS就失去了CPU的控制权，唯一办法是重启大法。这显然不合理，需要其他办法获得CPU的控制权。时钟中断，和系统调用一样，必须有对应的中断处理函数，以及在系统开始时也必须启动时钟。

上下文切换：系统开启时钟中断，中断产生后，发生进程切换，也就有进程间的上下文切换。上下文切换就是保存当前执行进程的寄存器的值，恢复即将执行进程的寄存器的值。为了保存当前进程的上下文，OS会执行一些汇编代码来保存通用寄存器、程序计数器、内核栈指针，然后恢复通用寄存器、程序计数器、切换内核栈，以供即将执行进程使用。通过切换栈，内核在进入切换代码调用时，是一个进程的上下文，返回时，是另一个进程的上下文。当OS最终执行从陷阱返回指令时，即将执行的进程变成了当前运行的进程，至此上下文切换完成。

这里可以这样理解，进程进行上下文切换，那么什么是上下文？上下文肯定是描述进程的东西，那么进程由什么来描述？就是PCB，简单来说主要包括寄存器和内存，每个进程有自己运行时寄存器的值，每个进程有自己看起来独有的内存空间（包括栈和堆）。那么上下文就包括这些，切换的也就是寄存器的值、内存空间。具体过程就是保存正在执行的进程A的寄存器（通用寄存器、程序计数器、栈指针等）到内核空间的一块内存中（cpu_context结构体保存），把即将要执行的进程B所保存的上下文内存块中（cpu_context）的值恢复到寄存器中，这是硬件上下文的切换；也别忘了还有内存空间的切换，具体就是切换进程B的pgd（页全局目录的虚拟地址）所对应的页全局目录的物理地址到页表基址寄存器中，当访问用户空间时MMU会通过页表基址寄存器来遍历页表得到物理地址。

而下边表中保存和恢复了两次寄存器，容易误导。实际上下文切换的就是调用switch()例程的一次寄存器的保存和恢复，而另一次是中断所伴随的，即不论是否是上下文切换的场景，只要中断产生都会将用户态的寄存器的值保存到内核栈，然后恢复，这里称为保存现场，不要与进程上下文搞混。

{{&lt; image src=&#34;/ostep_note_pic/受限直接执行协议.png&#34; caption=&#34;受限直接执行协议（时钟中断）&#34; &gt;}}

所谓的受限直接执行的含义是：OS在启动时设置陷阱处理程序并启动时钟中断，然后仅在受限模式下运行进程，以此为CPU提供宝宝防护。这是虚拟化CPU的基本机制。

### 进程调度
调度指标：周转时间，周转时间=完成时间-到达时间，这是一个性能指标。另一个有趣的目标是公平，两者往往是矛盾的。

1. 先进先出 FIFO
2. 最短任务优先 SJF
    - 同时到达的任务先运行短时间的
3. 最短完成时间优先 STCF
    - 向SJF添加抢占则为最短完成时间优先，随时到达随时评估
  
新的调度指标：响应时间，响应时间=首次运行时间-到达时间。其实就对应了任务的响应优先级，考虑到了某些运行时间长的任务一直得不到响应的情况。

1. 轮转 RR
    - 时间片轮转，划分固定的时间片给每个任务，按任务队列顺序依次占有CPU
    - 时间片时间越短响应时间越好，但是需要摊销上下文切换成本，让上下文切换时间占比不要太高
    - 上下文切换的成本不仅来自上述所说的操作，在CPU cache、TLB等硬件上建立了大量的状态，这些切换也可能导致显著的性能成本。
    - 这样会导致周转时间指标很差，因为作为一个公平的策略
2. 结合I/O
    - 这里主要是指实际场景中可以利用I/O操作耗时的间隙进行CPU操作从而更好利用CPU
3. 多级反馈队列 MLFQ
    - Corbato提出，获得图灵奖
    - 需要联合考虑周转时间和响应时间两个指标
    - 是用历史经验预测未来的一个经典例子，以史为鉴，更多的是基于行为制定规则，而不是先验知识
    - 划分不同优先级的不同队列，每个队列之间用RR，I/O密集型的优先级高（占用CPU时间短），CPU密集型的优先级低（占用CPU时间长），优先级大的先运行，相同优先级的轮转运行。但不能一成不变，需要改变优先级，工作进入系统时放在最高优先级，用完一个时间片后下降一个优先级，如果在其时间片内主动释放CPU优先级不变，（这里为了防止任务通过主动调用I/O操作释放CPU愚弄OS保持优先级而提出计算一层总的时间配额，而不是时间片），同时为了避免饥饿问题（CPU密集型的任务占用不到CPU）需要周期性的提升所有工作的优先级。

### 多处理器调度
多CPU相对于单CPU的区别核心在于对cache的使用以及多CPU之间共享数据的方式。

在单CPU中存在多级cache（L1、L2、L3），cache是很小但很快的存储设备（快于内存慢于寄存器），cache基于时间局部性（热点数据可能会再次访问）和空间局部性（访问x的数据可能紧接访问x周围的数据，遍历数组按行）。而对于多CPU就会出现缓存一致性的问题，就是在CPU1写内存A时只更新了缓存而没有立即写回到原内存地址，这时OS中断CPU1而去运行CPU2的程序，那么CPU2读的数就是原值而不是更改后的值。解决缓存一致性的办法：总线嗅探。每个缓存都通过监听连接所有缓存和内存的总线来发现内存访问，如果发现CPU对缓存中的数据进行更新会移除缓存或更新它。总线嗅探通过事件广播通知到其他核心，但是不能保证事务的串行化（是指某个CPU核心对数据的操作顺序必须在其他核心看起来顺序是一样的）。MESI（已修改、独占、共享、已失效）协议基于总线嗅探机制实现了事务的串行化，构成一个流动的状态机。

多CPU访问共享数据时需要使用互斥原语，才能保证准确性。对于共享数据，如多CPU对同一链表、容器执行增删操作都会出现问题，需要加锁，在函数开始时调用lock()，函数结束时调用unlock()，但随着CPU数量增加，访问这些共享的数据会变得很慢。

Linux多处理器调度目前存在3种不同的调度程序：O(1)基于优先级的多队列调度、CFS确定的比例调度多队列调度和BFS小心脏受不了的单队列调度。

### 地址空间
地址空间是OS提供的一个易用的物理内存抽象。一个进程的地址空间包含运行的程序的所有内存状态，比如，代码、栈、堆。必须提供虚拟地址到物理地址的转换，这是内存虚拟化的关键。作为用户级的程序员，可以看到（打印）的任何地址都是虚拟地址，只有OS和硬件才知道物理地址。虚拟内存为程序提供了一个巨大的、稀疏的、私有的地址空间假象，其中保存了程序的所有指令和数据。

忘记分配内存导致的段错误，在使用strcpy(dst,src)时，dst没有分配内存而导致，正确的代码是：
```C
char* src = &#34;hello&#34;;
char* dst = (char*) malloc(strlen(src) &#43; 1);
strcpy(dst,src);
```
没有分配足够的内存，导致的缓冲区溢出，如：
```C
char* src = &#34;hello&#34;;
char* dst = (char*) malloc(strlen(src));
strcpy(dst,src);
```
忘记释放内存，导致的内存泄漏，malloc必须对应free，不然会因为缓慢泄漏的内存导致内存不足，只能重启。

malloc是库调用，用到的系统调用有：
1. brk或sbrk，用来改变程序堆结束的位置，需要一个地址参数来增加或减小堆的大小。
2. mmap系统调用，可以创建一个匿名的内存区域，与交换空间相关联，这种内存也可以向堆一样对待并管理。

### 地址转换
硬件对每次内存访问进行处理，将虚拟地址转换为物理地址。仅靠硬件也不足以实现虚拟内存，OS必须在关键的位置介入，设置好硬件，因此必须管理内存，记录被占用和空闲的内存位置。所有的工作都是为了创造一种假象：每个程序都拥有私有的内存，实际上是同一时间共享着内存。

动态重定位：基址加界限机制，每个CPU需要两个硬件寄存器，基址寄存器和界限寄存器，能够将地址空间放到物理内存的任何位置，同时又能确保进程只能访问自己的地址空间。程序假设地址空间从零开始，OS将起始地址记录在基址寄存器中，其实就是OS确定要加载的物理地址作为基址，虚拟地址作为偏移量，从而得到实际的物理地址。界限寄存器提供访问保护，超过这个界限，CPU触发异常。这两个寄存器的硬件结构是MMU内存管理单元。但是这种简单的动态重定位技术效率低下，容易产生大量的内部碎片，因为我们将地址空间划分了固定大小的槽块。

为了解决内部碎片的问题，分段概念应运而生。一个段是地址空间里的一个连续定长的区域，典型的地址空间有三个逻辑段：代码段、栈和堆。这样就需要3对基址寄存器和界限寄存器，每个界限寄存器记录一个段的大小，如果超出边界就会发生段异常或段错误。硬件在地址转换时使用段寄存器，如何知道段内偏移量呢？显式方式就是用虚拟地址的开头几位来标识不同的段，比如前两位标识，00是代码段地址，使用代码段的基地址和界限寄存器；01是堆地址；02是栈地址。

对了，对于栈是反向增长的，即从高地址向低地址，与另两个段不同，所以硬件除了基址和界限以外还需要知道段的增长方向（用一位0或1区分是否反向增长），对应的地址转换计算也就不同。

要节省内存，在地址空间之间共享某些内存段时有用的。为了支持共享，需要一些额外的硬件支持，就是保护位（可读、可写、可执行）。因此物理内存一个段可以映射到多个虚拟内存。

然而，分段也带来了一些新问题。第一个：OS在上下文切换时，每个段寄存器中的内容必须保存和恢复。第二个：存在外部碎片问题，当一个进程需要分配20KB的段时，有24KB空闲但不连续，无法满足需求。解决这个问题的方法有成百上千种但都无法完全消除外部碎片，只是试图减小它。比如，重新安排所有段（copy所有数据成本很高）；空闲链表管理（最优匹配、最坏匹配、首次匹配、伙伴算法等）。

### 空闲内存管理
无论是malloc（管理进程中堆的页）还是OS本身（管理进程的地址空间），都涉及到空闲内存管理。如果管理的空闲内存单元大小不是固定的，如用malloc和free，OS使用分段的方式，管理就变得困难。

malloc在堆上管理空闲的数据结构称为空闲链表，包含所有空闲块的引用。malloc在所有空间快用完时，可以向内核申请增加堆空间（sbrk），以下假设malloc维护的空间大小固定。

分割与合并：空闲链表每个节点维护地址空间的起始地址和长度，找到满足要分配大小的节点进行分割，free后将连续地址空间的节点合并。

追踪已分配空间的大小：free函数并没有接收要free掉地址空间大小的参数，而是在header（分配的空闲块头块）中保存这个空闲块的大小，所以在用户请求N字节内存时，库是在寻找N&#43;header size的空闲块。

嵌入空闲链表：上述所使用的空闲链表也需要分配内存空间来存储，这里大概讲的意思是先有鸡还是先有蛋的问题，维护空闲块的链表也需要空闲块来存储，malloc要分配内存就需要空闲链表，而空闲链表还没有创建，那么就需要OS先用mmap直接分配一段大块的物理内存（4KB）作为空闲堆。

管理空闲块的一些策略：
1. 最优匹配：遍历整个空闲链表，找到和请求一样或更大的所有空闲块，选最小的。
2. 最差匹配：尝试找最大的空闲块，进行分割。
3. 首次匹配：找第一个足够大的块，空闲链表基于地址排序方便合并。
4. 下次匹配：基于首次匹配多维护一个指向上一次查找结束为止的指针。
5. 伙伴系统：方便合并，空闲空间被递归二分，如果free掉，会检查相邻的二分伙伴是否空闲，向上回溯一直合并。

{{&lt; image src=&#34;/ostep_note_pic/伙伴系统合并.png&#34; caption=&#34;伙伴系统合并&#34; &gt;}}

### 分页
分段会带来外部碎片的问题，分页不会导致外部碎片。分页是将空间分割成固定大小的页，对应的物理内存也看成是定长槽块的页帧，这样每一个物理页帧就对应一个虚拟内存页。页表记录了每个虚拟页在物理内存中的位置，为每个虚拟页保存地址转换。重要的是记住页表是每一个进程的数据结构，每一个进程都有自己的页表。

转换虚拟地址需要虚拟页面号（VPN）和页内偏移量（offset），将虚拟地址变成二进制格式，前几位表示VPN，后几位表示offset，VPN会根据页表找到对应的物理页帧（PFN），offset保持不变，从而得到物理地址的二进制形式。

{{&lt; image src=&#34;/ostep_note_pic/地址转换过程.png&#34; caption=&#34;地址转换过程&#34; &gt;}}

注意MMU中没有存储页表，而是将每个进程的页表存储在内存中。页表就是将虚拟地址映射到物理地址，最简单就可以采用数组实现，OS通过VPN检索到页表项PTE，从而找到对应的PFN。对于每个PTE有许多不同的标志位，如：有效位（标记空间是否有效），保护位（可读、可写、可执行），存在位（是否被换出），脏位（页面是否被修改过）等。

MMU内存管理单元是负责虚拟地址到物理地址转换的硬件，其主要组件有页表基址寄存器、地址转换逻辑、TLB、访问权限控制逻辑、页面异常处理逻辑，MMU通过页表基址寄存器访问内存中的页表，并利用TLB缓存加速地址转换。

为了进行上述虚拟地址到物理地址的转换必须知道页表的位置，页表的起始位置的物理地址存在于页表基址寄存器中。具体转换过程就是按位与MASK&#43;右移提取VPN位得到虚拟页号，根据页表基址寄存器的起始地址做VPN*sizeof(PTE)的偏移得到对应的PTE，也就得到了物理页帧；类似地提取offset，在物理页帧基础上做偏移即得到了物理地址。

有两个必须解决的实际问题：1. 页表查询会使系统运行变慢；2. 页表占用太多内存。

### TLB
分页会带来较高的性能开销，这是因为在转换虚拟地址时，分页需要额外的一次内存访问（页表访问），每次指令的获取、加载或保存都需要额外读一次，这会慢得无法接受。想让某些东西更快，往往需要硬件支持，那就是TLB地址转换旁路缓冲存储器（太长了这中文名，其实就是页表项缓存、快表，通常位于MMU中）。缓冲命中显然会使页表转换更快。与此同时，我们要尽量避免TLB未命中。

得益于空间局部性，同一页存储数组相邻元素，一个PTE会放到TLB中从而相邻元素命中TLB。一个页大小为4KB，能存很多数，那么会提供很好的TLB性能；以及得益于时间局部性，短时间内对内存项再次引用，所以TLB的命中率会很高。

上下文切换时对TLB的处理：因为每个进程有自己的页表，同样缓存在TLB的PTE在切换另一个进程时也会失效，所以要对TLB进行处理。最简单的方法就是清空TLB，但存在一定的开销。为了减少这种开销，有的OS在TLB中添加了一个地址空间标识符（ASID），添加这个字段到TLB中，类似于进程标识符PID标识不同的进程。因此硬件必须知道那个进程在运行，上下文切换时必须将某个特权寄存器设置为当前进程的ASID。

TLB替换策略：在TLB中插入新项时，很自然要考虑替换哪个旧项。页换出磁盘时也要考虑哪个内存页被替换掉。常见的策略就是替换最近最少（LRU）使用的项；另一个策略就是随机策略，避免了循环访问n&#43;1页，但TLB只能存放n个页导致LRU每次都要替换的极端情况。

### 较小的表
由于虚拟地址到物理地址的转换需要页表，页表存在于内存中，也会占用内存空间，而且每个进程都有一个页表，多个进程下页表内存总开销是很大的。鉴于此，我们希望尽量减小页表的大小。

分段会导致外部碎片，分页会导致内部碎片，那么两个结合起来是否会更好呢？将分段和分页结合来减少页表的内存开销。这种结合方法就是不为进程的整个地址空间提供单个页表，而是为每个逻辑段（代码段、堆、栈）提供一个。与线性页表相比，杂合方式实现了显著的内存节省。但是它依然是基于分段的大框架下的，依旧会面临某个段区稀疏导致的大量页表浪费，以及外部碎片。

使用多级页表有效的减小了页表的内存大小，因为它将线性页表变成了类似树的东西。多级页表的思想：将页表分成页大小的单元（值得注意的是页表也存在于内存中所以也按页大小划分了），如果整页的PTE无效就完全不分配该页的页表。而为了判断这页的PTE是否都是有效的，使用页目录的结构。增加了页目录的中间层，从而类似于懒分配的策略，节省了内存空间。

{{&lt; image src=&#34;/ostep_note_pic/多级页表.png&#34; caption=&#34;多级页表&#34; &gt;}}

这个图画的巨清楚，amazing感受到了这真是本好书。

多级页表是一个时间和空间的trade-off，节省了空间就牺牲了部分时间成本，体现在TLB为命中时要从内存加载两次（一次是页目录，一次是PTE本身，因为引入了中间层）。

如何获取VPN，并用它首先索引到页目录（PDE）中，然后再索引到页表的页中。PDEAddr = PageDirBase &#43; (PDIndex*sizeof(PDE)) ，这样得到页目录，如果页目录有效，使用VPN剩余位索引到页表的部分，PTEAddr = (PDE.PFN &lt;&lt; SHIFT) &#43; (PTIndex * sizeof(PTE))。

### 超越物理内存：机制
为了能支持许多同时运行的巨大虚拟地址空间，需要使用硬盘的交换空间。在硬盘上开辟一部分空间用于物理页的移入和移出，这样的空间称为交换空间，我们将内存中的页交换到其中，并在需要时又交换回去。因此，我们会假设OS能以页为单元读取或者写入交换空间，OS就需要记住给定页的硬盘地址。交换空间不是唯一的硬盘交换目的地，这里是指交换空间更多的是进程内存与硬盘的交换，而硬盘中一直存放的程序代码也是另一个目的地交换到内存中执行或腾出。

当给出一个VPN，首先查TLB，TLB命中直接返回PFN；未命中，查页表，如果页有效且存在于物理内存，则获得PFN并插入TLB，并重试该指令产生TLB命中。但如果查页表有映射却存在位（不是有效位）不为1（不存在物理内存中），称为页错误，OS会被唤醒，处理页错误。注意这里是完全合法的访问，页被映射到了虚拟地址空间，但不存在物理内存中，称为页未命中更合适。但通常，称为了页错误，再次说明不是未建立映射导致的页错误。

处理页错误是通过PTE中的某些位存储硬盘地址，查找这些位对应的硬盘地址，将请求发送到硬盘，将页读取到内存中。当硬盘I/O完成时，OS会更新页表，将此页标记为存在，更新PTE的PFN字段。这是换入操作。

当内存满了我们还需要将内存中的一个或多个页换出，然后才能将硬盘中的页换入。而选择哪些页换出的过程称为页交换策略。

页错误硬件控制流算法：
1. 页存在且有效，TLB未命中可从PTE中获取PFN，然后重试指令TLB命中。
2. 页不存在且有效，说明是合法页但不存在物理内存中，运行页错误处理程序。
3. 页无效，非法访问，运行trap处理程序。
   
页面交换不是在内存满了以后才发生的，会保证有少量的空闲内存。OS会设置高水位线HW和低水位线LW，决定何时从内存中清除页。OS发现有少于LW个页可用时，交换守护线程会运行释放内存，直到有HW个可用的物理页，然后休眠。

### 超越物理内存：策略
任何像FIFO或随机这样简单的策略都可能会踢出一个重要的页，而这个页马上会被引用，不太可能达到最优，需要更智能的策略。为了提高后续的命中率，像之前的调度策略一样，通过引入历史信息作为参考。例如，如果某个程序在过去访问过某个页，则很有可能在不久的将来会再次访问该页。

一个历史信息：频率，如果一个页被访问了很多次，也许不该被替换。以及访问的近期性，越近被访问过的页，也许再次访问的可能性越大。基于历史的算法：最不经常使用（LFU）会替换最不经常使用的页；最少最近使用（LRU）会替换最近最少使用的页。局部性原则不是硬性规定，是一件好事，但不能保证成功，而是一种经常证明在计算机系统设计中有用的启发式方法。LRU算法实现参考力扣LRU cache，使用哈希表和双向链表维护固定容量cache的换入换出操作。由于每次页访问时都必须更新一些数据，系统必须对每次内存引用做一些记录工作，这样的记录会极大影响性能。现代OS从计算开销角度来看使用了近似LRU，通过硬件增加一个引用位，时钟算法将所有页放到一个循环列表中，需要进行页替换时随机扫描各页，如果遇到一个页的引用位为1，就清除该位（设置为0），直到找到一个使用位为0的页，将这个页进行替换。时钟算法每个页只需维护一个访问位，而且通常由硬件MMU自动设置，不需软件干预，不需每次访问时都做记录（LRU对比）。

脏页：如果页被修改因此变脏，则踢出它必须写回磁盘，代价很大，因此尽量踢出非脏页。硬件增加一个修改位（脏位），每次写入页时都会设置此位，因此合并到页面置换算法中。

OS运行的进程所需的内存需求紧张，从而不断进行换页的情况称为抖动，此时OS会使用守护进程选择一个内存密集型的进程并杀死它。

### VAX/VMS虚拟内存系统
VMS是在多种硬件系统下构建的一个通用有效的工作系统。因为VAX页大小非常小，首要目标是确保VMS不会用页表占满内存，通过两种方式：首先将用户空间分为两部分P0和P1，每部分对应一个页表；其次，在内核虚拟内存中放置用户页表。实际的地址空间页0标记为无效，代码段不会从第0页开始。

补充：为什么空指针访问会导致段错误？对一个空指针解引用，会导致段错误。因为空指针地址对应虚拟地址0，对该地址解引用时，硬件试图在TLB中查找VPN，TLB未命中，查找页表，发现VPN 0 的PTE标记为无效。因此，遇到无效的访问，将控制权转交给OS，这可能会终止进程，触发段错误。

内核虚拟地址空间是每个用户地址空间的一部分，它们共享相同的内核地址映射，并映射到同样的物理地址。在上下文切换时，OS改变P0和P1寄存器以指向即将运行的进程的页表。如果OS收到用户程序递交的指针，很容易将数据从该指针复制到自己的结构（这里是指每个用户进程都有内核空间映射到其虚拟内存空间上，虚拟内存地址是连续的，可以直接访问用户传来的地址）。这样减少了内核空间切换页表的开销，节省了内存资源。

OS不希望用户程序读取或写入OS数据或代码，因此，硬件必须支持页面所需的特权级别来实现。试图访问OS内核的数据或代码将会在OS中产生一个trap，并可能会终止违规进程。

VMS有两个现在成为标准的技巧：按需置零和写时复制。
1. 按需置零（懒分配）：考虑在地址空间添加一个页，初级实现是在物理内存中找到物理页帧，将该页加入到虚拟内存的堆中，并将其置零，然后将其映射到虚拟内存上。利用按需置零，当页添加到虚拟地址空间时，会在页表中放入一个标记不可访问的PTE，如果进程读或写该页，会向OS发送trap，在处理trap时，OS根据PTE所标记的不可访问页知道要寻找物理页，将其置零再映射到虚拟内存中。
2. 写时复制：如果OS需要将一个页面从一个地址空间复制到另一个地址空间，不是实际复制它，而是将其映射到目标物理地址空间，并在两个地址空间中将其标记为只读。如果其中一个地址空间尝试写入页面，会发生trap，OS注意到这是一个COW页面，会分配一个新页，填充数据，并将该页建立映射。COW的用处：（1）任何类型的共享库（maybe动态链接库）都可以通过COW映射到许多进程的地址空间，节省内存。（2）fork()会copy父进程的内存空间，但一般立即执行exec()，这些内存空间的数据会被exec()执行的程序覆盖，通过写时复制版的fork()，避免了大量不必要的copy，既保留了正确的语义，又提高了性能。

## 并发
### 线程
多线程程序会有多个执行点（多个程序计数器，每个都用于取指和执行），共享地址空间，从而能访问相同的数据。线程有一个程序计数器，一组用于计算的寄存器。线程上下文切换类似于进程间的上下文切换，进程将状态保存到进程控制块（PCB）中，同样需要一个或多个线程控制块（TCB）保存每个线程的状态。但是线程有一点主要区别：地址空间保持不变（不需要切换当前使用的页表）。另一个主要区别是每个线程都有一个栈，进程只有一个栈。

竞态条件：结果取决于代码的时间执行，可能每次都会得到不同的结果，结果是不确定的。导致竞态条件的这段代码称为临界区，临界区是访问共享资源的代码片段，一定不能由多个线程同时执行。而真正想要的代码是互斥的，这个属性保证如果一个线程在临界区内执行，其他线程将被阻止进入临界区。所有这些术语是由Dijkstra创造的，因此获得图灵奖。

对一个共享变量进行加操作，在硬件上指令是有三条的，如果有一条超级指令能原子性地执行这三条指令的操作就不会出现上述得到不确定性结果的问题。我们要做的是要求硬件提供同步原语，加上OS的一些帮助，以同步和受控的方式访问临界区，从而产生正确的结果。

线程间还有另外一种常见的交互，即一个线程在继续之前必须等待另一个线程完成某些操作，如磁盘I/O操作进入睡眠状态后需要被唤醒。条件变量支持多线程睡眠唤醒机制。

OS是第一个并发程序，因为OS引入了中断，中断也会随时发生，所以更新这些共享结构都必须小心访问，并使用正确的同步原语才能正常工作。

### 线程API
线程创建：pthread_create()，第一个参数传入pthread_t结构体指针，需要利用这个结构体与该线程交互，以便初始化。第二个参数一般传NULL。第三个参数是在问这个线程应该在哪个函数中运行，是一个函数指针（函数名称start_routine，参数void*，返回值void*）。第四个参数是传递给线程开始执行的函数的参数。一旦创建了线程，就拥有了一个活着的执行实体，有自己的调用栈，与程序中所有当前存在的线程在相同的地址空间内运行。
```C
int pthread_create(pthread_t* thread,
                const pthread_attr_t* attr,
                void* (*start_routine)(void*),
                void* arg);
```
线程完成：pthread_join()，第一个参数是传入pthread_t结构体指针，指定要等待的线程。第二个参数是一个指针，指向你希望得到的返回值。在函数调用返回值时需要注意永远不要返回局部变量（栈上）的引用或指向局部变量的指针。这个等待线程执行完成的作用是：创建线程并行执行特定任务的并行程序，使用join来确保在退出或进入下一阶段计算前完成所有这些工作。
```C
int pthread_join(pthread_t* thread, void** value_ptr);
```
锁：提供互斥进入临界区的函数。所有锁必须准确初始化。
```C
int pthread_mutex_lock(pthread_mutex_t* mutex);
int pthread_mutex_unlock(pthread_mutex_t* mutex);
```
条件变量：当线程之间必须发生某种信号时，如果一个线程在等待另一个线程继续执行某些操作，条件变量就很有用。使用条件变量必须有一个与此条件相关的锁。
```C
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
Pthread_mutex_lock(&amp;lock);
while(ready == 0)
    Pthread_cond_wait(&amp;cond, &amp;lock); //ready没有设为0以外的值就调用等待函数休眠
Pthread_mutex_unlock(&amp;lock);
```
其他线程进行唤醒。
```C
Pthread_mutex_lock(&amp;lock);
ready = 1;
Pthread_cond_signal(&amp;cond);
Pthread_mutex_unlock(&amp;lock);
```

### 锁
锁就是一个变量，因此我们需要声明一个某种类型的锁变量才能使用。lock()尝试获取锁，如果没有其他线程持有锁，该线程会获得锁，处于临界区。锁的持有者一旦调用unlock()，锁的状态就变成可用了。如果有等待线程（卡在lock()里），其中一个线程会注意到（收到通知）锁状态的变化，获取锁，进入临界区。锁为程序员提供了最小程度的调度控制，将原本由OS调度的混乱状态变得更为可控。

POSIX库将锁称为互斥量，不同于任何临界区都使用同一个大锁（粗粒度的锁），通常大家会用不同的锁保护不同的数据和结构，从而允许更多的线程进入临界区（细粒度的锁）。

评价锁实现的效果：1. 互斥性；2. 公平性； 3. 性能。最早提供的互斥就是在临界区关闭中断，优点是简单，缺点有很多：1. 恶意调用lock()后死循环，系统失去控制权；2. 不支持多处理器，关中断是针对一个处理器而言的；3. 关中断会导致中断丢失；4. 效率低，关闭和打开中断的代码执行得很慢。基于以上原因，只在有限的情况下用关闭中断来实现互斥原语。

测试并设置指令：返回old_ptr指向的旧值，同时更新为new的新值，这些代码是原子执行的。
```C
typedef struct lock_t{
    int flag;
}lock_t;

void init(lock_t* lock){
    lock-&gt;flag = 0;
}

void lock(lock_t* lock){
    while(TestAndSet(&amp;lock-&gt;flag, 1) == 1)
        ;
}

void unlock(lock_t* lock){
    lock-&gt;flag = 0;
}
```
将test和set合并为一个原子操作后，保证了只有一个线程能获取锁，这就是实现了一个有效的互斥原语。这种使用while循环的锁是自旋锁，在单处理器上需要抢占式的调度器，否则在单CPU上无法使用，因为一个自旋的线程永不会放弃CPU。

自旋锁没有公平性，可能会导致饿死；在单CPU上性能开销很大，在一个线程持有锁进入临界区时，其他线程都在竞争锁，都会在放弃CPU之前自旋一个时间片浪费CPU周期。但是在多CPU上，自旋锁性能不错。

比较并交换：检测ptr指向的值是否和expected相等；如果是，更新ptr所指向的值为新值。

获取并增加：原子地返回特定地址的旧值且让该值自增一。

基于以上硬件原子指令实现的锁简单且有效，但可能会产生自旋过多：两个线程运行在同一个CPU上，当线程A持有锁时，被中断。线程B获取锁，发现锁已被占用，它就会自旋，直到时钟中断产生，线程A重新运行。如果有N个线程去竞争一个锁，会浪费N-1个时间片，时间片内只是自旋并等待一个线程释放该锁。

那这样只靠硬件支持是不够的，还需要OS支持，简单方法是：让出来。在要自旋的时候，放弃CPU。在这种方法中，OS提供原语yield()，主动放弃CPU，让其他线程运行。yield()系统调用能让运行态变为就绪态，从而允许其他线程运行。但是这样依旧存在上下文切换的成本，如果有100个线程竞争一把锁，其中99个线程虽然不会自旋浪费时间片，但是依旧有运行-让出的上下文切换的成本。

无论自旋还是让出都可能造成浪费和饿死，我们必须显示地施加某种控制，决定锁释放时，谁能抢到锁。使用队列，通过休眠代替自旋。park()能让调用线程休眠，unpark(threadID)则会唤醒threadID标识的线程，用这两个系统调用实现锁，让调用者在获取不到锁时睡眠，在锁可用时被唤醒。
```C
typedef struct lock_t{
    int flag;
    int guard;
    queue_t* q;
} lock_t;

void lock_init(lock_t* m){
    m-&gt;flag = 0;
    m-&gt;guard = 0;
    queue_init(m-&gt;q);
}

void lock(lock_t* m){
    while(TestAndSet(&amp;m-&gt;guard, 1) == 1)
        ;
    if(m-&gt;flag == 0){ // 锁被获取
        m-&gt;flag = 1;
        m-&gt;guard = 0;
    }else{ // 锁被占用
        queue_add(m-&gt;q, gettid());
        m-&gt;guard = 0;
        park(); // 休眠
    }
}

void unlock(lock_t* m){
    while(TestAndSet(&amp;m-&gt;guard, 1) == 1)
        ;
    if(queue_empty(m-&gt;q)) // 队列是空的 把锁释放 无线程需要
        m-&gt;flag = 0;
    else
        unpark(queue_remove(m-&gt;q));
    m-&gt;guard = 0;
}
```
两阶段锁（自旋和睡眠的杂合）：第一阶段自旋一段时间，如果没有获得锁，第二阶段休眠，直到锁可用。Linux锁就是这种锁，不过只自旋一次；更常见的方式是在循环中自旋固定的次数，然后使用futex休眠。

### 条件变量
锁并不是并发程序设计所需的唯一原语，很多情况下，线程需要检查某一条件满足之后，才会继续运行。线程可以使用条件变量等待一个条件变成真，条件变量是一个显示队列，当某些执行状态不满足时，线程可以把自己加入队列，等待该条件。另外某个线程，当它改变了上述状态时，就可以唤醒一个或多个等待线程让他们继续执行。

条件变量的声明：pthread_cond_t c;有两种相关操作：wait()和signal()，线程要睡眠的时候调用wait()，想要唤醒等待在某个条件变量上的睡眠线程时，调用signal()。wait()作用是释放锁，并让调用线程休眠，当线程被唤醒时，必须重新获取锁，再返回调用者。

使用条件变量，父进程等待子进程的示例代码：
```C
int done = 0;
pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t c = PTHREAD_COND_INITIALIZER;

void thr_exit(){
    Pthread_mutex_lock(&amp;m); // 获取锁
    done = 1;                // 标志位避免虚假唤醒
    Pthread_cond_signal(&amp;c); // 激活其他睡眠线程
    Pthread_mutex_unlock(&amp;m);
}

void* child(void* arg){
    printf(&#34;child\n&#34;);
    thr_exit();
    return NULL;
}

void thr_join(){
    Pthread_mutex_lock(&amp;m); // 同样也要获取锁
    while(done == 0)
        Pthread_cond_wait(&amp;c, &amp;m); // 睡眠等待
    Pthread_mutex_unlock(&amp;m);
}

int main(int argc, char *argv[]){
    printf(&#34;parent:begin\n&#34;);
    pthread_t p;
    Pthread_create(&amp;p, NULL, child, NULL); // 创建线程执行child函数
    thr_join();
    printf(&#34;parent:end\n&#34;);
    return 0;
}
```
有两种情况需要考虑：1. 父进程先于子进程执行，即父进程先thr_join()睡眠等待，子进程再thr_exit()激活父进程继续执行。2. 子进程先于父进程执行，即子进程立即唤醒其他线程，父进程调用thr_join()时发现done是1，就不需要睡眠了。

如果不使用状态变量done，是有问题的，假设是上述情况2，而没有done的判断，父进程就会一直睡眠。

如果使用发信号和等待时不加锁，会产生竞态条件，父进程在调用thr_join()时，判断done==0，在调用wait()进入睡眠之前，中断产生运行子进程，子进程此时将done设为1，唤醒其他进程，但发现没有睡眠进程，父进程再次运行时，就会长眠不醒。调用siganl和wait时要持有锁，会保持身心健康。
```C
void thr_exit(){
    done = 1;
    Pthread_cond_signal(&amp;c);
}

void thr_join(){
    if(done == 0)
        Pthread_cond_wait(&amp;c);
}
```
生产者消费者问题：假设有一个或多个生产者线程和一个或多个消费者线程。生产者把生产的数据放入缓冲区，消费者从缓冲区取出数据，以某种方式消费。例如，多线程的网络服务器中，一个生产者将HTTP请求放入工作队列（有界缓冲区），消费线程从队列中取走请求并处理；使用管道时也会使用有界缓冲区，例如grep foo file.txt | wc -l，并发执行两个进程，grep进程从file.txt中查找包括&#34;foo&#34;的行，写到标准输出；shell重定向到管道，管道的一端是wc进程的标准输入，wc统计完行数后打印出结果。因此，grep是生产者，wc是消费者，之间是内核中的有界缓冲区。由于Mesa语义，记住一条关于条件变量的简单规则：总是使用while循环。多线程程序在检查条件变量时，使用while循环总是对的，也解决了虚假唤醒的问题。

### 常见并发问题
常见并发问题分为两类：非死锁缺陷和死锁缺陷。非死锁问题：违反原子性缺陷（违反多次内存访问中预期的可串行性，对共享变量加锁）和错误顺序缺陷（两个内存访问的预期顺序被打破了，条件变量&#43;锁实现同步）。大部分（97%）的非死锁问题是违反原子性和违反顺序这两种。

死锁：当线程1持有锁L1，正在等待另一个锁2，而线程2持有锁L2，却在等待锁L1释放。死锁为什么会发生：1. 在大型代码库里存在循环依赖，如虚拟内存系统需要访问文件系统才能从磁盘读到内存页；文件系统随后又要和内存系统交互去申请内存。2. 封装，某些看起来没有关系的接口可能会导致死锁。比如：
```Java
Vector v1, v2;
v1.AddAll(v2);
```
在内部，vector需要线程安全，这个线程先对v1加锁，再对v2加锁，而另一个线程几乎同时调用v2.AddAll(v1);就可能遇到死锁。

产生死锁的条件：
1. 互斥：线程对资源进行互斥的访问。
2. 持有并等待：线程持有了资源（对应的锁），同时又在等待其他资源（需要获得其他锁）。
3. 非抢占：线程获得的资源不能被抢占。
4. 循环等待：线程之间存在一个环路，环路每个线程都额外持有一个资源，而这个资源又是下一个线程要申请的。

预防死锁：
1. 循环等待：获取锁时提供一个全序，每次按序申请锁。在复杂的系统中不会只有两个锁，锁的全序可能很难做到，偏序是一种有用的方法。Linux的内存映射代码就是一个偏序锁的好例子，然而这是一种约定，很容易出错。
2. 持有并等待：通过原子地抢锁来避免，任何线程在抢锁时先抢到全局的prevention锁。
3. 非抢占：在调用unlock之前，使用trylock()函数，尝试获得所需要的锁。
4. 互斥：使用硬件提供的原子指令，比如比较并交换指令。

通过调度避免死锁：
通过了解全局的信息，使得后续的调度能够避免产生死锁。避免在同一CPU上运行需要竞争同一锁的线程，这种保守的静态方案会明显增加完成任务的总时间，付出牺牲性能的代价。银行家算法就是类似的著名解决方案，但这种方法会限制并发，不是广泛使用的通用方案。
检查和恢复：允许死锁偶尔发生，检查到死锁时再采取行动。很多数据库系统使用了死锁检测和恢复技术，死锁检测器定期运行，当死锁发生时，系统重启。

### 基于事件的并发
一些基于图形用户界面的应用或某些类型的网络服务器，常常采用另一种并发方式（非线程实现），这种方式是基于事件的并发。我们等待某事件发生，当它发生时，检查事件类型，然后做少量的相应工作。基于事件的服务器：基于一个简单的结构（事件循环），主循环等待某些事件发生，然后依次处理这些发生的事件（事件处理程序），调度决定接下来处理哪个事件。

解决如何接收事件：select()、poll()、epoll()系统调用，select()检查I/O描述符集合，描述符集合的地址通过readfds、writefds和errorfds传入，分别查看它们中的某些描述符是否已准备好读取，是否准备好写入、是否有异常情况待处理。返回所有集合中就绪描述符的总数。

使用单个CPU和基于事件的应用程序，就不存在并发问题，但不能阻塞基于事件的服务器。使用基于线程的服务器时，一个事件因为I/O请求而阻塞不是问题，可以挂起该线程运行其他线程；但是使用基于事件的方法时，只有主事件循环，意味着在基于事件的系统中必须遵守一条规则：不允许阻塞调用。
异步I/O：使应用程序能够发出I/O请求，并在I/O完成之前立即将控制权返回给调用者，以及让应用程序确定各种I/O是否已完成。AIO控制块：
```C
struct aiocb{
    int             aio_fildes; // 文件描述符
    off_t            aio_offset; // 文件偏移量
    volatile void*     aio_buf; // 复制读取内容的目标内存位置
    size_t            aio_nbytes; // 长度
};
```
先填充上述AIO控制块，再发出异步调用aio_read()异步读取。如何知道I/O何时完成，并且缓冲区（aio_buf指向的）现在有了请求的数据？还需要一个aio_error()的API来检查aiocb引用的请求是否已完成。对于未完成的异步I/O，应用程序可以通过调用aio_error()来周期性地轮询系统来确定I/O是否完成。而这种基于轮询的方式很低效，一些系统提供了基于中断的方法，使用UNIX信号在异步I/O完成时通知应用程序。
这种基于事件的方法存在一些困难：1. 需要保存异步程序状态；2. 现代多核系统上依旧存在同步问题；3. 与现代系统其他方面（例如分页）集成有难度。

线程和事件在未来多年可能会持续作为解决同一并发问题的两种不同方法。

## 持久性
### I/O设备
计算机原型系统的架构，分层架构，CPU和内存通过内存总线连接，图像或其他高性能I/O设备通过常规的I/O总线连接，外围将最慢的设备连接到系统。为什么分层？物理布局及造价成本，越快的总线越短，造价越贵，高性能设备离CPU更近一些，低性能设备离CPU更远一些，且可以在外围总线上连接大量设备。

{{&lt; image src=&#34;/ostep_note_pic/计算机原型系统架构.png&#34; caption=&#34;计算机原型系统架构&#34; &gt;}}

标准设备（假想的）包含：1. 接口，向系统展现的硬件接口，所有设备都有自己的特定接口以及典型交互的协议。2. 内部结构，实现设备展示给系统的抽象接口，其实就是硬件设备。

{{&lt; image src=&#34;/ostep_note_pic/标准设备.png&#34; caption=&#34;标准设备&#34; &gt;}}

一个设备接口包含三个寄存器：1. 状态寄存器；2. 命令寄存器；3. 数据寄存器；

OS与设备的典型交互协议：1. 轮询设备，反复读取状态寄存器，等待设备进入可以接收命令的就绪状态；2. OS下发数据到数据寄存器；3. OS将命令写入命令寄存器，开始执行命令；4. OS再次不断轮询设备，等待并判断设备是否执行完成命令。

中断允许计算与I/O重叠，这是提高CPU利用率的关键。如果设备非常快，中断反而会使系统变慢，因为进程切换代价高于设备处理代价，最好的办法反而是轮询。另一个最好不要用中断的场景是网络，网络收到的每个包都通过中断进行处理可能导致OS发生活锁。

使用DMA进行高效的数据传送：进程需要向磁盘写一些数据，要先将数据从内存拷贝到磁盘，拷贝结束后，磁盘的I/O操作才开始执行，CPU才可以处理其他请求。

{{&lt; image src=&#34;/ostep_note_pic/没用DMA的数据传送.png&#34; caption=&#34;没用DMA的数据传送&#34; &gt;}}

这个图很有意思的展示了，c（copy）是占用CPU时间的，copy之后才可以切进程重叠I/O操作，即这个copy是该进程必须的。解决方案就是使用DMA（Direct Memory Access），直接内存访问，是一个特殊设备，可以不需要CPU介入完成内存和设备间的数据传递。其工作过程是：OS告诉DMA数据在内存的位置，要拷贝的大小及目的地，当DMA完成后会抛出中断告诉OS已经完成数据传输，即：

{{&lt; image src=&#34;/ostep_note_pic/使用DMA的数据传送.png&#34; caption=&#34;使用DMA的数据传送&#34; &gt;}}

OS如何与硬件通信？1. 用明确的I/O指令，这些指令规定了OS将数据发送到特定设备寄存器的方法；2. 内存映射I/O，硬件将设备寄存器作为内存地址。当需要访问设备时，OS装载或存入到该内存地址，然后硬件会将装载或存入转移到设备上，而不是物理内存。这两种方法今天都在用。

设备驱动程序：每个设备都有具体的接口，需要将他们纳入到OS中。我们希望构建尽可能通用的OS，比如文件系统，可以工作在各种硬盘设备之上且希望文件系统不需要清楚读写请求的全部细节。这就要用到抽象技术，在底层设备OS的部分软件称为驱动程序，将所有设备交互的细节进行了封装。

文件系统完全不知道它使用的什么类型磁盘，只需要简单地向通用块设备层发送读写请求即可，块设备层会讲这些请求路由到对应的设备驱动，然后设备驱动完成真正的底层操作。

### 文件和目录
存储虚拟化形成了两个关键的抽象：1. 文件，文件就是一个线性字节数组，每个字节都可以读取或写入。每个文件都有某种低级名称，通常是数字，用户不知道这个数字，称为inode。2. 目录，也有一个低级名字（即inode），但是它的内容非常具体：包含一个（用户可读名字，低级名字）对的列表。例如，假设存在inode为“10”的文件，它的可读用户名称为“foo”，“foo”所在的目录因此会有条目（“foo”，“10”），目录中的每个条目都指向文件或其他目录。通过将目录放到其他目录中，用户可以构建任意的目录树，在该目录树下存储所有文件和目录，目录结构从根目录（“/”）开始。

创建文件：通过open系统调用，调用open()，并传入O_CREAT标志，程序可以创建一个新文件。open返回值是文件描述符，是一个整数，每个进程私有。一旦一个文件被open，你就可以使用文件描述符来读取或写入文件；或者将其作为指向文件类型对象的指针。一旦有了文件描述符就可以调用read()和write()来访问文件。

cat系统调用是查看文件内容，它是如何做到的？使用strace工具，可以跟踪程序在运行时所做的每个系统调用。

{{&lt; image src=&#34;/ostep_note_pic/strace_cat.png&#34; caption=&#34;strace cat&#34; &gt;}}

1. 打开foo文件，以只读的形式，返回文件描述符3（0：标准输入；1：标准输出；2：标准错误；第一次打开另一个文件时几乎肯定是文件描述符3）
2. read第一个参数是文件描述符，第二个参数指向一个用于放read结果的缓冲区，第三个参数是缓冲区的大小
3. write将缓冲区的结果写入到文件描述符1对应的输出中
4. 继续试图read更多的内容，但没有剩余字节可读，返回0
5. 程序调用close()，传入对应的文件描述符，关闭对应的文件
   
使用lseek()系统调用能读取或写入文件中特定偏移量的数据。

当程序调用write()时不是立即写入到磁盘，而是文件系统将这些写入在内存中缓冲区（buffer）一段时间，之后写入到存储设备。但可能就在write()调用之后，写入到存储设备之前，设备发生崩溃，数据会丢失。对于数据库管理系统需要保证这些数据能强制写入磁盘，在UNIX中提供了fsync(int fd)系统调用，文件系统将把所有脏数据（上述在内存缓冲区的数据）强制写入到磁盘中，而不等待，一旦fsync()返回，应用程序就知道数据已被保存了。

文件元数据：文件系统保存的每个文件的信息。可以使用stat()查看特定文件的元数据，该系统调用将路径名添加到一个文件中，并填充一个stat结构，包括大小、inode号、设备id等信息。文件系统会将这种类型的信息保存在一个名为inode的结构中，可以将其看作是由文件系统保存的持久数据结构。
mv文件重命名，使用了rename()；rm删除文件，使用了unlink()。rm *删除当前目录所有文件，若也删除目录：rm -rf *递归方式进入每个目录并删除其内容。

mkdir创建目录，创建的目录是“空的”，有两个条目：一个引用自身的条目（“.”），一个引用其父目录的条目（“..”）。ls读取目录，使用opendir()、readdir()和closedir()三个系统调用打印目录中每个文件的名称和inode编号。rmdir删除目录，要求该目录在被删除之前是空的（只有“.”和“..”），试图删除一个非空目录，对rmdir()的调用就会失败。

硬链接：ln file file2，将一个新的文件名“链接”到一个旧的文件名时，实际上创建了另一种引用同一个文件的方法。link在要创建链接的目录中创建了另一个名称，并将其指向原有文件的相同inode号。其实就是两个文件指向了一个inode号，对同一个inode的引用。

创建文件时实际做了两件事：1. 创建一个结构inode，跟踪几乎所有关于文件的信息（大小、文件块在磁盘上的位置等）。2. 将我们可读的名称链接到该文件并放到目录中。

unlink()就是删除文件名到inode的链接，不一定真正删除inode对应的文件，文件系统会检查inode号中的引用计数，只有当引用计数为零（没有对应的链接了）才会释放inode和相关的数据块。

符号链接：硬链接存在局限，即不能创建目录的硬链接（可能会在目录树中创建一个环）；不能硬链接到其他磁盘分区的文件（inode号在特定文件系统是唯一的）。

ln -s file file2创建软链接，软链接是创建一个文件，文件内容是指向文件的路径名。那这样可能会产生悬空引用，即删除真正指向的文件后，创建的软连接将指向不存在的路径。

如何从许多底层文件系统组建完整的目录树？先制作文件系统，然后挂载它们，使其内容可以访问。大多数文件系统使用mkfs创建一个文件系统，作为输入，为该工具提供一个设备（例如磁盘分区，/dev/sda1），一种文件系统类型（例如ext3），就在该磁盘分区上写入一个空文件系统，从根目录开始。然后还需要通过mount挂载到文件系统树中进行访问，本质上是将新建的文件系统粘贴到目录树的挂载点上。
```Shell
mount -t ext3 /dev/sda1 /home/users
```
将设备/dev/sda1创建的文件系统ext3挂载到/home/users挂载点上，那么就可以通过/home/users访问原文件系统的文件了。mount的美妙之处在于：它将所有文件系统统一到一棵树中，而不是拥有多个独立的文件系统，命名统一且方便。

### 文件系统实现
文件系统的数据结构：在磁盘上使用哪些类型的结构来组织其数据和元数据？

文件组织：将磁盘分块（类似内存分页），每一块大小4KB，如一块磁盘划分了64个块，后56个存的是用户数据，称为数据区域。再把其中5个块用于inode。还需要记录这些inode块和数据块是空闲还是已分配，可以使用位图，一个位图块用于表示inode，一个位图块用于表示数据块。还有一个块是超级块，包含关于该特定文件系统的信息，包括该文件系统有多少个inode和数据块、inode表的开始位置等等，因此在挂载文件系统时会OS先读取超级块，初始化各种参数，然后将该卷添加到文件系统树中。读取inode号32时，会计算inode区域的偏移量（32 * inode的大小），再加上inode表的起始地址，从而得到inode块的正确字节地址。在每个inode中，存的是元数据，关于文件的信息（文件类型、大小、分配的块数等），在inode中有一个或多个直接指针，每个指针指向属于该文件的一个磁盘块（数据块）。那么如果一个文件很大，就需要很多指针，而超出inode块能存放的指针数，就不行了。间接指针：不直接指向数据块，而是指向包含更多指针的块。因此，inode可以有一些固定数量的直接指针和一个间接指针。如果文件变得足够大，就会分配一个间接块（数据块区域），假设一个块是4KB，磁盘地址是4字节，那就增加了1024个指针，文件可以有13 * 4KB增长到（12 &#43; 1024）* 4KB = 4144KB。想要支持更大的文件，双重间接指针，所指向的间接块的指针再去指向间接块，这样就是（12 &#43; 1024 &#43; 1024 * 1024）* 4KB，这种不平衡树称为指向文件块的多级索引。许多文件系统都使用了多级索引。

{{&lt; image src=&#34;/ostep_note_pic/文件系统组织.png&#34; caption=&#34;文件系统组织&#34; &gt;}}

目录组织：目录包含一个二元组（条目名称，inode号）的列表。文件系统将目录视为特殊类型的文件，因此，目录有一个inode，位于inode块的某处，所以上述磁盘结构不变。每个条目有一个inode号、记录长度、字符串长度、条目名称。

{{&lt; image src=&#34;/ostep_note_pic/目录组织.png&#34; caption=&#34;目录组织&#34; &gt;}}

文件系统的访问方法：如何将进程发出的调用（如open()、read()、write()等）映射到它的结构中？

从磁盘读取文件：发出open(&#34;/foo/bar&#34;, O_RDONLY)调用时，文件系统首先需要找到bar的inode，从而获得文件的一些基本信息。文件系统必须遍历路径名，从而找到所需的inode，所有遍历都从文件系统的根开始，即根目录“/”，因此第一次磁盘读取是根目录的inode，要找根目录的inode，必须在其父目录中找，但根目录没有父目录，故根的inode号必须是众所周知的，大多数UNIX文件系统，根的inode号是2，因此文件系统会先读入inode号为2的块。然后就是递归遍历路径名，找所需的inode号，最后找到bar的inode读入内存。然后文件系统进行最后的权限检查，为此进程分配一个文件描述符，并将它返回给用户。注意，open导致的I/O量与路径名的长度成正比，对于路径中每个增加的目录，都必须读取它的inode及其数据。

写入磁盘：写入一个新文件时，每次写入操作不仅需要将数据写入磁盘，还必须决定将哪个块分配给文件，从而相应地更新磁盘的其他结构。每次写入时，会有5个I/O：1. 读取数据位图；2. 写入位图；3. 读取数据块；4. 写入inode；5. 写入数据块。即使是最简单的操作，如打开、读取或写入文件，也会产生大量I/O操作。

缓存和缓冲：现代OS采用动态划分方法，将虚拟内存页面和文件系统页面集成到统一页面缓存中。大多数现代文件系统将写入在内存中缓冲5-30s，如果系统在更新传递到磁盘之前崩溃，更新就会丢失。但是，将内存写入时间延长，则可以通过批处理、调度甚至避免写入，提高性能。数据库就不能这样，要求强制写入磁盘，调用fsync()。

### 崩溃一致性
文件系统面临一个主要挑战：如何在出现断电或系统崩溃的情况下，更新持久数据结构。文件更新时，文件系统必须对磁盘执行3次单独写入，分别针对inode、位图和数据块。但当发出write()系统调用时，这些写操作通常不会立即发生，会在内存中存在一段时间，然后写入，这就可能发生崩溃导致磁盘更新错误。这就会产生崩溃一致性问题。

解决方案1：文件系统检查程序fsck，在许多阶段运行，决定让不一致的事情发生，然后再修复它们（重启时）。

解决方案2：最流行的方案，从数据块管理系统借鉴的，预写日志。更新磁盘时，在覆写结构之前，首先写下一点注记（预写），把它写入一个结构并组织成日志。可以保证在发生崩溃时，能返回并查看注记，然后重试。因此，在崩溃后能准确知道要修复的内容，而不必扫描整个磁盘。

### 分布式系统
故障是构建分布式系统的核心挑战，通过聚集一组机器，可以构建一个看起来很少失败的系统，尽管它的组件经常出现故障。系统性能也很关键，必须经常仔细考虑如何使通信尽可能高效（低延迟、高带宽）。安全也是必要因素，连接到远程站点时，确保远程方是他们声称的人，以及确保第三方无法监听或改变双方之间的通信。

分布式如何通信？通信层如何处理故障？现代网络的核心原则是，通信基本是不可靠的。丢包是网络的基本现象，所以问题变成如何处理丢包？这里讲述了UDP和TCP通信协议，基础网络知识。在这里再次记录下TCP是如何实现可靠传输的。1. 使用ack确认报文，发送方发送给接收方后，接收方要回一个确认报文，如果接收方没收到（丢失fin报文），就要超时重传；2. 接收方的ack报文丢失，会导致发送方超时重传，在接收方看来发送方发了两次同样的fin报文，所以我们需要检测这种重复的消息传输。发送方就要在发送时生成唯一的ID，就是初始化随机序列号，在ack回应时将序列号加一，且附赠自己的随机序列号。

远程过程调用（RPC），久闻大名的RPC，但一直未了解，这里在分布式系统中出现了。OS对于构建分布式系统使用的编程语言抽象，有一个简单的目标：实现在远程机器上执行代码像调用本地函数一样简单直接。RPC系统通常有两部分：存根生成器和运行时库。

存根生成器：通过自动化，消除将函数参数和结果打包成消息的一些痛苦。存根生成器接受接口，并生成一些不同的代码片段。对于客户端生成客户端存根，包含接口中指定的每个函数，希望客户端程序链接此客户端存根，调用它以进行RPC。

客户端存根中的代码执行的操作：
- 创建消息缓冲区
- 将所需消息打包到消息缓冲区中（消息的序列化：将要调用函数的某种标识符，以及函数所需的所有参数放入单个缓冲区的过程）
- 将消息发送到目标RPC服务器
- 等待回复
- 解包返回代码和其他参数
- 返回调用者

服务器时执行的操作：
- 解包消息（反序列化）
- 调用实际函数
- 打包结果
- 发送回复

如果一个RPC调用阻塞，就会浪费服务器资源。大多数服务器以某种并发方式构造，常见的组织方式是线程池。服务器启动时会创建一组有限的线程，消息到达时，被分派给这些工作线程之一，然后执行RPC调用，最终回复。

运行时库处理RPC系统中的大部分繁重工作，处理大多数性能和可靠性问题。使用IP和端口号找到远程服务器通信的地址，使用不可靠的通信，如UDP来实现更高效的RPC层。RPC层通过使用超时重传和确认来实现所需的责任级别，通过使用某种形式的序列编号，通信层可以保证每个RPC只发生一次，或者最多只发生一次。

运行时必须处理具有大参数的过程调用，大于可以放入单个数据包的过程。一些底层的网络协议提供这样的发送方分组和接收方重组，如果没有，RPC需要自己实现这样的功能。

还需要支持在不同字节序（大端序和小端序）的机器之间进行通信，RPC在其消息格式中提供明确定义的字节序，如果发送和接收消息的计算机与RPC中定义的字节序一致，就会按预期发送和接收消息，如果不一致必须转换每条消息。


---

> 作者: [UAreFree](https://github.com/UAreFree)  
> URL: http://localhost:1313/posts/ostep-note/  

